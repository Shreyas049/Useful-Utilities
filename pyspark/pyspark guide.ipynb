{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced54736",
   "metadata": {},
   "source": [
    "## **PySpark Fundamentals - Part 1: Setting Up and Core Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c3589",
   "metadata": {},
   "source": [
    "### What is PySpark?\n",
    "PySpark is the Python API for Apache Spark, which is a distributed computing framework designed to process large datasets across clusters of computers. Think of it as a way to run Python code on multiple machines simultaneously, making it possible to handle datasets that are too large for a single computer's memory.\n",
    "The key insight is that PySpark operates on the principle of \"lazy evaluation\" - it builds up a plan of what you want to do with your data, but doesn't actually execute anything until you explicitly ask for results. This allows Spark to optimize the entire workflow before running it.\n",
    "\n",
    "### Setting Up PySpark\n",
    "First, let's look at how to initialize a PySpark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a506ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Setup and Initialization\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark session - this is your entry point to PySpark\n",
    "# Think of it as opening a connection to the Spark cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApplication\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Context available as 'sc': {spark.sparkContext}\")\n",
    "print(f\"SQL Context available as 'spark': {spark}\")\n",
    "\n",
    "# At the end of your program, always stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6108da4",
   "metadata": {},
   "source": [
    "### **Core Data Structures in PySpark**\n",
    "\n",
    "PySpark has two main data structures you'll work with:\n",
    "#### **1. RDD (Resilient Distributed Dataset)**\n",
    "This is the lower-level abstraction. Think of it as a collection of objects distributed across your cluster. While powerful, it's more complex to work with and requires you to think about the underlying distributed nature of your data.\n",
    "\n",
    "#### **2. DataFrame**\n",
    "\n",
    "This is the higher-level abstraction that sits on top of RDDs. It's similar to a pandas DataFrame or a SQL table, with named columns and defined data types. This is what you'll use 95% of the time, and it's what we'll focus on.\n",
    "The DataFrame abstraction is incredibly powerful because it allows Spark's Catalyst optimizer to understand your data's structure and optimize your queries automatically. When you write DataFrame operations, Spark can rearrange, combine, and optimize your operations in ways that would be impossible with raw RDDs.\n",
    "Let me show you how to create and work with DataFrames in the next section. This will include reading data from various sources, understanding the DataFrame structure, and performing basic operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47a5e3",
   "metadata": {},
   "source": [
    "## **PySpark Fundamentals - Part 2: DataFrames and Basic Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8e65e",
   "metadata": {},
   "source": [
    "### **Creating DataFrames**\n",
    "DataFrames are like tables in a database or spreadsheets - they have rows and columns with defined data types. Let's explore the different ways to create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()\n",
    "\n",
    "# Method 1: Create DataFrame from Python list of tuples\n",
    "# This is great for small test datasets or when prototyping\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineer\", 75000.0),\n",
    "    (\"Bob\", 30, \"Manager\", 85000.0),\n",
    "    (\"Charlie\", 35, \"Director\", 120000.0),\n",
    "    (\"Diana\", 28, \"Analyst\", 65000.0)\n",
    "]\n",
    "\n",
    "# Define column names - Spark will infer data types\n",
    "columns = [\"name\", \"age\", \"job_title\", \"salary\"]\n",
    "df_simple = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Simple DataFrame created:\")\n",
    "df_simple.show()\n",
    "df_simple.printSchema()  # This shows the structure and data types\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 2: Create DataFrame with explicit schema\n",
    "# This is better for production code as you control exactly what data types you want\n",
    "# Think of schema as a blueprint that tells Spark exactly what to expect\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),      # True means nullable\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"job_title\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_with_schema = spark.createDataFrame(data, schema)\n",
    "print(\"DataFrame with explicit schema:\")\n",
    "df_with_schema.show()\n",
    "df_with_schema.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 3: Reading from files (most common in real applications)\n",
    "# Let's create a sample CSV first, then read it\n",
    "\n",
    "# Create sample data and save as CSV\n",
    "df_from_csv = spark.read.option(\"header\", \"true\").csv(\"path/to/your/file.csv\")\n",
    "df_from_json = spark.read.json(\"path/to/your/file.json\")\n",
    "df_from_parquet = spark.read.parquet(\"path/to/your/file.parquet\")\n",
    "\n",
    "print(\"Sample employee data:\")\n",
    "df_from_csv.show()\n",
    "\n",
    "# Method 4: Create DataFrame from dictionary (useful for configuration data)\n",
    "dict_data = [\n",
    "    {\"product\": \"Laptop\", \"price\": 1200, \"category\": \"Electronics\"},\n",
    "    {\"product\": \"Chair\", \"price\": 150, \"category\": \"Furniture\"},\n",
    "    {\"product\": \"Book\", \"price\": 25, \"category\": \"Education\"}\n",
    "]\n",
    "\n",
    "df_from_dict = spark.createDataFrame(dict_data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "df_from_dict.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae0aa2",
   "metadata": {},
   "source": [
    "### **Understanding DataFrames - Key Concepts**\n",
    "\n",
    "Now that you've seen how to create DataFrames, let's understand what makes them special. Think of a DataFrame as a distributed table where the data is automatically split across multiple computers, but you can work with it as if it were a single table.\n",
    "\n",
    "Here are the fundamental concepts you need to grasp:\n",
    "\n",
    "#### **Lazy Evaluation:** \n",
    "When you write DataFrame operations, Spark doesn't immediately execute them. Instead, it builds what's called a \"logical plan\" - essentially a recipe of what you want to do. Only when you call an \"action\" (like .show(), .collect(), or .count()) does Spark actually execute the plan. This allows Spark to optimize your entire workflow before running it.\n",
    "\n",
    "#### **Immutability:**\n",
    "DataFrames are immutable, meaning you can't change them directly. Every operation creates a new DataFrame. This might seem inefficient, but it's actually what allows Spark to run operations in parallel safely and recover from failures.\n",
    "\n",
    "#### **Partitioning:**\n",
    "Your data is automatically split into chunks called partitions, distributed across different machines. You usually don't need to worry about this, but understanding it helps explain why Spark can process massive datasets efficiently.\n",
    "\n",
    "#### **Basic DataFrame Operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame Operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()\n",
    "\n",
    "# Create sample data for demonstration\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineer\", 75000.0, \"Tech\"),\n",
    "    (\"Bob\", 30, \"Manager\", 85000.0, \"Sales\"),\n",
    "    (\"Charlie\", 35, \"Director\", 120000.0, \"Tech\"),\n",
    "    (\"Diana\", 28, \"Analyst\", 65000.0, \"Finance\"),\n",
    "    (\"Eve\", 32, \"Engineer\", 78000.0, \"Tech\"),\n",
    "    (\"Frank\", 45, \"VP\", 150000.0, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"job_title\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 1. SELECTING COLUMNS\n",
    "# Think of this like choosing which columns you want to see in your spreadsheet\n",
    "print(\"1. Selecting specific columns:\")\n",
    "\n",
    "# Method 1: Using column names as strings\n",
    "df_selected = df.select(\"name\", \"salary\")\n",
    "df_selected.show()\n",
    "\n",
    "# Method 2: Using column objects (more flexible for complex operations)\n",
    "df_selected2 = df.select(col(\"name\"), col(\"salary\"))\n",
    "df_selected2.show()\n",
    "\n",
    "# Method 3: Selecting all columns from original df\n",
    "df_all = df.select(\"*\")\n",
    "print(f\"Total columns in original DataFrame: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. FILTERING ROWS\n",
    "# This is like applying a filter in Excel - only show rows that meet certain conditions\n",
    "print(\"2. Filtering rows:\")\n",
    "\n",
    "# Find employees with salary > 75000\n",
    "high_earners = df.filter(col(\"salary\") > 75000)\n",
    "print(\"Employees earning more than $75,000:\")\n",
    "high_earners.show()\n",
    "\n",
    "# Multiple conditions - find Tech employees earning > 75000\n",
    "tech_high_earners = df.filter(\n",
    "    (col(\"department\") == \"Tech\") & (col(\"salary\") > 75000)\n",
    ")\n",
    "print(\"Tech employees earning more than $75,000:\")\n",
    "tech_high_earners.show()\n",
    "\n",
    "# Using SQL-like string expressions (alternative syntax)\n",
    "young_employees = df.filter(\"age < 35\")\n",
    "print(\"Employees younger than 35:\")\n",
    "young_employees.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. ADDING NEW COLUMNS\n",
    "# Think of this as creating calculated fields in your spreadsheet\n",
    "print(\"3. Adding new columns:\")\n",
    "\n",
    "# Add a column for annual bonus (10% of salary)\n",
    "df_with_bonus = df.withColumn(\"annual_bonus\", col(\"salary\") * 0.10)\n",
    "print(\"DataFrame with bonus column:\")\n",
    "df_with_bonus.show()\n",
    "\n",
    "# Add multiple columns at once\n",
    "df_enhanced = df.withColumn(\"annual_bonus\", col(\"salary\") * 0.10) \\\n",
    "                .withColumn(\"age_category\", \n",
    "                           when(col(\"age\") < 30, \"Young\")\n",
    "                           .when(col(\"age\") < 40, \"Mid-Career\")\n",
    "                           .otherwise(\"Senior\"))\n",
    "\n",
    "print(\"DataFrame with multiple new columns:\")\n",
    "df_enhanced.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 4. RENAMING COLUMNS\n",
    "# Simple way to change column names\n",
    "print(\"4. Renaming columns:\")\n",
    "\n",
    "df_renamed = df.withColumnRenamed(\"job_title\", \"position\") \\\n",
    "               .withColumnRenamed(\"salary\", \"annual_salary\")\n",
    "\n",
    "print(\"DataFrame with renamed columns:\")\n",
    "df_renamed.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 5. SORTING DATA\n",
    "# Order your data by one or more columns\n",
    "print(\"5. Sorting data:\")\n",
    "\n",
    "# Sort by salary in descending order\n",
    "df_sorted = df.orderBy(col(\"salary\").desc())\n",
    "print(\"Employees sorted by salary (highest first):\")\n",
    "df_sorted.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_multi_sort = df.orderBy(col(\"department\").asc(), col(\"salary\").desc())\n",
    "print(\"Employees sorted by department, then by salary within each department:\")\n",
    "df_multi_sort.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 6. BASIC STATISTICS AND AGGREGATIONS\n",
    "print(\"6. Basic statistics:\")\n",
    "\n",
    "# Count total rows\n",
    "total_count = df.count()\n",
    "print(f\"Total number of employees: {total_count}\")\n",
    "\n",
    "# Basic statistics for numeric columns\n",
    "df.describe().show()\n",
    "\n",
    "# Specific aggregations\n",
    "avg_salary = df.agg(avg(\"salary\")).collect()[0][0]  # collect() brings data to driver\n",
    "max_age = df.agg(max(\"age\")).collect()[0][0]\n",
    "print(f\"Average salary: ${avg_salary:,.2f}\")\n",
    "print(f\"Maximum age: {max_age}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 7. VIEWING DATA IN DIFFERENT WAYS\n",
    "print(\"7. Different ways to examine your data:\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"First 3 rows:\")\n",
    "df.show(3)\n",
    "\n",
    "# Show all data (be careful with large datasets!)\n",
    "print(\"All data with unlimited width:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Get column names and types\n",
    "print(\"Column information:\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Get a single row as a Row object\n",
    "first_row = df.first()\n",
    "print(f\"First row: {first_row}\")\n",
    "print(f\"First employee's name: {first_row['name']}\")\n",
    "\n",
    "# Understanding the difference between transformations and actions:\n",
    "# Transformations (lazy - create new DataFrame): select, filter, withColumn, orderBy\n",
    "# Actions (eager - trigger computation): show, count, collect, first, take"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f10351",
   "metadata": {},
   "source": [
    "- The operations represent the foundation of almost everything you'll do with PySpark DataFrames. Notice how each operation follows a pattern: you start with a DataFrame, apply a transformation, and get back a new DataFrame. This chaining pattern is central to how PySpark works and makes it possible to build complex data processing pipelines by combining simple operations.\n",
    "\n",
    "- The key insight here is understanding the difference between transformations and actions. Transformations like select(), filter(), and withColumn() are lazy - they just build up a plan of what you want to do. Actions like show(), count(), and collect() actually trigger Spark to execute all the transformations you've defined. This design allows Spark to optimize your entire pipeline before running it, often making dramatic performance improvements.\n",
    "\n",
    "- Think of it like planning a route on a map app - you can add multiple stops and change your route as much as you want, but the app only calculates the optimal path when you press \"Start Navigation.\" Similarly, Spark waits until you ask for results before figuring out the most efficient way to get them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a1bbd",
   "metadata": {},
   "source": [
    "## **PySpark Advanced - Part 3: Grouping, Aggregations, and Joins**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb00dbe",
   "metadata": {},
   "source": [
    "### **Grouping and Aggregations**\n",
    "\n",
    "Think of grouping like creating pivot tables in Excel - you're organizing your data by certain categories and then calculating summaries for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregations in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a more comprehensive dataset for demonstration\n",
    "sales_data = [\n",
    "    (\"Alice\", \"Tech\", \"Q1\", 2023, 150000, 5),\n",
    "    (\"Alice\", \"Tech\", \"Q2\", 2023, 180000, 6),\n",
    "    (\"Bob\", \"Sales\", \"Q1\", 2023, 120000, 8),\n",
    "    (\"Bob\", \"Sales\", \"Q2\", 2023, 140000, 9),\n",
    "    (\"Charlie\", \"Tech\", \"Q1\", 2023, 200000, 4),\n",
    "    (\"Charlie\", \"Tech\", \"Q2\", 2023, 220000, 5),\n",
    "    (\"Diana\", \"Marketing\", \"Q1\", 2023, 80000, 12),\n",
    "    (\"Diana\", \"Marketing\", \"Q2\", 2023, 95000, 15),\n",
    "    (\"Eve\", \"Sales\", \"Q1\", 2023, 110000, 7),\n",
    "    (\"Eve\", \"Sales\", \"Q2\", 2023, 130000, 8),\n",
    "    (\"Frank\", \"Tech\", \"Q1\", 2023, 175000, 3),\n",
    "    (\"Frank\", \"Tech\", \"Q2\", 2023, 190000, 4)\n",
    "]\n",
    "\n",
    "columns = [\"employee\", \"department\", \"quarter\", \"year\", \"revenue\", \"deals_closed\"]\n",
    "df_sales = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "df_sales.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. BASIC GROUPING - Group by single column\n",
    "print(\"1. Basic Grouping - Revenue by Department:\")\n",
    "\n",
    "# Group by department and calculate total revenue\n",
    "dept_revenue = df_sales.groupBy(\"department\") \\\n",
    "                      .agg(sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                           avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "                           count(\"*\").alias(\"record_count\")) \\\n",
    "                      .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "dept_revenue.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 2. MULTIPLE GROUPING COLUMNS\n",
    "print(\"2. Multiple Grouping - Revenue by Department and Quarter:\")\n",
    "\n",
    "dept_quarter_stats = df_sales.groupBy(\"department\", \"quarter\") \\\n",
    "                            .agg(sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                                 sum(\"deals_closed\").alias(\"total_deals\"),\n",
    "                                 avg(\"revenue\").alias(\"avg_revenue_per_person\"),\n",
    "                                 count(\"employee\").alias(\"employee_count\")) \\\n",
    "                            .orderBy(\"department\", \"quarter\")\n",
    "\n",
    "dept_quarter_stats.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 3. ADVANCED AGGREGATIONS\n",
    "print(\"3. Advanced Aggregations with Multiple Functions:\")\n",
    "\n",
    "comprehensive_stats = df_sales.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        # Revenue statistics\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "        min(\"revenue\").alias(\"min_revenue\"),\n",
    "        max(\"revenue\").alias(\"max_revenue\"),\n",
    "        \n",
    "        # Deal statistics\n",
    "        sum(\"deals_closed\").alias(\"total_deals\"),\n",
    "        avg(\"deals_closed\").alias(\"avg_deals_per_person\"),\n",
    "        \n",
    "        # Count statistics\n",
    "        count(\"*\").alias(\"total_records\"),\n",
    "        countDistinct(\"employee\").alias(\"unique_employees\"),\n",
    "        \n",
    "        # Advanced: Standard deviation and variance\n",
    "        stddev(\"revenue\").alias(\"revenue_stddev\"),\n",
    "        \n",
    "        # Custom aggregation: Revenue per deal\n",
    "        (sum(\"revenue\") / sum(\"deals_closed\")).alias(\"revenue_per_deal\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "comprehensive_stats.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 4. CONDITIONAL AGGREGATIONS\n",
    "print(\"4. Conditional Aggregations:\")\n",
    "\n",
    "# Count how many high performers (>= 150k revenue) vs regular performers by department\n",
    "performance_analysis = df_sales.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        sum(when(col(\"revenue\") >= 150000, 1).otherwise(0)).alias(\"high_performers\"),\n",
    "        sum(when(col(\"revenue\") < 150000, 1).otherwise(0)).alias(\"regular_performers\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "        # Calculate percentage of high performers\n",
    "        (sum(when(col(\"revenue\") >= 150000, 1).otherwise(0)) * 100.0 / count(\"*\")).alias(\"high_performer_percentage\")\n",
    "    )\n",
    "\n",
    "performance_analysis.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 5. COLLECT_LIST and COLLECT_SET - Gathering values\n",
    "print(\"5. Collecting Values - Who works in each department:\")\n",
    "\n",
    "department_employees = df_sales.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        collect_set(\"employee\").alias(\"unique_employees\"),  # collect_set removes duplicates\n",
    "        collect_list(\"employee\").alias(\"all_employee_records\"),  # collect_list keeps duplicates\n",
    "        count(\"*\").alias(\"total_records\")\n",
    "    )\n",
    "\n",
    "department_employees.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 6. PIVOT OPERATIONS - Reshape data like Excel pivot tables\n",
    "print(\"6. Pivot Operations - Department revenue by quarter:\")\n",
    "\n",
    "# Pivot: Turn quarter values into columns\n",
    "pivoted_data = df_sales.groupBy(\"department\") \\\n",
    "                      .pivot(\"quarter\") \\\n",
    "                      .agg(sum(\"revenue\")) \\\n",
    "                      .orderBy(\"department\")\n",
    "\n",
    "pivoted_data.show()\n",
    "\n",
    "# You can also pivot with multiple aggregations\n",
    "print(\"Multi-value pivot - Revenue and Deals by quarter:\")\n",
    "multi_pivot = df_sales.groupBy(\"department\") \\\n",
    "                     .pivot(\"quarter\") \\\n",
    "                     .agg(sum(\"revenue\").alias(\"revenue\"), \n",
    "                          sum(\"deals_closed\").alias(\"deals\"))\n",
    "\n",
    "multi_pivot.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 7. ROLLUP and CUBE - Multi-dimensional aggregations\n",
    "print(\"7. Rollup Operations - Hierarchical totals:\")\n",
    "\n",
    "# Rollup provides subtotals at different levels\n",
    "rollup_stats = df_sales.rollup(\"department\", \"quarter\") \\\n",
    "                      .agg(sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                           sum(\"deals_closed\").alias(\"total_deals\")) \\\n",
    "                      .orderBy(\"department\", \"quarter\")\n",
    "\n",
    "print(\"Rollup (includes subtotals and grand total):\")\n",
    "rollup_stats.show()\n",
    "\n",
    "# Cube provides all possible combinations of subtotals\n",
    "cube_stats = df_sales.cube(\"department\", \"quarter\") \\\n",
    "                    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n",
    "                    .orderBy(\"department\", \"quarter\")\n",
    "\n",
    "print(\"Cube (all possible subtotal combinations):\")\n",
    "cube_stats.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 8. WINDOW FUNCTIONS PREVIEW\n",
    "print(\"8. Adding Running Totals with Window Functions:\")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window: partition by department, order by quarter\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"quarter\")\n",
    "\n",
    "# Add running total of revenue within each department\n",
    "df_with_running_total = df_sales.withColumn(\n",
    "    \"running_revenue_total\", \n",
    "    sum(\"revenue\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"revenue_rank_in_dept\",\n",
    "    rank().over(Window.partitionBy(\"department\").orderBy(col(\"revenue\").desc()))\n",
    ")\n",
    "\n",
    "print(\"Data with running totals and rankings:\")\n",
    "df_with_running_total.select(\"employee\", \"department\", \"quarter\", \"revenue\", \n",
    "                            \"running_revenue_total\", \"revenue_rank_in_dept\") \\\n",
    "                     .orderBy(\"department\", \"quarter\") \\\n",
    "                     .show()\n",
    "\n",
    "# Key insights about grouping and aggregations:\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY CONCEPTS:\")\n",
    "print(\"1. groupBy() creates groups, agg() performs calculations on each group\")\n",
    "print(\"2. Always use .alias() to name your aggregated columns clearly\")\n",
    "print(\"3. You can combine multiple aggregation functions in one agg() call\")\n",
    "print(\"4. Pivot reshapes data - turns row values into columns\")\n",
    "print(\"5. Rollup/Cube provide hierarchical subtotals\")\n",
    "print(\"6. Window functions let you do calculations across related rows\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dfdec",
   "metadata": {},
   "source": [
    "### **Joining DataFrames**\n",
    "\n",
    "Joins are how you combine data from multiple DataFrames, similar to JOIN operations in SQL. This is crucial when your data is spread across multiple tables or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Joins in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create sample datasets for demonstration\n",
    "# Employee basic info\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineer\", \"Tech\"),\n",
    "    (2, \"Bob\", \"Manager\", \"Sales\"),\n",
    "    (3, \"Charlie\", \"Director\", \"Tech\"),\n",
    "    (4, \"Diana\", \"Analyst\", \"Finance\"),\n",
    "    (5, \"Eve\", \"Engineer\", \"Tech\")\n",
    "]\n",
    "employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"name\", \"job_title\", \"department\"])\n",
    "\n",
    "# Employee salary info (separate table)\n",
    "salaries_data = [\n",
    "    (1, 75000, \"2023-01-01\"),\n",
    "    (2, 85000, \"2023-01-01\"),\n",
    "    (3, 120000, \"2023-01-01\"),\n",
    "    (4, 65000, \"2023-01-01\"),\n",
    "    (6, 90000, \"2023-01-01\")  # Note: emp_id 6 doesn't exist in employees table\n",
    "]\n",
    "salaries_df = spark.createDataFrame(salaries_data, [\"emp_id\", \"salary\", \"effective_date\"])\n",
    "\n",
    "# Department budget info\n",
    "dept_budget_data = [\n",
    "    (\"Tech\", 500000, \"John Smith\"),\n",
    "    (\"Sales\", 300000, \"Sarah Johnson\"),\n",
    "    (\"Finance\", 200000, \"Mike Brown\"),\n",
    "    (\"HR\", 150000, \"Lisa Davis\")  # Note: No employees in HR department\n",
    "]\n",
    "dept_budget_df = spark.createDataFrame(dept_budget_data, [\"department\", \"budget\", \"manager\"])\n",
    "\n",
    "print(\"SAMPLE DATASETS:\")\n",
    "print(\"\\nEmployees:\")\n",
    "employees_df.show()\n",
    "print(\"Salaries:\")\n",
    "salaries_df.show()\n",
    "print(\"Department Budgets:\")\n",
    "dept_budget_df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. INNER JOIN - Only matching records from both sides\n",
    "print(\"1. INNER JOIN - Employees with salary information:\")\n",
    "\n",
    "inner_join_result = employees_df.join(salaries_df, \"emp_id\", \"inner\")\n",
    "inner_join_result.show()\n",
    "\n",
    "print(\"Notice: Only employees 1-4 appear because emp_id 5 has no salary, and emp_id 6 has no employee record\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. LEFT (OUTER) JOIN - All records from left table, matching from right\n",
    "print(\"2. LEFT JOIN - All employees, with salary where available:\")\n",
    "\n",
    "left_join_result = employees_df.join(salaries_df, \"emp_id\", \"left\")\n",
    "left_join_result.show()\n",
    "\n",
    "print(\"Notice: All 5 employees appear, but Eve (emp_id 5) has null salary values\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. RIGHT (OUTER) JOIN - All records from right table, matching from left\n",
    "print(\"3. RIGHT JOIN - All salary records, with employee info where available:\")\n",
    "\n",
    "right_join_result = employees_df.join(salaries_df, \"emp_id\", \"right\")\n",
    "right_join_result.show()\n",
    "\n",
    "print(\"Notice: emp_id 6 appears with salary but null employee information\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. FULL OUTER JOIN - All records from both tables\n",
    "print(\"4. FULL OUTER JOIN - Everything from both tables:\")\n",
    "\n",
    "full_join_result = employees_df.join(salaries_df, \"emp_id\", \"full\")\n",
    "full_join_result.show()\n",
    "\n",
    "print(\"Notice: All employees AND all salary records appear, with nulls where data is missing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 5. JOINING ON MULTIPLE CONDITIONS\n",
    "print(\"5. Complex Join Conditions:\")\n",
    "\n",
    "# Let's create data with multiple join keys\n",
    "projects_data = [\n",
    "    (1, \"Tech\", \"Project Alpha\", \"2023-Q1\"),\n",
    "    (2, \"Sales\", \"Project Beta\", \"2023-Q1\"),\n",
    "    (3, \"Tech\", \"Project Gamma\", \"2023-Q2\"),\n",
    "    (1, \"Tech\", \"Project Delta\", \"2023-Q2\")\n",
    "]\n",
    "projects_df = spark.createDataFrame(projects_data, [\"emp_id\", \"department\", \"project_name\", \"quarter\"])\n",
    "\n",
    "# Join on multiple conditions\n",
    "multi_condition_join = employees_df.join(\n",
    "    projects_df, \n",
    "    (employees_df.emp_id == projects_df.emp_id) & \n",
    "    (employees_df.department == projects_df.department),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"Employees matched to projects (by employee ID AND department):\")\n",
    "multi_condition_join.select(\n",
    "    employees_df.name, \n",
    "    employees_df.department, \n",
    "    projects_df.project_name, \n",
    "    projects_df.quarter\n",
    ").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 6. JOINING TABLES WITH DIFFERENT COLUMN NAMES\n",
    "print(\"6. Joining on columns with different names:\")\n",
    "\n",
    "# Create a table where the join key has a different name\n",
    "performance_data = [\n",
    "    (1, 85, \"Excellent\"),\n",
    "    (2, 78, \"Good\"),\n",
    "    (3, 92, \"Outstanding\"),\n",
    "    (4, 70, \"Satisfactory\")\n",
    "]\n",
    "performance_df = spark.createDataFrame(performance_data, [\"employee_id\", \"score\", \"rating\"])\n",
    "\n",
    "# Join where left table has 'emp_id' and right table has 'employee_id'\n",
    "different_names_join = employees_df.join(\n",
    "    performance_df,\n",
    "    employees_df.emp_id == performance_df.employee_id,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"Employees with performance ratings:\")\n",
    "different_names_join.select(\n",
    "    \"name\", \"job_title\", \"score\", \"rating\"\n",
    ").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 7. SELF JOINS - Joining a table with itself\n",
    "print(\"7. Self Join - Finding employees in the same department:\")\n",
    "\n",
    "# Join employees table with itself to find colleagues\n",
    "emp_alias1 = employees_df.alias(\"emp1\")\n",
    "emp_alias2 = employees_df.alias(\"emp2\")\n",
    "\n",
    "colleagues = emp_alias1.join(\n",
    "    emp_alias2,\n",
    "    (emp_alias1.department == emp_alias2.department) & \n",
    "    (emp_alias1.emp_id != emp_alias2.emp_id),  # Don't match employee with themselves\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp1.name\").alias(\"employee_1\"),\n",
    "    col(\"emp2.name\").alias(\"employee_2\"),\n",
    "    col(\"emp1.department\").alias(\"shared_department\")\n",
    ")\n",
    "\n",
    "print(\"Employees who work in the same department:\")\n",
    "colleagues.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. ANTI JOIN - Records in left table that DON'T have matches in right table\n",
    "print(\"8. Anti Join - Employees without salary records:\")\n",
    "\n",
    "employees_without_salary = employees_df.join(salaries_df, \"emp_id\", \"left_anti\")\n",
    "employees_without_salary.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 9. SEMI JOIN - Records in left table that DO have matches in right table\n",
    "print(\"9. Semi Join - Employees who have salary records:\")\n",
    "\n",
    "employees_with_salary = employees_df.join(salaries_df, \"emp_id\", \"left_semi\")\n",
    "employees_with_salary.show()\n",
    "\n",
    "print(\"Notice: This shows employee info only, even though we joined with salary table\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 10. COMPLEX MULTI-TABLE JOIN\n",
    "print(\"10. Complex Multi-table Join - Complete Employee Information:\")\n",
    "\n",
    "# Join all three tables together\n",
    "complete_info = employees_df \\\n",
    "    .join(salaries_df, \"emp_id\", \"left\") \\\n",
    "    .join(dept_budget_df, \"department\", \"left\") \\\n",
    "    .select(\n",
    "        \"name\",\n",
    "        \"job_title\", \n",
    "        \"department\",\n",
    "        \"salary\",\n",
    "        \"budget\",\n",
    "        \"manager\"\n",
    "    ).orderBy(\"name\")\n",
    "\n",
    "print(\"Complete employee information with department details:\")\n",
    "complete_info.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 11. HANDLING DUPLICATE COLUMN NAMES AFTER JOINS\n",
    "print(\"11. Handling duplicate column names:\")\n",
    "\n",
    "# When joining tables with same column names, you need to be explicit\n",
    "employees_with_dept_col = employees_df.select(\"emp_id\", \"name\", \"department\")\n",
    "projects_with_dept_col = projects_df.select(\"emp_id\", \"department\", \"project_name\")\n",
    "\n",
    "# This would cause ambiguous column reference - which department column?\n",
    "# Solution: Select specific columns or rename before joining\n",
    "clean_join = employees_with_dept_col.alias(\"emp\").join(\n",
    "    projects_with_dept_col.alias(\"proj\"),\n",
    "    col(\"emp.emp_id\") == col(\"proj.emp_id\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp.name\"),\n",
    "    col(\"emp.department\").alias(\"employee_dept\"),\n",
    "    col(\"proj.department\").alias(\"project_dept\"),\n",
    "    col(\"proj.project_name\")\n",
    ")\n",
    "\n",
    "print(\"Handling duplicate column names:\")\n",
    "clean_join.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JOIN TYPE SUMMARY:\")\n",
    "print(\"• INNER: Only matching records from both tables\")\n",
    "print(\"• LEFT: All from left table + matching from right\")\n",
    "print(\"• RIGHT: All from right table + matching from left\") \n",
    "print(\"• FULL: All records from both tables\")\n",
    "print(\"• LEFT_ANTI: Records in left that DON'T match right\")\n",
    "print(\"• LEFT_SEMI: Records in left that DO match right (left columns only)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012c871",
   "metadata": {},
   "source": [
    "### **Key Concepts**\n",
    "\n",
    "- **Grouping and Aggregations** allow you to answer questions like \"What's the total sales by region?\" or \"Who are the top performers in each department?\" The pattern is always: group your data by some categories, then calculate summaries for each group.\n",
    "Joins let you bring together related information from different datasets. Think of them as ways to connect the dots between different pieces of information. The type of join you choose depends on what you want to include in your final result.\n",
    "\n",
    "- The most important insight about joins is understanding what each type preserves:\n",
    "\n",
    "    **Inner joins are conservative** - only keep perfect matches \\\n",
    "    **Left joins prioritize the left table** - keep everything from it \\\n",
    "    **Full outer joins are comprehensive** - keep everything from everywhere\n",
    "\n",
    "- These operations form the backbone of most data analysis workflows. You'll typically read data from multiple sources, join them together to create a complete picture, then group and aggregate to generate insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61498093",
   "metadata": {},
   "source": [
    "## **PySpark Advanced - Part 4: Window Functions and Complex Data Manipulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fc857",
   "metadata": {},
   "source": [
    "Window functions are one of PySpark's most powerful features. They let you perform calculations across a set of related rows without collapsing your data into groups. Think of them as a way to \"look around\" at neighboring rows while keeping all your original data intact.\n",
    "\n",
    "### **Window Functions Deep Dive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Functions in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a comprehensive sales dataset\n",
    "sales_data = [\n",
    "    (\"Alice\", \"Tech\", \"2023-01\", 150000, 5),\n",
    "    (\"Alice\", \"Tech\", \"2023-02\", 180000, 6),\n",
    "    (\"Alice\", \"Tech\", \"2023-03\", 200000, 7),\n",
    "    (\"Bob\", \"Sales\", \"2023-01\", 120000, 8),\n",
    "    (\"Bob\", \"Sales\", \"2023-02\", 140000, 9),\n",
    "    (\"Bob\", \"Sales\", \"2023-03\", 160000, 10),\n",
    "    (\"Charlie\", \"Tech\", \"2023-01\", 200000, 4),\n",
    "    (\"Charlie\", \"Tech\", \"2023-02\", 220000, 5),\n",
    "    (\"Charlie\", \"Tech\", \"2023-03\", 240000, 6),\n",
    "    (\"Diana\", \"Marketing\", \"2023-01\", 80000, 12),\n",
    "    (\"Diana\", \"Marketing\", \"2023-02\", 95000, 15),\n",
    "    (\"Diana\", \"Marketing\", \"2023-03\", 110000, 18),\n",
    "    (\"Eve\", \"Sales\", \"2023-01\", 110000, 7),\n",
    "    (\"Eve\", \"Sales\", \"2023-02\", 130000, 8),\n",
    "    (\"Eve\", \"Sales\", \"2023-03\", 150000, 9)\n",
    "]\n",
    "\n",
    "columns = [\"employee\", \"department\", \"month\", \"revenue\", \"deals_closed\"]\n",
    "df = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "df.orderBy(\"employee\", \"month\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. RANKING FUNCTIONS\n",
    "print(\"1. Ranking Functions - Who are the top performers?\")\n",
    "\n",
    "# Define window for ranking within each department\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(col(\"revenue\").desc())\n",
    "\n",
    "df_with_rankings = df.withColumn(\"revenue_rank\", rank().over(dept_window)) \\\n",
    "                    .withColumn(\"revenue_dense_rank\", dense_rank().over(dept_window)) \\\n",
    "                    .withColumn(\"revenue_row_number\", row_number().over(dept_window))\n",
    "\n",
    "print(\"Rankings within each department:\")\n",
    "df_with_rankings.select(\"employee\", \"department\", \"revenue\", \n",
    "                       \"revenue_rank\", \"revenue_dense_rank\", \"revenue_row_number\") \\\n",
    "                .orderBy(\"department\", \"revenue_rank\") \\\n",
    "                .show()\n",
    "\n",
    "print(\"Key differences:\")\n",
    "print(\"- rank(): Gaps in ranking when there are ties (1, 2, 2, 4)\")\n",
    "print(\"- dense_rank(): No gaps in ranking (1, 2, 2, 3)\")\n",
    "print(\"- row_number(): Always unique numbers (1, 2, 3, 4)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. RUNNING TOTALS AND CUMULATIVE CALCULATIONS\n",
    "print(\"2. Running Totals - Track cumulative performance over time\")\n",
    "\n",
    "# Window ordered by month for each employee\n",
    "employee_time_window = Window.partitionBy(\"employee\").orderBy(\"month\")\n",
    "\n",
    "df_with_running_totals = df.withColumn(\n",
    "    \"cumulative_revenue\", sum(\"revenue\").over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"cumulative_deals\", sum(\"deals_closed\").over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"running_avg_revenue\", avg(\"revenue\").over(employee_time_window)\n",
    ")\n",
    "\n",
    "print(\"Running totals for each employee over time:\")\n",
    "df_with_running_totals.select(\"employee\", \"month\", \"revenue\", \"deals_closed\",\n",
    "                             \"cumulative_revenue\", \"cumulative_deals\", \"running_avg_revenue\") \\\n",
    "                     .orderBy(\"employee\", \"month\") \\\n",
    "                     .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. LAG AND LEAD - Looking at previous and next rows\n",
    "print(\"3. Lag and Lead - Compare with previous/next periods\")\n",
    "\n",
    "df_with_lag_lead = df.withColumn(\n",
    "    \"prev_month_revenue\", lag(\"revenue\", 1).over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"next_month_revenue\", lead(\"revenue\", 1).over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"revenue_growth\", col(\"revenue\") - lag(\"revenue\", 1).over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"revenue_growth_pct\", \n",
    "    ((col(\"revenue\") - lag(\"revenue\", 1).over(employee_time_window)) / \n",
    "     lag(\"revenue\", 1).over(employee_time_window) * 100)\n",
    ")\n",
    "\n",
    "print(\"Month-over-month comparisons:\")\n",
    "df_with_lag_lead.select(\"employee\", \"month\", \"revenue\", \"prev_month_revenue\", \n",
    "                       \"revenue_growth\", \"revenue_growth_pct\") \\\n",
    "                .orderBy(\"employee\", \"month\") \\\n",
    "                .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. MOVING AVERAGES AND WINDOW FRAMES\n",
    "print(\"4. Moving Averages - Smooth out trends\")\n",
    "\n",
    "# Define a window with a specific frame: current row and 1 preceding row\n",
    "moving_avg_window = Window.partitionBy(\"employee\") \\\n",
    "                         .orderBy(\"month\") \\\n",
    "                         .rowsBetween(-1, 0)  # Previous row and current row\n",
    "\n",
    "df_with_moving_avg = df.withColumn(\n",
    "    \"moving_avg_revenue_2months\", avg(\"revenue\").over(moving_avg_window)\n",
    ").withColumn(\n",
    "    \"moving_sum_deals_2months\", sum(\"deals_closed\").over(moving_avg_window)\n",
    ")\n",
    "\n",
    "# 3-month moving average (current + 2 preceding)\n",
    "moving_avg_3month_window = Window.partitionBy(\"employee\") \\\n",
    "                                .orderBy(\"month\") \\\n",
    "                                .rowsBetween(-2, 0)\n",
    "\n",
    "df_with_moving_avg = df_with_moving_avg.withColumn(\n",
    "    \"moving_avg_revenue_3months\", avg(\"revenue\").over(moving_avg_3month_window)\n",
    ")\n",
    "\n",
    "print(\"Moving averages to smooth trends:\")\n",
    "df_with_moving_avg.select(\"employee\", \"month\", \"revenue\", \n",
    "                         \"moving_avg_revenue_2months\", \"moving_avg_revenue_3months\") \\\n",
    "                 .orderBy(\"employee\", \"month\") \\\n",
    "                 .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 5. PERCENTILES AND NTILE\n",
    "print(\"5. Percentiles and Distribution Analysis\")\n",
    "\n",
    "# Overall window for percentiles across all employees\n",
    "overall_window = Window.orderBy(\"revenue\")\n",
    "\n",
    "df_with_percentiles = df.withColumn(\n",
    "    \"revenue_percentile\", percent_rank().over(overall_window)\n",
    ").withColumn(\n",
    "    \"revenue_quartile\", ntile(4).over(overall_window)  # Divide into 4 groups\n",
    ").withColumn(\n",
    "    \"revenue_decile\", ntile(10).over(overall_window)   # Divide into 10 groups\n",
    ")\n",
    "\n",
    "print(\"Percentile analysis:\")\n",
    "df_with_percentiles.select(\"employee\", \"month\", \"revenue\", \n",
    "                          \"revenue_percentile\", \"revenue_quartile\", \"revenue_decile\") \\\n",
    "                  .orderBy(\"revenue\") \\\n",
    "                  .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 6. FIRST_VALUE AND LAST_VALUE\n",
    "print(\"6. First and Last Values - Benchmarking against period extremes\")\n",
    "\n",
    "dept_month_window = Window.partitionBy(\"department\", \"month\").orderBy(\"revenue\")\n",
    "\n",
    "df_with_first_last = df.withColumn(\n",
    "    \"dept_min_revenue_this_month\", first(\"revenue\").over(dept_month_window)\n",
    ").withColumn(\n",
    "    \"dept_max_revenue_this_month\", last(\"revenue\").over(dept_month_window)\n",
    ").withColumn(\n",
    "    \"performance_vs_dept_min\", col(\"revenue\") - first(\"revenue\").over(dept_month_window)\n",
    ").withColumn(\n",
    "    \"performance_vs_dept_max\", col(\"revenue\") - last(\"revenue\").over(dept_month_window)\n",
    ")\n",
    "\n",
    "print(\"Performance vs department extremes:\")\n",
    "df_with_first_last.select(\"employee\", \"department\", \"month\", \"revenue\",\n",
    "                         \"dept_min_revenue_this_month\", \"dept_max_revenue_this_month\",\n",
    "                         \"performance_vs_dept_min\", \"performance_vs_dept_max\") \\\n",
    "                 .orderBy(\"department\", \"month\", \"revenue\") \\\n",
    "                 .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 7. COMPLEX WINDOW ANALYSIS - Multiple windows in one query\n",
    "print(\"7. Complex Analysis - Multiple Window Functions Together\")\n",
    "\n",
    "# Define multiple windows for different analyses\n",
    "employee_window = Window.partitionBy(\"employee\").orderBy(\"month\")\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(col(\"revenue\").desc())\n",
    "overall_window = Window.orderBy(\"revenue\")\n",
    "\n",
    "comprehensive_analysis = df.withColumn(\n",
    "    # Time-based analysis for each employee\n",
    "    \"employee_running_total\", sum(\"revenue\").over(employee_window)\n",
    ").withColumn(\n",
    "    \"employee_avg_so_far\", avg(\"revenue\").over(employee_window)\n",
    ").withColumn(\n",
    "    \"month_over_month_change\", col(\"revenue\") - lag(\"revenue\", 1).over(employee_window)\n",
    ").withColumn(\n",
    "    # Department ranking\n",
    "    \"dept_revenue_rank\", rank().over(dept_window)\n",
    ").withColumn(\n",
    "    # Overall percentile\n",
    "    \"overall_percentile\", percent_rank().over(overall_window)\n",
    ").withColumn(\n",
    "    # Performance indicators\n",
    "    \"is_top_performer_in_dept\", when(rank().over(dept_window) <= 2, \"Yes\").otherwise(\"No\")\n",
    ").withColumn(\n",
    "    \"performance_tier\", \n",
    "    when(percent_rank().over(overall_window) >= 0.8, \"Top 20%\")\n",
    "    .when(percent_rank().over(overall_window) >= 0.6, \"Top 40%\") \n",
    "    .when(percent_rank().over(overall_window) >= 0.4, \"Middle 20%\")\n",
    "    .when(percent_rank().over(overall_window) >= 0.2, \"Bottom 40%\")\n",
    "    .otherwise(\"Bottom 20%\")\n",
    ")\n",
    "\n",
    "print(\"Comprehensive performance analysis:\")\n",
    "comprehensive_analysis.select(\n",
    "    \"employee\", \"department\", \"month\", \"revenue\",\n",
    "    \"employee_running_total\", \"month_over_month_change\",\n",
    "    \"dept_revenue_rank\", \"performance_tier\", \"is_top_performer_in_dept\"\n",
    ").orderBy(\"employee\", \"month\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. WINDOW FRAMES - Different ways to define your \"window\"\n",
    "print(\"8. Understanding Window Frames\")\n",
    "\n",
    "# Current row only\n",
    "current_only = Window.partitionBy(\"employee\").orderBy(\"month\").rowsBetween(0, 0)\n",
    "\n",
    "# Unbounded preceding to current (running total)\n",
    "running_total_frame = Window.partitionBy(\"employee\").orderBy(\"month\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Range-based frame (useful for time-based windows)\n",
    "# Note: This would work better with actual date columns\n",
    "range_frame = Window.partitionBy(\"employee\").orderBy(\"month\").rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_frame_examples = df.withColumn(\n",
    "    \"current_revenue_only\", sum(\"revenue\").over(current_only)\n",
    ").withColumn(\n",
    "    \"running_total_explicit\", sum(\"revenue\").over(running_total_frame)\n",
    ").withColumn(\n",
    "    \"count_records_so_far\", count(\"*\").over(running_total_frame)\n",
    ")\n",
    "\n",
    "print(\"Different window frame examples:\")\n",
    "df_frame_examples.select(\"employee\", \"month\", \"revenue\", \n",
    "                        \"current_revenue_only\", \"running_total_explicit\", \"count_records_so_far\") \\\n",
    "                 .orderBy(\"employee\", \"month\") \\\n",
    "                 .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WINDOW FUNCTIONS KEY CONCEPTS:\")\n",
    "print(\"• Window = Partition (group) + Order + Frame (which rows to include)\")\n",
    "print(\"• Partition: Which records to group together\")\n",
    "print(\"• Order: How to sort within each partition\") \n",
    "print(\"• Frame: Which rows to include in calculation (default is unbounded preceding to current)\")\n",
    "print(\"• Ranking: rank(), dense_rank(), row_number(), ntile()\")\n",
    "print(\"• Analytics: lag(), lead(), first_value(), last_value()\")\n",
    "print(\"• Aggregates: sum(), avg(), count(), etc. over windows\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da041f56",
   "metadata": {},
   "source": [
    "### **Advanced Data Manipulation Techniques**\n",
    "\n",
    "Now let's explore some sophisticated data manipulation techniques that you'll use in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43672dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Manipulation in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# Create complex sample data for demonstration\n",
    "complex_data = [\n",
    "    (1, \"Alice Johnson\", \"alice.johnson@company.com\", \"2023-01-15\", \n",
    "     \"{'skills': ['Python', 'SQL', 'Machine Learning'], 'years_exp': 5, 'certifications': ['AWS', 'GCP']}\", \n",
    "     \"Engineering,Data Science\", \"New York,San Francisco\"),\n",
    "    (2, \"Bob Smith\", \"bob.smith@company.com\", \"2023-02-20\",\n",
    "     \"{'skills': ['Java', 'Spring', 'Microservices'], 'years_exp': 8, 'certifications': ['Oracle', 'Spring']}\", \n",
    "     \"Engineering,Architecture\", \"Chicago\"),\n",
    "    (3, \"Carol Davis\", \"carol.davis@company.com\", \"2023-03-10\",\n",
    "     \"{'skills': ['Marketing', 'Analytics', 'SEO'], 'years_exp': 6, 'certifications': ['Google Analytics', 'HubSpot']}\", \n",
    "     \"Marketing,Growth\", \"Los Angeles,Austin\"),\n",
    "    (4, \"David Wilson\", \"david.wilson@company.com\", \"2023-01-05\",\n",
    "     \"{'skills': ['Finance', 'Excel', 'PowerBI'], 'years_exp': 10, 'certifications': ['CPA', 'CFA']}\", \n",
    "     \"Finance,Analytics\", \"Boston\")\n",
    "]\n",
    "\n",
    "columns = [\"emp_id\", \"full_name\", \"email\", \"hire_date\", \"profile_json\", \"departments\", \"locations\"]\n",
    "df = spark.createDataFrame(complex_data, columns)\n",
    "\n",
    "print(\"Original Complex Dataset:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 1. STRING MANIPULATION AND EXTRACTION\n",
    "print(\"1. Advanced String Operations\")\n",
    "\n",
    "df_string_ops = df.withColumn(\n",
    "    # Extract first and last names\n",
    "    \"first_name\", split(col(\"full_name\"), \" \").getItem(0)\n",
    ").withColumn(\n",
    "    \"last_name\", split(col(\"full_name\"), \" \").getItem(1)\n",
    ").withColumn(\n",
    "    # Extract domain from email\n",
    "    \"email_domain\", regexp_extract(col(\"email\"), r\"@(.+)\", 1)\n",
    ").withColumn(\n",
    "    # Clean and standardize email\n",
    "    \"email_clean\", lower(trim(col(\"email\")))\n",
    ").withColumn(\n",
    "    # Extract year from hire date\n",
    "    \"hire_year\", year(to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    ").withColumn(\n",
    "    # Create initials\n",
    "    \"initials\", concat(\n",
    "        substring(col(\"first_name\"), 1, 1),\n",
    "        lit(\".\"),\n",
    "        substring(col(\"last_name\"), 1, 1),\n",
    "        lit(\".\")\n",
    "    )\n",
    ").withColumn(\n",
    "    # Check if name contains specific pattern\n",
    "    \"has_common_lastname\", when(col(\"last_name\").rlike(\"Smith|Johnson|Davis\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "print(\"String manipulation results:\")\n",
    "df_string_ops.select(\"full_name\", \"first_name\", \"last_name\", \"email_domain\", \n",
    "                    \"initials\", \"hire_year\", \"has_common_lastname\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 2. WORKING WITH ARRAYS AND COMPLEX DATA\n",
    "print(\"2. Array and Complex Data Operations\")\n",
    "\n",
    "# Split comma-separated values into arrays\n",
    "df_arrays = df.withColumn(\n",
    "    \"departments_array\", split(col(\"departments\"), \",\")\n",
    ").withColumn(\n",
    "    \"locations_array\", split(col(\"locations\"), \",\")\n",
    ").withColumn(\n",
    "    # Get array size\n",
    "    \"num_departments\", size(split(col(\"departments\"), \",\"))\n",
    ").withColumn(\n",
    "    \"num_locations\", size(split(col(\"locations\"), \",\"))\n",
    ").withColumn(\n",
    "    # Check if array contains specific value\n",
    "    \"works_in_engineering\", array_contains(split(col(\"departments\"), \",\"), \"Engineering\")\n",
    ").withColumn(\n",
    "    # Get first element\n",
    "    \"primary_department\", split(col(\"departments\"), \",\").getItem(0)\n",
    ").withColumn(\n",
    "    # Get all except first element (if more than one)\n",
    "    \"secondary_departments\", \n",
    "    when(size(split(col(\"departments\"), \",\")) > 1,\n",
    "         slice(split(col(\"departments\"), \",\"), 2, 10))\n",
    "    .otherwise(array())\n",
    ")\n",
    "\n",
    "print(\"Array operations:\")\n",
    "df_arrays.select(\"emp_id\", \"full_name\", \"departments_array\", \"num_departments\", \n",
    "                \"works_in_engineering\", \"primary_department\", \"secondary_departments\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 3. WORKING WITH JSON DATA\n",
    "print(\"3. JSON Data Extraction\")\n",
    "\n",
    "# Parse JSON strings to extract structured data\n",
    "df_json = df.withColumn(\n",
    "    # Parse JSON string to struct\n",
    "    \"profile_struct\", from_json(col(\"profile_json\"), \n",
    "        StructType([\n",
    "            StructField(\"skills\", ArrayType(StringType()), True),\n",
    "            StructField(\"years_exp\", IntegerType(), True),\n",
    "            StructField(\"certifications\", ArrayType(StringType()), True)\n",
    "        ])\n",
    "    )\n",
    ").withColumn(\n",
    "    # Extract specific fields from JSON\n",
    "    \"skills_array\", col(\"profile_struct.skills\")\n",
    ").withColumn(\n",
    "    \"years_experience\", col(\"profile_struct.years_exp\")\n",
    ").withColumn(\n",
    "    \"certifications_array\", col(\"profile_struct.certifications\")\n",
    ").withColumn(\n",
    "    # Work with extracted arrays\n",
    "    \"num_skills\", size(col(\"profile_struct.skills\"))\n",
    ").withColumn(\n",
    "    \"num_certifications\", size(col(\"profile_struct.certifications\"))\n",
    ").withColumn(\n",
    "    \"has_python_skill\", array_contains(col(\"profile_struct.skills\"), \"Python\")\n",
    ").withColumn(\n",
    "    \"primary_skill\", col(\"profile_struct.skills\").getItem(0)\n",
    ")\n",
    "\n",
    "print(\"JSON extraction results:\")\n",
    "df_json.select(\"emp_id\", \"full_name\", \"skills_array\", \"years_experience\", \n",
    "              \"num_skills\", \"has_python_skill\", \"primary_skill\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 4. CONDITIONAL LOGIC AND CASE STATEMENTS\n",
    "print(\"4. Complex Conditional Logic\")\n",
    "\n",
    "df_conditional = df_json.withColumn(\n",
    "    # Multi-level conditional logic\n",
    "    \"experience_level\",\n",
    "    when(col(\"years_experience\") < 3, \"Junior\")\n",
    "    .when(col(\"years_experience\") < 7, \"Mid-Level\") \n",
    "    .when(col(\"years_experience\") < 10, \"Senior\")\n",
    "    .otherwise(\"Principal\")\n",
    ").withColumn(\n",
    "    # Complex conditions with AND/OR\n",
    "    \"tech_leader\",\n",
    "    when(\n",
    "        (col(\"years_experience\") >= 5) & \n",
    "        (array_contains(col(\"skills_array\"), \"Python\") | array_contains(col(\"skills_array\"), \"Java\")) &\n",
    "        (size(col(\"certifications_array\")) >= 2),\n",
    "        \"Yes\"\n",
    "    ).otherwise(\"No\")\n",
    ").withColumn(\n",
    "    # Nested conditions\n",
    "    \"employee_category\",\n",
    "    when(col(\"works_in_engineering\") & (col(\"years_experience\") >= 5), \"Senior Engineer\")\n",
    "    .when(col(\"works_in_engineering\") & (col(\"years_experience\") < 5), \"Junior Engineer\")\n",
    "    .when(~col(\"works_in_engineering\") & (col(\"years_experience\") >= 8), \"Senior Non-Tech\")\n",
    "    .otherwise(\"Junior Non-Tech\")\n",
    ").withColumn(\n",
    "    # Using case with multiple columns\n",
    "    \"compensation_band\",\n",
    "    when((col(\"experience_level\") == \"Principal\") | (col(\"tech_leader\") == \"Yes\"), \"Band A\")\n",
    "    .when(col(\"experience_level\") == \"Senior\", \"Band B\")\n",
    "    .when(col(\"experience_level\") == \"Mid-Level\", \"Band C\")\n",
    "    .otherwise(\"Band D\")\n",
    ")\n",
    "\n",
    "print(\"Conditional logic results:\")\n",
    "df_conditional.select(\"full_name\", \"years_experience\", \"experience_level\", \n",
    "                     \"tech_leader\", \"employee_category\", \"compensation_band\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 5. DATA TYPE CONVERSIONS AND CLEANING\n",
    "print(\"5. Data Type Conversions and Cleaning\")\n",
    "\n",
    "# Create some messy data for cleaning demonstration\n",
    "messy_data = [\n",
    "    (\"  John Doe  \", \"123.45\", \"2023-01-15T10:30:00\", \"true\", \"NULL\"),\n",
    "    (\"Jane Smith\", \"67.8\", \"2023-02-20T14:15:30\", \"false\", \"\"),\n",
    "    (\"Bob Johnson\", \"invalid_number\", \"invalid_date\", \"maybe\", \"N/A\"),\n",
    "    (\"\", \"0.00\", \"2023-03-10T09:45:00\", \"1\", \"none\")\n",
    "]\n",
    "\n",
    "messy_df = spark.createDataFrame(messy_data, [\"name\", \"score\", \"timestamp\", \"active\", \"notes\"])\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "messy_df.show()\n",
    "\n",
    "# Clean and convert the data\n",
    "cleaned_df = messy_df.withColumn(\n",
    "    # Clean strings\n",
    "    \"name_clean\", trim(col(\"name\"))\n",
    ").withColumn(\n",
    "    # Handle empty strings\n",
    "    \"name_final\", when(trim(col(\"name\")) == \"\", lit(\"Unknown\")).otherwise(trim(col(\"name\")))\n",
    ").withColumn(\n",
    "    # Safe numeric conversion\n",
    "    \"score_numeric\", \n",
    "    when(col(\"score\").rlike(r\"^\\d+\\.?\\d*$\"), col(\"score\").cast(DoubleType()))\n",
    "    .otherwise(lit(0.0))\n",
    ").withColumn(\n",
    "    # Safe date conversion\n",
    "    \"timestamp_clean\",\n",
    "    when(col(\"timestamp\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}$\"),\n",
    "         to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "    .otherwise(lit(None).cast(TimestampType()))\n",
    ").withColumn(\n",
    "    # Standardize boolean values\n",
    "    \"active_boolean\",\n",
    "    when(lower(col(\"active\")).isin(\"true\", \"1\", \"yes\"), lit(True))\n",
    "    .when(lower(col(\"active\")).isin(\"false\", \"0\", \"no\"), lit(False))\n",
    "    .otherwise(lit(None).cast(BooleanType()))\n",
    ").withColumn(\n",
    "    # Handle various null representations\n",
    "    \"notes_clean\",\n",
    "    when(lower(trim(col(\"notes\"))).isin(\"null\", \"\", \"n/a\", \"none\"), lit(None))\n",
    "    .otherwise(col(\"notes\"))\n",
    ")\n",
    "\n",
    "print(\"Cleaned data:\")\n",
    "cleaned_df.show()\n",
    "print(\"Schema after cleaning:\")\n",
    "cleaned_df.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 6. EXPLODING ARRAYS - Converting arrays to separate rows\n",
    "print(\"6. Exploding Arrays - Array to Rows Conversion\")\n",
    "\n",
    "# Using our earlier data with arrays\n",
    "df_to_explode = df_json.select(\"emp_id\", \"full_name\", \"skills_array\", \"certifications_array\")\n",
    "\n",
    "# Explode skills array - each skill becomes a separate row\n",
    "df_skills_exploded = df_to_explode.select(\n",
    "    \"emp_id\", \"full_name\", \n",
    "    explode(\"skills_array\").alias(\"individual_skill\")\n",
    ")\n",
    "\n",
    "print(\"Skills exploded to separate rows:\")\n",
    "df_skills_exploded.show()\n",
    "\n",
    "# Explode with position to keep track of array index\n",
    "df_skills_with_pos = df_to_explode.select(\n",
    "    \"emp_id\", \"full_name\",\n",
    "    posexplode(\"skills_array\").alias(\"skill_position\", \"individual_skill\")\n",
    ")\n",
    "\n",
    "print(\"Skills exploded with positions:\")\n",
    "df_skills_with_pos.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 7. PIVOT AND UNPIVOT OPERATIONS\n",
    "print(\"7. Pivot and Unpivot Operations\")\n",
    "\n",
    "# Create data suitable for pivoting\n",
    "performance_data = [\n",
    "    (\"Alice\", \"Q1\", 85, 12),\n",
    "    (\"Alice\", \"Q2\", 92, 15),\n",
    "    (\"Alice\", \"Q3\", 88, 13),\n",
    "    (\"Bob\", \"Q1\", 78, 10),\n",
    "    (\"Bob\", \"Q2\", 85, 14),\n",
    "    (\"Bob\", \"Q3\", 90, 16),\n",
    "    (\"Carol\", \"Q1\", 95, 18),\n",
    "    (\"Carol\", \"Q2\", 88, 16),\n",
    "    (\"Carol\", \"Q3\", 92, 19)\n",
    "]\n",
    "\n",
    "perf_df = spark.createDataFrame(performance_data, [\"employee\", \"quarter\", \"score\", \"sales\"])\n",
    "\n",
    "print(\"Original performance data:\")\n",
    "perf_df.show()\n",
    "\n",
    "# Pivot to make quarters into columns\n",
    "pivoted_perf = perf_df.groupBy(\"employee\") \\\n",
    "                     .pivot(\"quarter\") \\\n",
    "                     .agg(first(\"score\").alias(\"score\"), \n",
    "                          first(\"sales\").alias(\"sales\"))\n",
    "\n",
    "print(\"Pivoted performance data:\")\n",
    "pivoted_perf.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 8. USER-DEFINED FUNCTIONS (UDFs)\n",
    "print(\"8. User-Defined Functions for Custom Logic\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define a complex function that would be hard to express with built-in functions\n",
    "def calculate_risk_score(years_exp, num_skills, num_certs):\n",
    "    \"\"\"Calculate employee risk score based on multiple factors\"\"\"\n",
    "    if years_exp is None or num_skills is None or num_certs is None:\n",
    "        return 0\n",
    "    \n",
    "    base_score = min(years_exp * 10, 100)  # Cap at 100\n",
    "    skill_bonus = min(num_skills * 5, 50)   # Cap at 50\n",
    "    cert_bonus = min(num_certs * 10, 30)    # Cap at 30\n",
    "    \n",
    "    total_score = base_score + skill_bonus + cert_bonus\n",
    "    \n",
    "    # Apply risk categories\n",
    "    if total_score >= 150:\n",
    "        return 1  # Low risk\n",
    "    elif total_score >= 100:\n",
    "        return 2  # Medium risk\n",
    "    else:\n",
    "        return 3  # High risk\n",
    "\n",
    "# Register UDF\n",
    "risk_score_udf = udf(calculate_risk_score, IntegerType())\n",
    "\n",
    "# Apply UDF\n",
    "df_with_risk = df_json.withColumn(\n",
    "    \"risk_score\",\n",
    "    risk_score_udf(col(\"years_experience\"), col(\"num_skills\"), col(\"num_certifications\"))\n",
    ").withColumn(\n",
    "    \"risk_category\",\n",
    "    when(col(\"risk_score\") == 1, \"Low Risk\")\n",
    "    .when(col(\"risk_score\") == 2, \"Medium Risk\")\n",
    "    .otherwise(\"High Risk\")\n",
    ")\n",
    "\n",
    "print(\"Data with custom risk scoring:\")\n",
    "df_with_risk.select(\"full_name\", \"years_experience\", \"num_skills\", \"num_certifications\",\n",
    "                   \"risk_score\", \"risk_category\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ADVANCED MANIPULATION KEY CONCEPTS:\")\n",
    "print(\"• String functions: split(), regexp_extract(), substring(), concat()\")\n",
    "print(\"• Array functions: array_contains(), size(), slice(), explode()\")\n",
    "print(\"• JSON functions: from_json(), to_json(), get_json_object()\")\n",
    "print(\"• Conditional logic: when().otherwise(), complex boolean conditions\")\n",
    "print(\"• Data cleaning: trim(), cast(), safe conversions with when()\")\n",
    "print(\"• UDFs: For complex logic that can't be expressed with built-in functions\")\n",
    "print(\"• Always prefer built-in functions over UDFs for performance\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73b097",
   "metadata": {},
   "source": [
    "### **What You've Just Mastered**\n",
    "\n",
    "- **Window Functions** are the secret weapon of advanced data analysts. They let you perform complex calculations like running totals, rankings, and comparisons without losing the detail of your original data. The key insight is understanding the three components: partition (which records to group), order (how to sort within groups), and frame (which rows to include in calculations).\n",
    "\n",
    "- **Advanced Data Manipulation** techniques let you handle real-world messy data. You'll rarely get clean, perfectly structured data in practice. These techniques - from JSON parsing to complex conditional logic to custom functions - are what transform you from someone who can work with toy datasets to someone who can handle production data systems.\n",
    "The most important concept here is that PySpark gives you both high-level abstractions (DataFrame operations) and the flexibility to handle complex scenarios (UDFs, complex transformations). The art is knowing when to use each approach - built-in functions are almost always faster and more reliable than custom code.\n",
    "\n",
    "- **Window functions**, in particular, solve a class of problems that would otherwise require complex self-joins or multiple passes through your data. They're essential for time-series analysis, ranking problems, and any scenario where you need to compare each row with its neighbors or with aggregate statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d0652",
   "metadata": {},
   "source": [
    "# **PySpark Advanced - Part 5: SQL Operations and Advanced Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84110719",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this chapter, we'll explore PySpark's SQL capabilities and advanced analytics features. PySpark provides seamless integration with SQL, allowing you to write SQL queries directly on DataFrames, use built-in statistical functions, and perform machine learning operations. This bridges the gap between traditional SQL users and big data processing.\n",
    "\n",
    "## 5.1 Spark SQL Fundamentals\n",
    "\n",
    "### Creating Temporary Views\n",
    "\n",
    "Before writing SQL queries, you need to create temporary views from your DataFrames:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQLOperations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "employees_data = [\n",
    "    (1, \"John\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "    (2, \"Sarah\", \"Marketing\", 65000, \"2019-03-20\"),\n",
    "    (3, \"Mike\", \"Engineering\", 80000, \"2021-06-10\"),\n",
    "    (4, \"Lisa\", \"HR\", 55000, \"2018-11-05\"),\n",
    "    (5, \"David\", \"Engineering\", 85000, \"2020-08-22\"),\n",
    "    (6, \"Emma\", \"Marketing\", 70000, \"2021-02-14\")\n",
    "]\n",
    "\n",
    "employees_schema = [\"id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# Create temporary view\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Now you can query with SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, \n",
    "           AVG(salary) as avg_salary,\n",
    "           COUNT(*) as employee_count\n",
    "    FROM employees \n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "```\n",
    "\n",
    "### Global Temporary Views\n",
    "\n",
    "Global temporary views are accessible across multiple Spark sessions:\n",
    "\n",
    "```python\n",
    "# Create global temporary view\n",
    "employees_df.createGlobalTempView(\"global_employees\")\n",
    "\n",
    "# Access global view (note the global_temp prefix)\n",
    "spark.sql(\"SELECT * FROM global_temp.global_employees\").show()\n",
    "```\n",
    "\n",
    "## 5.2 Complex SQL Queries\n",
    "\n",
    "### Subqueries and CTEs\n",
    "\n",
    "```python\n",
    "# Common Table Expressions (CTEs)\n",
    "complex_query = \"\"\"\n",
    "WITH department_stats AS (\n",
    "    SELECT \n",
    "        department,\n",
    "        AVG(salary) as avg_dept_salary,\n",
    "        MAX(salary) as max_dept_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "),\n",
    "high_earners AS (\n",
    "    SELECT *\n",
    "    FROM employees\n",
    "    WHERE salary > (SELECT AVG(salary) FROM employees)\n",
    ")\n",
    "SELECT \n",
    "    h.name,\n",
    "    h.department,\n",
    "    h.salary,\n",
    "    d.avg_dept_salary,\n",
    "    ROUND(h.salary / d.avg_dept_salary, 2) as salary_ratio\n",
    "FROM high_earners h\n",
    "JOIN department_stats d ON h.department = d.department\n",
    "ORDER BY salary_ratio DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(complex_query).show()\n",
    "```\n",
    "\n",
    "### Window Functions in SQL\n",
    "\n",
    "```python\n",
    "window_sql = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    department,\n",
    "    salary,\n",
    "    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank,\n",
    "    DENSE_RANK() OVER (ORDER BY salary DESC) as overall_rank,\n",
    "    LAG(salary) OVER (PARTITION BY department ORDER BY salary) as prev_salary,\n",
    "    salary - LAG(salary) OVER (PARTITION BY department ORDER BY salary) as salary_diff\n",
    "FROM employees\n",
    "ORDER BY department, salary DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(window_sql).show()\n",
    "```\n",
    "\n",
    "## 5.3 Advanced SQL Functions\n",
    "\n",
    "### Date and Time Functions\n",
    "\n",
    "```python\n",
    "# Add sample data with timestamps\n",
    "sales_data = [\n",
    "    (1, \"2023-01-15 10:30:00\", 1500.00),\n",
    "    (2, \"2023-02-20 14:45:00\", 2300.00),\n",
    "    (3, \"2023-03-10 09:15:00\", 1800.00),\n",
    "    (4, \"2023-04-05 16:20:00\", 2100.00),\n",
    "    (5, \"2023-05-12 11:10:00\", 1900.00)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"id\", \"timestamp\", \"amount\"])\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "date_functions_sql = \"\"\"\n",
    "SELECT \n",
    "    timestamp,\n",
    "    amount,\n",
    "    DATE(timestamp) as sale_date,\n",
    "    YEAR(timestamp) as sale_year,\n",
    "    MONTH(timestamp) as sale_month,\n",
    "    DAYOFWEEK(timestamp) as day_of_week,\n",
    "    HOUR(timestamp) as sale_hour,\n",
    "    DATE_FORMAT(timestamp, 'EEEE') as day_name,\n",
    "    DATEDIFF(CURRENT_DATE(), DATE(timestamp)) as days_ago,\n",
    "    ADD_MONTHS(DATE(timestamp), 3) as three_months_later\n",
    "FROM sales\n",
    "ORDER BY timestamp\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(date_functions_sql).show()\n",
    "```\n",
    "\n",
    "### String Functions\n",
    "\n",
    "```python\n",
    "# Sample text data\n",
    "text_data = [\n",
    "    (1, \"John Doe\", \"john.doe@email.com\"),\n",
    "    (2, \"SARAH SMITH\", \"sarah.smith@company.org\"),\n",
    "    (3, \"mike johnson\", \"mike.j@service.net\")\n",
    "]\n",
    "\n",
    "text_df = spark.createDataFrame(text_data, [\"id\", \"full_name\", \"email\"])\n",
    "text_df.createOrReplaceTempView(\"contacts\")\n",
    "\n",
    "string_functions_sql = \"\"\"\n",
    "SELECT \n",
    "    full_name,\n",
    "    email,\n",
    "    UPPER(full_name) as upper_name,\n",
    "    LOWER(full_name) as lower_name,\n",
    "    INITCAP(full_name) as proper_case,\n",
    "    LENGTH(full_name) as name_length,\n",
    "    SUBSTRING(email, 1, LOCATE('@', email) - 1) as username,\n",
    "    REGEXP_EXTRACT(email, '@(.+)', 1) as domain,\n",
    "    CONCAT(SUBSTRING(full_name, 1, 1), SUBSTRING(SPLIT(full_name, ' ')[1], 1, 1)) as initials,\n",
    "    REPLACE(full_name, ' ', '_') as underscore_name\n",
    "FROM contacts\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(string_functions_sql).show()\n",
    "```\n",
    "\n",
    "## 5.4 Statistical Functions\n",
    "\n",
    "### Basic Statistics\n",
    "\n",
    "```python\n",
    "# Generate sample numerical data\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "numerical_data = [(i, random.gauss(50, 15), random.uniform(0, 100)) \n",
    "                  for i in range(1, 101)]\n",
    "\n",
    "stats_df = spark.createDataFrame(numerical_data, [\"id\", \"normal_dist\", \"uniform_dist\"])\n",
    "stats_df.createOrReplaceTempView(\"statistics\")\n",
    "\n",
    "basic_stats_sql = \"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as count,\n",
    "    AVG(normal_dist) as mean_normal,\n",
    "    STDDEV(normal_dist) as stddev_normal,\n",
    "    VAR_POP(normal_dist) as variance_normal,\n",
    "    MIN(normal_dist) as min_normal,\n",
    "    MAX(normal_dist) as max_normal,\n",
    "    PERCENTILE_APPROX(normal_dist, 0.5) as median_normal,\n",
    "    PERCENTILE_APPROX(normal_dist, 0.25) as q1_normal,\n",
    "    PERCENTILE_APPROX(normal_dist, 0.75) as q3_normal,\n",
    "    SKEWNESS(normal_dist) as skewness_normal,\n",
    "    KURTOSIS(normal_dist) as kurtosis_normal\n",
    "FROM statistics\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(basic_stats_sql).show()\n",
    "```\n",
    "\n",
    "### Correlation and Covariance\n",
    "\n",
    "```python\n",
    "correlation_sql = \"\"\"\n",
    "SELECT \n",
    "    CORR(normal_dist, uniform_dist) as correlation,\n",
    "    COVAR_POP(normal_dist, uniform_dist) as covariance_pop,\n",
    "    COVAR_SAMP(normal_dist, uniform_dist) as covariance_samp\n",
    "FROM statistics\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(correlation_sql).show()\n",
    "```\n",
    "\n",
    "## 5.5 Advanced Analytics with SQL\n",
    "\n",
    "### Pivot Operations\n",
    "\n",
    "```python\n",
    "# Sample sales data by region and product\n",
    "pivot_data = [\n",
    "    (\"North\", \"ProductA\", 1000),\n",
    "    (\"North\", \"ProductB\", 1500),\n",
    "    (\"South\", \"ProductA\", 800),\n",
    "    (\"South\", \"ProductB\", 1200),\n",
    "    (\"East\", \"ProductA\", 1100),\n",
    "    (\"East\", \"ProductB\", 1300),\n",
    "    (\"West\", \"ProductA\", 900),\n",
    "    (\"West\", \"ProductB\", 1400)\n",
    "]\n",
    "\n",
    "pivot_df = spark.createDataFrame(pivot_data, [\"region\", \"product\", \"sales\"])\n",
    "pivot_df.createOrReplaceTempView(\"regional_sales\")\n",
    "\n",
    "pivot_sql = \"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT region, product, sales\n",
    "    FROM regional_sales\n",
    ")\n",
    "PIVOT (\n",
    "    SUM(sales)\n",
    "    FOR product IN ('ProductA' as ProductA, 'ProductB' as ProductB)\n",
    ")\n",
    "ORDER BY region\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(pivot_sql).show()\n",
    "```\n",
    "\n",
    "### Moving Averages and Cumulative Calculations\n",
    "\n",
    "```python\n",
    "# Time series data\n",
    "timeseries_data = [(i, f\"2023-01-{i:02d}\", random.randint(100, 1000)) \n",
    "                   for i in range(1, 16)]\n",
    "\n",
    "ts_df = spark.createDataFrame(timeseries_data, [\"day\", \"date\", \"value\"])\n",
    "ts_df.createOrReplaceTempView(\"timeseries\")\n",
    "\n",
    "moving_avg_sql = \"\"\"\n",
    "SELECT \n",
    "    date,\n",
    "    value,\n",
    "    AVG(value) OVER (\n",
    "        ORDER BY date \n",
    "        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "    ) as moving_avg_3day,\n",
    "    SUM(value) OVER (\n",
    "        ORDER BY date \n",
    "        ROWS UNBOUNDED PRECEDING\n",
    "    ) as cumulative_sum,\n",
    "    value - LAG(value) OVER (ORDER BY date) as daily_change,\n",
    "    ROUND(\n",
    "        (value - LAG(value) OVER (ORDER BY date)) * 100.0 / LAG(value) OVER (ORDER BY date), \n",
    "        2\n",
    "    ) as pct_change\n",
    "FROM timeseries\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(moving_avg_sql).show()\n",
    "```\n",
    "\n",
    "## 5.6 Combining SQL with DataFrame API\n",
    "\n",
    "You can seamlessly mix SQL queries with DataFrame operations:\n",
    "\n",
    "```python\n",
    "# Start with SQL\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "\n",
    "# Continue with DataFrame API\n",
    "final_result = sql_result \\\n",
    "    .filter(col(\"avg_salary\") > 60000) \\\n",
    "    .withColumn(\"salary_category\", \n",
    "                when(col(\"avg_salary\") > 75000, \"High\")\n",
    "                .when(col(\"avg_salary\") > 65000, \"Medium\")\n",
    "                .otherwise(\"Low\")) \\\n",
    "    .orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "final_result.show()\n",
    "\n",
    "# Convert back to SQL view if needed\n",
    "final_result.createOrReplaceTempView(\"salary_categories\")\n",
    "```\n",
    "\n",
    "## 5.7 User Defined Functions (UDFs) in SQL\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a Python UDF\n",
    "def salary_grade(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"Senior\"\n",
    "    elif salary >= 65000:\n",
    "        return \"Mid\"\n",
    "    else:\n",
    "        return \"Junior\"\n",
    "\n",
    "# Register UDF for SQL use\n",
    "spark.udf.register(\"salary_grade\", salary_grade, StringType())\n",
    "\n",
    "# Use UDF in SQL\n",
    "udf_sql = \"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    salary,\n",
    "    salary_grade(salary) as grade,\n",
    "    department\n",
    "FROM employees\n",
    "ORDER BY salary DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(udf_sql).show()\n",
    "```\n",
    "\n",
    "## 5.8 Performance Optimization\n",
    "\n",
    "### Query Execution Plans\n",
    "\n",
    "```python\n",
    "# Analyze query execution plan\n",
    "query = \"\"\"\n",
    "SELECT e.name, e.salary, d.avg_salary\n",
    "FROM employees e\n",
    "JOIN (\n",
    "    SELECT department, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    ") d ON e.department = d.department\n",
    "WHERE e.salary > d.avg_salary\n",
    "\"\"\"\n",
    "\n",
    "# Show execution plan\n",
    "spark.sql(query).explain(True)\n",
    "\n",
    "# Execute the query\n",
    "spark.sql(query).show()\n",
    "```\n",
    "\n",
    "### Caching Temporary Views\n",
    "\n",
    "```python\n",
    "# Cache frequently used views\n",
    "spark.sql(\"CACHE TABLE employees\")\n",
    "\n",
    "# Check if table is cached\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Uncache when done\n",
    "spark.sql(\"UNCACHE TABLE employees\")\n",
    "```\n",
    "\n",
    "## 5.9 Practical Examples\n",
    "\n",
    "### Data Quality Checks\n",
    "\n",
    "```python\n",
    "data_quality_sql = \"\"\"\n",
    "SELECT \n",
    "    'employees' as table_name,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT id) as unique_ids,\n",
    "    COUNT(*) - COUNT(DISTINCT id) as duplicate_ids,\n",
    "    SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END) as null_names,\n",
    "    SUM(CASE WHEN salary <= 0 THEN 1 ELSE 0 END) as invalid_salaries,\n",
    "    MIN(salary) as min_salary,\n",
    "    MAX(salary) as max_salary,\n",
    "    COUNT(DISTINCT department) as unique_departments\n",
    "FROM employees\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(data_quality_sql).show()\n",
    "```\n",
    "\n",
    "### Business Intelligence Queries\n",
    "\n",
    "```python\n",
    "bi_report_sql = \"\"\"\n",
    "SELECT \n",
    "    department,\n",
    "    COUNT(*) as headcount,\n",
    "    ROUND(AVG(salary), 2) as avg_salary,\n",
    "    ROUND(STDDEV(salary), 2) as salary_stddev,\n",
    "    MIN(salary) as min_salary,\n",
    "    MAX(salary) as max_salary,\n",
    "    ROUND(SUM(salary), 2) as total_payroll,\n",
    "    ROUND(AVG(DATEDIFF(CURRENT_DATE(), hire_date)) / 365.25, 1) as avg_tenure_years\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY total_payroll DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(bi_report_sql).show()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter, we covered:\n",
    "- Creating and using temporary views for SQL operations\n",
    "- Writing complex SQL queries with CTEs and subqueries\n",
    "- Advanced SQL functions for dates, strings, and statistics\n",
    "- Pivot operations and time series analysis\n",
    "- Combining SQL with DataFrame API\n",
    "- User Defined Functions in SQL\n",
    "- Performance optimization techniques\n",
    "- Practical business intelligence examples\n",
    "\n",
    "The ability to use SQL with PySpark makes it accessible to SQL users while providing the power of distributed computing. This hybrid approach allows teams to leverage existing SQL knowledge while scaling to big data workloads.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next chapter, we'll explore PySpark's machine learning capabilities with MLlib, including data preprocessing, feature engineering, and building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758891ed",
   "metadata": {},
   "source": [
    "# **PySpark Advanced - Part 6: Machine Learning with MLlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd55a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "PySpark's MLlib is a scalable machine learning library that provides high-level APIs for common machine learning algorithms and utilities. It's designed to work seamlessly with Spark's distributed computing capabilities, making it ideal for large-scale machine learning tasks. In this chapter, we'll explore data preprocessing, feature engineering, and building various types of machine learning models.\n",
    "\n",
    "## 6.1 MLlib Fundamentals\n",
    "\n",
    "### Setting Up MLlib Environment\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.clustering import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark session with MLlib\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLlibTutorial\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "```\n",
    "\n",
    "### Understanding MLlib Data Types\n",
    "\n",
    "MLlib uses specific data structures:\n",
    "- **Vector**: Dense or sparse vectors for features\n",
    "- **LabeledPoint**: Combines label and features\n",
    "- **DataFrame**: Primary data structure for ML pipelines\n",
    "\n",
    "```python\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "# Create sample data with vectors\n",
    "data = [\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]), 1.0),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]), 0.0),\n",
    "    (2, Vectors.dense([13.0, 10.1, 3.0]), 0.0),\n",
    "    (3, Vectors.sparse(3, {0: 1.0, 2: -2.0}), 1.0)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True),\n",
    "    StructField(\"label\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "## 6.2 Data Preprocessing and Feature Engineering\n",
    "\n",
    "### Loading and Exploring Data\n",
    "\n",
    "```python\n",
    "# Create a comprehensive dataset for demonstration\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Generate synthetic customer data\n",
    "customer_data = []\n",
    "for i in range(1000):\n",
    "    age = random.randint(18, 80)\n",
    "    income = random.randint(20000, 150000)\n",
    "    spending_score = random.randint(1, 100)\n",
    "    # Create a simple relationship for classification\n",
    "    high_value = 1 if (income > 80000 and spending_score > 50) else 0\n",
    "    \n",
    "    customer_data.append((\n",
    "        i, age, income, spending_score, \n",
    "        random.choice(['Male', 'Female']),\n",
    "        random.choice(['Single', 'Married', 'Divorced']),\n",
    "        random.choice(['High School', 'Bachelor', 'Master', 'PhD']),\n",
    "        high_value\n",
    "    ))\n",
    "\n",
    "schema = [\"customer_id\", \"age\", \"income\", \"spending_score\", \n",
    "          \"gender\", \"marital_status\", \"education\", \"high_value_customer\"]\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, schema)\n",
    "\n",
    "# Basic exploration\n",
    "print(\"Dataset shape:\", customer_df.count(), len(customer_df.columns))\n",
    "customer_df.describe().show()\n",
    "customer_df.groupBy(\"high_value_customer\").count().show()\n",
    "```\n",
    "\n",
    "### Handling Categorical Variables\n",
    "\n",
    "```python\n",
    "# String Indexing\n",
    "gender_indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
    "marital_indexer = StringIndexer(inputCol=\"marital_status\", outputCol=\"marital_index\")\n",
    "education_indexer = StringIndexer(inputCol=\"education\", outputCol=\"education_index\")\n",
    "\n",
    "# One-Hot Encoding\n",
    "gender_encoder = OneHotEncoder(inputCol=\"gender_index\", outputCol=\"gender_vec\")\n",
    "marital_encoder = OneHotEncoder(inputCol=\"marital_index\", outputCol=\"marital_vec\")\n",
    "education_encoder = OneHotEncoder(inputCol=\"education_index\", outputCol=\"education_vec\")\n",
    "\n",
    "# Apply transformations\n",
    "indexed_df = gender_indexer.fit(customer_df).transform(customer_df)\n",
    "indexed_df = marital_indexer.fit(indexed_df).transform(indexed_df)\n",
    "indexed_df = education_indexer.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "encoded_df = gender_encoder.fit(indexed_df).transform(indexed_df)\n",
    "encoded_df = marital_encoder.fit(encoded_df).transform(encoded_df)\n",
    "encoded_df = education_encoder.fit(encoded_df).transform(encoded_df)\n",
    "\n",
    "encoded_df.select(\"gender\", \"gender_index\", \"gender_vec\").show(5, truncate=False)\n",
    "```\n",
    "\n",
    "### Feature Scaling and Normalization\n",
    "\n",
    "```python\n",
    "# Vector Assembler to combine features\n",
    "feature_cols = [\"age\", \"income\", \"spending_score\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\", \n",
    "                       withStd=True, withMean=True)\n",
    "\n",
    "# Min-Max Scaler\n",
    "min_max_scaler = MinMaxScaler(inputCol=\"raw_features\", outputCol=\"minmax_features\")\n",
    "\n",
    "# Normalizer (L2 norm)\n",
    "normalizer = Normalizer(inputCol=\"raw_features\", outputCol=\"normalized_features\", p=2.0)\n",
    "\n",
    "# Apply transformations\n",
    "assembled_df = assembler.transform(encoded_df)\n",
    "scaled_df = scaler.fit(assembled_df).transform(assembled_df)\n",
    "minmax_df = min_max_scaler.fit(scaled_df).transform(scaled_df)\n",
    "final_df = normalizer.transform(minmax_df)\n",
    "\n",
    "# Compare different scaling methods\n",
    "final_df.select(\"raw_features\", \"scaled_features\", \"minmax_features\", \"normalized_features\").show(5, truncate=False)\n",
    "```\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "```python\n",
    "# Chi-Square Feature Selection\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "# Prepare features for selection\n",
    "all_features = [\"age\", \"income\", \"spending_score\", \"gender_index\", \"marital_index\", \"education_index\"]\n",
    "feature_assembler = VectorAssembler(inputCols=all_features, outputCol=\"all_features\")\n",
    "feature_df = feature_assembler.transform(encoded_df)\n",
    "\n",
    "# Chi-Square selection\n",
    "chi_selector = ChiSqSelector(numTopFeatures=4, featuresCol=\"all_features\", \n",
    "                            outputCol=\"selected_features\", labelCol=\"high_value_customer\")\n",
    "\n",
    "chi_model = chi_selector.fit(feature_df)\n",
    "selected_df = chi_model.transform(feature_df)\n",
    "\n",
    "print(\"Selected features indices:\", chi_model.selectedFeatures)\n",
    "selected_df.select(\"selected_features\", \"high_value_customer\").show(5, truncate=False)\n",
    "```\n",
    "\n",
    "## 6.3 Classification Models\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "```python\n",
    "# Prepare data for classification\n",
    "train_data, test_data = selected_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training data: {train_data.count()} rows\")\n",
    "print(f\"Test data: {test_data.count()} rows\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"selected_features\", labelCol=\"high_value_customer\",\n",
    "                       maxIter=100, regParam=0.01)\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show results\n",
    "lr_predictions.select(\"high_value_customer\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Model coefficients and statistics\n",
    "print(\"Coefficients:\", lr_model.coefficients)\n",
    "print(\"Intercept:\", lr_model.intercept)\n",
    "\n",
    "# Training summary\n",
    "training_summary = lr_model.summary\n",
    "print(\"Training accuracy:\", training_summary.accuracy)\n",
    "print(\"Training AUC:\", training_summary.areaUnderROC)\n",
    "```\n",
    "\n",
    "### Decision Tree Classifier\n",
    "\n",
    "```python\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(featuresCol=\"selected_features\", labelCol=\"high_value_customer\")\n",
    "\n",
    "# Train the model\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Feature importance\n",
    "print(\"Feature Importance:\", dt_model.featureImportances)\n",
    "\n",
    "# Show some predictions\n",
    "dt_predictions.select(\"high_value_customer\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "```\n",
    "\n",
    "### Random Forest Classifier\n",
    "\n",
    "```python\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"selected_features\", labelCol=\"high_value_customer\",\n",
    "                           numTrees=100, maxDepth=5, seed=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Feature importance\n",
    "print(\"Feature Importance:\", rf_model.featureImportances)\n",
    "\n",
    "# Show predictions\n",
    "rf_predictions.select(\"high_value_customer\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "```\n",
    "\n",
    "### Gradient Boosted Trees\n",
    "\n",
    "```python\n",
    "# Gradient Boosted Trees\n",
    "gbt = GBTClassifier(featuresCol=\"selected_features\", labelCol=\"high_value_customer\",\n",
    "                   maxIter=100, maxDepth=5, seed=42)\n",
    "\n",
    "# Train the model\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Feature importance\n",
    "print(\"Feature Importance:\", gbt_model.featureImportances)\n",
    "\n",
    "# Show predictions\n",
    "gbt_predictions.select(\"high_value_customer\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "```\n",
    "\n",
    "## 6.4 Model Evaluation\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "```python\n",
    "# Binary Classification Evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"high_value_customer\", \n",
    "                                        rawPredictionCol=\"rawPrediction\",\n",
    "                                        metricName=\"areaUnderROC\")\n",
    "\n",
    "# Multiclass Classification Evaluator\n",
    "mc_evaluator = MulticlassClassificationEvaluator(labelCol=\"high_value_customer\", \n",
    "                                               predictionCol=\"prediction\")\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_predictions,\n",
    "    \"Decision Tree\": dt_predictions,\n",
    "    \"Random Forest\": rf_predictions,\n",
    "    \"Gradient Boosted Trees\": gbt_predictions\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, predictions in models.items():\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    accuracy = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"accuracy\"})\n",
    "    precision = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"f1\"})\n",
    "    \n",
    "    results.append((name, auc, accuracy, precision, recall, f1))\n",
    "\n",
    "# Display results\n",
    "results_df = spark.createDataFrame(results, \n",
    "                                 [\"Model\", \"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "results_df.show(truncate=False)\n",
    "```\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "```python\n",
    "# Create confusion matrix for best model (let's use Random Forest)\n",
    "def create_confusion_matrix(predictions):\n",
    "    # Get actual vs predicted\n",
    "    actual_predicted = predictions.select(\"high_value_customer\", \"prediction\").collect()\n",
    "    \n",
    "    # Count combinations\n",
    "    tp = sum(1 for row in actual_predicted if row[0] == 1.0 and row[1] == 1.0)\n",
    "    tn = sum(1 for row in actual_predicted if row[0] == 0.0 and row[1] == 0.0)\n",
    "    fp = sum(1 for row in actual_predicted if row[0] == 0.0 and row[1] == 1.0)\n",
    "    fn = sum(1 for row in actual_predicted if row[0] == 1.0 and row[1] == 0.0)\n",
    "    \n",
    "    return {\"TP\": tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}\n",
    "\n",
    "cm = create_confusion_matrix(rf_predictions)\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(f\"True Positives: {cm['TP']}\")\n",
    "print(f\"True Negatives: {cm['TN']}\")\n",
    "print(f\"False Positives: {cm['FP']}\")\n",
    "print(f\"False Negatives: {cm['FN']}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = cm['TP'] / (cm['TP'] + cm['FP']) if (cm['TP'] + cm['FP']) > 0 else 0\n",
    "recall = cm['TP'] / (cm['TP'] + cm['FN']) if (cm['TP'] + cm['FN']) > 0 else 0\n",
    "specificity = cm['TN'] / (cm['TN'] + cm['FP']) if (cm['TN'] + cm['FP']) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "```\n",
    "\n",
    "## 6.5 Regression Models\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "```python\n",
    "# Create regression dataset\n",
    "regression_data = []\n",
    "for i in range(1000):\n",
    "    x1 = random.uniform(0, 10)\n",
    "    x2 = random.uniform(-5, 5)\n",
    "    x3 = random.uniform(0, 1)\n",
    "    # y = 2*x1 + 3*x2 + x3 + noise\n",
    "    y = 2*x1 + 3*x2 + x3 + random.gauss(0, 0.5)\n",
    "    regression_data.append((i, x1, x2, x3, y))\n",
    "\n",
    "reg_df = spark.createDataFrame(regression_data, [\"id\", \"x1\", \"x2\", \"x3\", \"y\"])\n",
    "\n",
    "# Feature assembly\n",
    "reg_assembler = VectorAssembler(inputCols=[\"x1\", \"x2\", \"x3\"], outputCol=\"features\")\n",
    "reg_assembled = reg_assembler.transform(reg_df)\n",
    "\n",
    "# Split data\n",
    "reg_train, reg_test = reg_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Linear Regression\n",
    "lr_reg = LinearRegression(featuresCol=\"features\", labelCol=\"y\", \n",
    "                         maxIter=100, regParam=0.01, elasticNetParam=0.8)\n",
    "\n",
    "# Train model\n",
    "lr_reg_model = lr_reg.fit(reg_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_reg_predictions = lr_reg_model.transform(reg_test)\n",
    "\n",
    "# Show results\n",
    "lr_reg_predictions.select(\"y\", \"prediction\").show(10)\n",
    "\n",
    "# Model statistics\n",
    "print(\"Coefficients:\", lr_reg_model.coefficients)\n",
    "print(\"Intercept:\", lr_reg_model.intercept)\n",
    "print(\"RMSE:\", lr_reg_model.summary.rootMeanSquaredError)\n",
    "print(\"R-squared:\", lr_reg_model.summary.r2)\n",
    "```\n",
    "\n",
    "### Random Forest Regression\n",
    "\n",
    "```python\n",
    "# Random Forest Regression\n",
    "rf_reg = RandomForestRegressor(featuresCol=\"features\", labelCol=\"y\",\n",
    "                              numTrees=100, maxDepth=10, seed=42)\n",
    "\n",
    "# Train model\n",
    "rf_reg_model = rf_reg.fit(reg_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_reg_predictions = rf_reg_model.transform(reg_test)\n",
    "\n",
    "# Show results\n",
    "rf_reg_predictions.select(\"y\", \"prediction\").show(10)\n",
    "\n",
    "print(\"Feature Importance:\", rf_reg_model.featureImportances)\n",
    "```\n",
    "\n",
    "### Regression Evaluation\n",
    "\n",
    "```python\n",
    "# Regression Evaluator\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"y\", predictionCol=\"prediction\")\n",
    "\n",
    "# Evaluate both models\n",
    "lr_rmse = reg_evaluator.evaluate(lr_reg_predictions, {reg_evaluator.metricName: \"rmse\"})\n",
    "lr_mae = reg_evaluator.evaluate(lr_reg_predictions, {reg_evaluator.metricName: \"mae\"})\n",
    "lr_r2 = reg_evaluator.evaluate(lr_reg_predictions, {reg_evaluator.metricName: \"r2\"})\n",
    "\n",
    "rf_rmse = reg_evaluator.evaluate(rf_reg_predictions, {reg_evaluator.metricName: \"rmse\"})\n",
    "rf_mae = reg_evaluator.evaluate(rf_reg_predictions, {reg_evaluator.metricName: \"mae\"})\n",
    "rf_r2 = reg_evaluator.evaluate(rf_reg_predictions, {reg_evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"Linear Regression - RMSE:\", lr_rmse, \"MAE:\", lr_mae, \"R²:\", lr_r2)\n",
    "print(\"Random Forest - RMSE:\", rf_rmse, \"MAE:\", rf_mae, \"R²:\", rf_r2)\n",
    "```\n",
    "\n",
    "## 6.6 Clustering\n",
    "\n",
    "### K-Means Clustering\n",
    "\n",
    "```python\n",
    "# Prepare data for clustering (using customer data)\n",
    "clustering_assembler = VectorAssembler(inputCols=[\"age\", \"income\", \"spending_score\"], \n",
    "                                     outputCol=\"features\")\n",
    "clustering_df = clustering_assembler.transform(customer_df)\n",
    "\n",
    "# Scale features for clustering\n",
    "clustering_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaled_clustering_df = clustering_scaler.fit(clustering_df).transform(clustering_df)\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", k=4, seed=42)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = kmeans.fit(scaled_clustering_df)\n",
    "\n",
    "# Make predictions\n",
    "clustered_df = kmeans_model.transform(scaled_clustering_df)\n",
    "\n",
    "# Show cluster assignments\n",
    "clustered_df.select(\"customer_id\", \"age\", \"income\", \"spending_score\", \"cluster\").show(20)\n",
    "\n",
    "# Cluster centers\n",
    "print(\"Cluster Centers:\")\n",
    "for i, center in enumerate(kmeans_model.clusterCenters()):\n",
    "    print(f\"Cluster {i}: {center}\")\n",
    "\n",
    "# Cluster statistics\n",
    "clustered_df.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    avg(\"income\").alias(\"avg_income\"),\n",
    "    avg(\"spending_score\").alias(\"avg_spending\")\n",
    ").orderBy(\"cluster\").show()\n",
    "```\n",
    "\n",
    "### Determining Optimal Number of Clusters\n",
    "\n",
    "```python\n",
    "# Elbow method for optimal k\n",
    "silhouette_evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", \n",
    "                                         predictionCol=\"cluster\",\n",
    "                                         metricName=\"silhouette\")\n",
    "\n",
    "costs = []\n",
    "silhouettes = []\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans_temp = KMeans(featuresCol=\"scaled_features\", predictionCol=\"cluster\", \n",
    "                        k=k, seed=42)\n",
    "    model_temp = kmeans_temp.fit(scaled_clustering_df)\n",
    "    predictions_temp = model_temp.transform(scaled_clustering_df)\n",
    "    \n",
    "    cost = model_temp.summary.trainingCost\n",
    "    silhouette = silhouette_evaluator.evaluate(predictions_temp)\n",
    "    \n",
    "    costs.append(cost)\n",
    "    silhouettes.append(silhouette)\n",
    "    print(f\"k={k}, Cost={cost:.2f}, Silhouette={silhouette:.3f}\")\n",
    "\n",
    "# Find optimal k (highest silhouette score)\n",
    "optimal_k = k_values[silhouettes.index(max(silhouettes))]\n",
    "print(f\"Optimal k based on silhouette score: {optimal_k}\")\n",
    "```\n",
    "\n",
    "## 6.7 Model Pipelines\n",
    "\n",
    "### Creating ML Pipelines\n",
    "\n",
    "```python\n",
    "# Create a complete ML pipeline\n",
    "pipeline_stages = [\n",
    "    # Feature preprocessing\n",
    "    StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\"),\n",
    "    StringIndexer(inputCol=\"marital_status\", outputCol=\"marital_index\"),\n",
    "    OneHotEncoder(inputCol=\"gender_index\", outputCol=\"gender_vec\"),\n",
    "    OneHotEncoder(inputCol=\"marital_index\", outputCol=\"marital_vec\"),\n",
    "    \n",
    "    # Feature assembly\n",
    "    VectorAssembler(inputCols=[\"age\", \"income\", \"spending_score\", \"gender_vec\", \"marital_vec\"], \n",
    "                   outputCol=\"features\"),\n",
    "    \n",
    "    # Feature scaling\n",
    "    StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\"),\n",
    "    \n",
    "    # Machine learning model\n",
    "    RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"high_value_customer\",\n",
    "                          numTrees=100, seed=42, predictionCol=\"prediction\")\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Split data\n",
    "pipeline_train, pipeline_test = customer_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline_model = pipeline.fit(pipeline_train)\n",
    "\n",
    "# Make predictions\n",
    "pipeline_predictions = pipeline_model.transform(pipeline_test)\n",
    "\n",
    "# Evaluate\n",
    "pipeline_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_value_customer\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ").evaluate(pipeline_predictions)\n",
    "\n",
    "print(f\"Pipeline Accuracy: {pipeline_accuracy:.3f}\")\n",
    "\n",
    "# Show predictions\n",
    "pipeline_predictions.select(\"customer_id\", \"high_value_customer\", \"prediction\", \"probability\").show(10)\n",
    "```\n",
    "\n",
    "## 6.8 Hyperparameter Tuning\n",
    "\n",
    "### Grid Search with Cross Validation\n",
    "\n",
    "```python\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create a simpler pipeline for tuning\n",
    "tuning_pipeline = Pipeline(stages=[\n",
    "    VectorAssembler(inputCols=[\"age\", \"income\", \"spending_score\"], outputCol=\"features\"),\n",
    "    RandomForestClassifier(featuresCol=\"features\", labelCol=\"high_value_customer\", seed=42)\n",
    "])\n",
    "\n",
    "# Create parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(tuning_pipeline.getStages()[1].numTrees, [50, 100, 200]) \\\n",
    "    .addGrid(tuning_pipeline.getStages()[1].maxDepth, [3, 5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "# Create cross validator\n",
    "cv = CrossValidator(estimator=tuning_pipeline,\n",
    "                   estimatorParamMaps=param_grid,\n",
    "                   evaluator=BinaryClassificationEvaluator(labelCol=\"high_value_customer\"),\n",
    "                   numFolds=3,\n",
    "                   seed=42)\n",
    "\n",
    "# Fit cross validator\n",
    "cv_model = cv.fit(pipeline_train)\n",
    "\n",
    "# Best model predictions\n",
    "cv_predictions = cv_model.transform(pipeline_test)\n",
    "\n",
    "# Evaluate best model\n",
    "cv_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"high_value_customer\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ").evaluate(cv_predictions)\n",
    "\n",
    "print(f\"Cross-Validated Model Accuracy: {cv_accuracy:.3f}\")\n",
    "\n",
    "# Get best parameters\n",
    "best_model = cv_model.bestModel\n",
    "best_rf = best_model.stages[1]  # RandomForest is the second stage\n",
    "print(f\"Best parameters: numTrees={best_rf.getNumTrees()}, maxDepth={best_rf.getMaxDepth()}\")\n",
    "```\n",
    "\n",
    "## 6.9 Model Persistence\n",
    "\n",
    "### Saving and Loading Models\n",
    "\n",
    "```python\n",
    "# Save the pipeline model\n",
    "model_path = \"/tmp/customer_classification_model\"\n",
    "pipeline_model.write().overwrite().save(model_path)\n",
    "\n",
    "# Save individual model\n",
    "best_rf_path = \"/tmp/best_rf_model\"\n",
    "best_rf.write().overwrite().save(best_rf_path)\n",
    "\n",
    "print(f\"Models saved successfully\")\n",
    "\n",
    "# Load models (in a new session)\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "# Load pipeline model\n",
    "loaded_pipeline = PipelineModel.load(model_path)\n",
    "\n",
    "# Load individual model\n",
    "loaded_rf = RandomForestClassificationModel.load(best_rf_path)\n",
    "\n",
    "# Use loaded model for predictions\n",
    "loaded_predictions = loaded_pipeline.transform(pipeline_test.limit(10))\n",
    "loaded_predictions.select(\"customer_id\", \"prediction\", \"probability\").show()\n",
    "```\n",
    "\n",
    "## 6.10 Advanced Topics\n",
    "\n",
    "### Custom Transformers\n",
    "\n",
    "```python\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "class AgeGroupTransformer(Transformer, HasInputCol, HasOutputCol, \n",
    "                         DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(AgeGroupTransformer, self).__init__()\n",
    "        self._setDefault(inputCol=\"age\", outputCol=\"age_group\")\n",
    "        if inputCol is not None:\n",
    "            self.setInputCol(inputCol)\n",
    "        if outputCol is not None:\n",
    "            self.setOutputCol(outputCol)\n",
    "    \n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        input_col = self.getInputCol()\n",
    "        output_col = self.getOutputCol()\n",
    "        \n",
    "        return dataset.withColumn(output_col, \n",
    "                                when(col(input_col) < 30, \"Young\")\n",
    "                                .when(col(input_col) < 50, \"Middle\")\n",
    "                                .otherwise(\"Senior\"))\n",
    "\n",
    "# Use custom transformer\n",
    "age_transformer = AgeGroupTransformer(inputCol=\"age\", outputCol=\"age_group\")\n",
    "transformed_df = age_transformer.transform(customer_df)\n",
    "transformed_df.select(\"customer_id\", \"age\", \"age_group\").show(10)\n",
    "```\n",
    "\n",
    "### Feature Importance Analysis\n",
    "\n",
    "```python\n",
    "# Detailed feature importance analysis\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    importances = model.featureImportances.toArray()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_data = [(name, float(importance)) \n",
    "                      for name, importance in zip(feature_names, importances)]\n",
    "    \n",
    "    importance_df = spark.createDataFrame(importance_data, [\"feature\", \"importance\"])\n",
    "    return importance_df.orderBy(col(\"importance\").desc())\n",
    "\n",
    "# Analyze Random Forest feature importance\n",
    "feature_names = [\"age\", \"income\", \"spending_score\"]\n",
    "importance_analysis = analyze_feature_importance(rf_reg_model, feature_names)\n",
    "importance_analysis.show()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter, we covered:\n",
    "- MLlib fundamentals and data structures\n",
    "- Comprehensive data preprocessing and feature engineering\n",
    "- Classification models (Logistic Regression, Decision Trees, Random Forest, GBT)\n",
    "- Model evaluation and metrics\n",
    "- Regression models and evaluation\n",
    "- Clustering with K-Means\n",
    "- ML Pipelines for end-to-end workflows\n",
    "- Hyperparameter tuning with cross-validation\n",
    "- Model persistence and loading\n",
    "- Advanced topics including custom transformers\n",
    "\n",
    "MLlib provides a robust framework for scalable machine learning on big data. The pipeline approach ensures reproducible and maintainable ML workflows, while the distributed nature of Spark allows these models to scale to massive datasets.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next chapter, we'll explore PySpark Streaming for real-time data processing and analytics, including integration with various streaming sources and sinks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f92f4",
   "metadata": {},
   "source": [
    "# **PySpark Advanced - Part 7: Streaming with Structured Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a366ad4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Structured Streaming is Apache Spark's scalable and fault-tolerant stream processing engine built on the Spark SQL engine. It allows you to express streaming computations the same way you would express a batch computation on static data. This chapter covers real-time data processing, various streaming sources and sinks, windowing operations, and advanced streaming patterns.\n",
    "\n",
    "## 7.1 Structured Streaming Fundamentals\n",
    "\n",
    "### Understanding Streaming Concepts\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.streaming import *\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "import json\n",
    "import socket\n",
    "\n",
    "# Initialize Spark session for streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingTutorial\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Basic streaming concepts\n",
    "print(\"Structured Streaming Key Concepts:\")\n",
    "print(\"1. Input Source: Where streaming data comes from\")\n",
    "print(\"2. Query: Transformations applied to streaming data\")\n",
    "print(\"3. Result Table: Conceptual table updated with each trigger\")\n",
    "print(\"4. Output Sink: Where results are written\")\n",
    "print(\"5. Trigger: When to update the result\")\n",
    "```\n",
    "\n",
    "### Creating a Simple Socket Stream\n",
    "\n",
    "```python\n",
    "# Socket stream for testing (requires netcat or similar)\n",
    "def create_socket_stream_demo():\n",
    "    \"\"\"\n",
    "    Demo function to show socket streaming setup\n",
    "    Note: In practice, you'd run 'nc -lk 9999' in terminal first\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define schema for incoming data\n",
    "    lines_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"socket\") \\\n",
    "        .option(\"host\", \"localhost\") \\\n",
    "        .option(\"port\", 9999) \\\n",
    "        .load()\n",
    "    \n",
    "    # Simple word count transformation\n",
    "    words_df = lines_df.select(\n",
    "        explode(split(col(\"value\"), \" \")).alias(\"word\")\n",
    "    )\n",
    "    \n",
    "    word_counts = words_df.groupBy(\"word\").count()\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Example of setting up the query (don't run without socket server)\n",
    "print(\"Socket stream setup example:\")\n",
    "print(\"1. Start socket server: nc -lk 9999\")\n",
    "print(\"2. Create streaming DataFrame\")\n",
    "print(\"3. Apply transformations\")\n",
    "print(\"4. Start the query\")\n",
    "```\n",
    "\n",
    "## 7.2 File-Based Streaming Sources\n",
    "\n",
    "### Reading from File Streams\n",
    "\n",
    "```python\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create temporary directory for file streaming demo\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "input_dir = os.path.join(temp_dir, \"input\")\n",
    "output_dir = os.path.join(temp_dir, \"output\")\n",
    "checkpoint_dir = os.path.join(temp_dir, \"checkpoint\")\n",
    "\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Streaming directories created:\")\n",
    "print(f\"Input: {input_dir}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "print(f\"Checkpoint: {checkpoint_dir}\")\n",
    "\n",
    "# Define schema for structured data\n",
    "json_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create streaming DataFrame from JSON files\n",
    "json_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(json_schema) \\\n",
    "    .option(\"path\", input_dir) \\\n",
    "    .load()\n",
    "\n",
    "print(\"JSON stream created with schema:\")\n",
    "json_stream.printSchema()\n",
    "```\n",
    "\n",
    "### CSV File Streaming\n",
    "\n",
    "```python\n",
    "# CSV schema\n",
    "csv_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# CSV stream\n",
    "csv_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(csv_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"path\", input_dir) \\\n",
    "    .load()\n",
    "\n",
    "# Basic transformations on CSV stream\n",
    "enriched_stream = csv_stream \\\n",
    "    .withColumn(\"total_amount\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .withColumn(\"hour\", hour(col(\"order_timestamp\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"order_timestamp\")))\n",
    "\n",
    "print(\"CSV stream with enrichment created\")\n",
    "```\n",
    "\n",
    "## 7.3 Streaming Transformations\n",
    "\n",
    "### Basic Transformations\n",
    "\n",
    "```python\n",
    "# Sample streaming transformations using the JSON stream\n",
    "transformed_stream = json_stream \\\n",
    "    .filter(col(\"value\") > 0) \\\n",
    "    .withColumn(\"processed_time\", current_timestamp()) \\\n",
    "    .withColumn(\"value_category\", \n",
    "                when(col(\"value\") < 10, \"low\")\n",
    "                .when(col(\"value\") < 50, \"medium\")\n",
    "                .otherwise(\"high\")) \\\n",
    "    .select(\"user_id\", \"product_id\", \"action\", \"value\", \n",
    "            \"value_category\", \"processed_time\")\n",
    "\n",
    "print(\"Basic transformations applied to stream\")\n",
    "```\n",
    "\n",
    "### Aggregations in Streaming\n",
    "\n",
    "```python\n",
    "# Simple aggregations\n",
    "user_stats = json_stream \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"action_count\"),\n",
    "        avg(\"value\").alias(\"avg_value\"),\n",
    "        max(\"value\").alias(\"max_value\"),\n",
    "        sum(\"value\").alias(\"total_value\")\n",
    "    )\n",
    "\n",
    "# Action-based aggregations\n",
    "action_summary = json_stream \\\n",
    "    .groupBy(\"action\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"value\").alias(\"avg_value\")\n",
    "    )\n",
    "\n",
    "print(\"Aggregation streams created\")\n",
    "```\n",
    "\n",
    "## 7.4 Windowing Operations\n",
    "\n",
    "### Time-Based Windows\n",
    "\n",
    "```python\n",
    "# Tumbling windows (non-overlapping)\n",
    "tumbling_window_agg = json_stream \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\"),\n",
    "        col(\"action\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"value\").alias(\"avg_value\"),\n",
    "        sum(\"value\").alias(\"total_value\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"action\"),\n",
    "        col(\"count\"),\n",
    "        col(\"avg_value\"),\n",
    "        col(\"total_value\")\n",
    "    )\n",
    "\n",
    "print(\"Tumbling window aggregation created\")\n",
    "\n",
    "# Sliding windows (overlapping)\n",
    "sliding_window_agg = json_stream \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\"),\n",
    "        col(\"user_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"activity_count\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "        sum(\"value\").alias(\"total_spent\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"user_id\"),\n",
    "        col(\"activity_count\"),\n",
    "        col(\"unique_products\"),\n",
    "        col(\"total_spent\")\n",
    "    )\n",
    "\n",
    "print(\"Sliding window aggregation created\")\n",
    "```\n",
    "\n",
    "### Session Windows\n",
    "\n",
    "```python\n",
    "# Session-based grouping (simulate with custom logic)\n",
    "session_analysis = json_stream \\\n",
    "    .withWatermark(\"timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        col(\"user_id\"),\n",
    "        window(col(\"timestamp\"), \"2 minutes\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"actions_in_session\"),\n",
    "        collect_list(\"action\").alias(\"action_sequence\"),\n",
    "        min(\"timestamp\").alias(\"session_start\"),\n",
    "        max(\"timestamp\").alias(\"session_end\"),\n",
    "        sum(\"value\").alias(\"session_value\")\n",
    "    ) \\\n",
    "    .withColumn(\"session_duration\", \n",
    "                col(\"session_end\").cast(\"long\") - col(\"session_start\").cast(\"long\"))\n",
    "\n",
    "print(\"Session analysis stream created\")\n",
    "```\n",
    "\n",
    "## 7.5 Stateful Operations\n",
    "\n",
    "### Update Mode Operations\n",
    "\n",
    "```python\n",
    "# Running totals (stateful)\n",
    "running_totals = json_stream \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        sum(\"value\").alias(\"lifetime_value\"),\n",
    "        count(\"*\").alias(\"total_actions\"),\n",
    "        max(\"value\").alias(\"max_transaction\"),\n",
    "        avg(\"value\").alias(\"avg_transaction\")\n",
    "    )\n",
    "\n",
    "print(\"Running totals stream created\")\n",
    "```\n",
    "\n",
    "### Deduplication\n",
    "\n",
    "```python\n",
    "# Remove duplicate events based on key columns\n",
    "deduplicated_stream = json_stream \\\n",
    "    .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "    .dropDuplicates([\"user_id\", \"product_id\", \"timestamp\"])\n",
    "\n",
    "print(\"Deduplication stream created\")\n",
    "```\n",
    "\n",
    "## 7.6 Stream-Stream Joins\n",
    "\n",
    "### Inner Join Between Streams\n",
    "\n",
    "```python\n",
    "# Create two related streams for joining\n",
    "stream1_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"login_time\", TimestampType(), True),\n",
    "    StructField(\"session_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "stream2_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"purchase_time\", TimestampType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"session_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Simulate two streams (in practice, these would come from different sources)\n",
    "login_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(stream1_schema) \\\n",
    "    .option(\"path\", os.path.join(input_dir, \"logins\")) \\\n",
    "    .load()\n",
    "\n",
    "purchase_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(stream2_schema) \\\n",
    "    .option(\"path\", os.path.join(input_dir, \"purchases\")) \\\n",
    "    .load()\n",
    "\n",
    "# Stream-stream join with watermarks\n",
    "joined_stream = login_stream \\\n",
    "    .withWatermark(\"login_time\", \"5 minutes\") \\\n",
    "    .join(\n",
    "        purchase_stream.withWatermark(\"purchase_time\", \"5 minutes\"),\n",
    "        [\"user_id\", \"session_id\"],\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .where(\n",
    "        col(\"purchase_time\") >= col(\"login_time\")\n",
    "    ) \\\n",
    "    .where(\n",
    "        col(\"purchase_time\") <= col(\"login_time\") + interval(\"1 hour\")\n",
    "    )\n",
    "\n",
    "print(\"Stream-stream join created\")\n",
    "```\n",
    "\n",
    "## 7.7 Output Sinks\n",
    "\n",
    "### Console Sink\n",
    "\n",
    "```python\n",
    "def create_console_query(stream_df, query_name, output_mode=\"append\"):\n",
    "    \"\"\"Create a console output query\"\"\"\n",
    "    return stream_df.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .queryName(query_name) \\\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "\n",
    "# Example console queries\n",
    "console_query_complete = create_console_query(\n",
    "    user_stats, \"user_stats_console\", \"complete\"\n",
    ")\n",
    "\n",
    "console_query_append = create_console_query(\n",
    "    transformed_stream, \"transformed_console\", \"append\"\n",
    ")\n",
    "\n",
    "print(\"Console output queries created\")\n",
    "```\n",
    "\n",
    "### File Sink\n",
    "\n",
    "```python\n",
    "def create_file_query(stream_df, output_path, format_type=\"json\"):\n",
    "    \"\"\"Create a file output query\"\"\"\n",
    "    return stream_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(format_type) \\\n",
    "        .option(\"path\", output_path) \\\n",
    "        .option(\"checkpointLocation\", f\"{output_path}_checkpoint\") \\\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "\n",
    "# File output examples\n",
    "json_file_query = create_file_query(\n",
    "    transformed_stream, \n",
    "    os.path.join(output_dir, \"transformed_json\"),\n",
    "    \"json\"\n",
    ")\n",
    "\n",
    "parquet_file_query = create_file_query(\n",
    "    tumbling_window_agg,\n",
    "    os.path.join(output_dir, \"windowed_parquet\"),\n",
    "    \"parquet\"\n",
    ")\n",
    "\n",
    "print(\"File output queries created\")\n",
    "```\n",
    "\n",
    "### Memory Sink (for Testing)\n",
    "\n",
    "```python\n",
    "def create_memory_query(stream_df, table_name):\n",
    "    \"\"\"Create an in-memory table query for testing\"\"\"\n",
    "    return stream_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(table_name) \\\n",
    "        .trigger(processingTime=\"5 seconds\")\n",
    "\n",
    "# Memory sink for testing\n",
    "memory_query = create_memory_query(action_summary, \"action_summary_table\")\n",
    "\n",
    "print(\"Memory output query created\")\n",
    "```\n",
    "\n",
    "## 7.8 Trigger Types and Processing\n",
    "\n",
    "### Different Trigger Types\n",
    "\n",
    "```python\n",
    "# Micro-batch trigger (default)\n",
    "microbatch_trigger = transformed_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .outputMode(\"append\")\n",
    "\n",
    "# Continuous trigger (experimental)\n",
    "continuous_trigger = transformed_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(continuous=\"1 second\") \\\n",
    "    .outputMode(\"append\")\n",
    "\n",
    "# One-time trigger\n",
    "onetime_trigger = transformed_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .outputMode(\"append\")\n",
    "\n",
    "print(\"Different trigger types configured\")\n",
    "```\n",
    "\n",
    "### Query Management\n",
    "\n",
    "```python\n",
    "def manage_streaming_query(query):\n",
    "    \"\"\"Utility function to manage streaming queries\"\"\"\n",
    "    try:\n",
    "        # Start the query\n",
    "        streaming_query = query.start()\n",
    "        \n",
    "        print(f\"Query started: {streaming_query.name}\")\n",
    "        print(f\"Query ID: {streaming_query.id}\")\n",
    "        print(f\"Run ID: {streaming_query.runId}\")\n",
    "        \n",
    "        # Query status\n",
    "        print(f\"Is Active: {streaming_query.isActive}\")\n",
    "        \n",
    "        # Wait for a short time\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Get progress\n",
    "        progress = streaming_query.lastProgress\n",
    "        if progress:\n",
    "            print(\"Last Progress:\")\n",
    "            print(f\"  Batch ID: {progress.get('batchId', 'N/A')}\")\n",
    "            print(f\"  Input Rows: {progress.get('inputRowsPerSecond', 'N/A')}\")\n",
    "            print(f\"  Processing Time: {progress.get('durationMs', {}).get('triggerExecution', 'N/A')} ms\")\n",
    "        \n",
    "        # Stop the query\n",
    "        streaming_query.stop()\n",
    "        print(\"Query stopped successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error managing query: {e}\")\n",
    "\n",
    "print(\"Query management utilities ready\")\n",
    "```\n",
    "\n",
    "## 7.9 Advanced Streaming Patterns\n",
    "\n",
    "### Custom Stateful Processing\n",
    "\n",
    "```python\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "# Define state update function\n",
    "def update_user_state(key, values, state):\n",
    "    \"\"\"\n",
    "    Custom stateful processing function\n",
    "    \"\"\"\n",
    "    if state.hasTimedOut:\n",
    "        # Handle timeout\n",
    "        return None\n",
    "    \n",
    "    # Get current state or initialize\n",
    "    current_total = state.get if state.exists else 0.0\n",
    "    \n",
    "    # Process new values\n",
    "    new_total = sum(row.value for row in values) + current_total\n",
    "    \n",
    "    # Update state\n",
    "    state.update(new_total)\n",
    "    \n",
    "    # Set timeout\n",
    "    state.setTimeoutDuration(\"1 minute\")\n",
    "    \n",
    "    return (key[0], new_total)\n",
    "\n",
    "# Note: mapGroupsWithState requires Scala/Java implementation in practice\n",
    "print(\"Custom stateful processing pattern defined\")\n",
    "```\n",
    "\n",
    "### Complex Event Processing\n",
    "\n",
    "```python\n",
    "# Pattern detection in streams\n",
    "def detect_patterns(stream_df):\n",
    "    \"\"\"\n",
    "    Detect complex patterns in streaming data\n",
    "    \"\"\"\n",
    "    # Example: Detect users with unusual activity patterns\n",
    "    user_activity_pattern = stream_df \\\n",
    "        .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "        .groupBy(\n",
    "            col(\"user_id\"),\n",
    "            window(col(\"timestamp\"), \"5 minutes\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"activity_count\"),\n",
    "            countDistinct(\"action\").alias(\"unique_actions\"),\n",
    "            sum(\"value\").alias(\"total_value\")\n",
    "        ) \\\n",
    "        .withColumn(\"is_anomaly\",\n",
    "                   when((col(\"activity_count\") > 100) | \n",
    "                        (col(\"total_value\") > 1000), True)\n",
    "                   .otherwise(False)) \\\n",
    "        .filter(col(\"is_anomaly\") == True)\n",
    "    \n",
    "    return user_activity_pattern\n",
    "\n",
    "anomaly_detection = detect_patterns(json_stream)\n",
    "print(\"Anomaly detection pattern created\")\n",
    "```\n",
    "\n",
    "## 7.10 Error Handling and Monitoring\n",
    "\n",
    "### Query Exception Handling\n",
    "\n",
    "```python\n",
    "def robust_streaming_query(stream_df, output_path):\n",
    "    \"\"\"\n",
    "    Create a robust streaming query with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = stream_df.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"json\") \\\n",
    "            .option(\"path\", output_path) \\\n",
    "            .option(\"checkpointLocation\", f\"{output_path}_checkpoint\") \\\n",
    "            .trigger(processingTime=\"30 seconds\") \\\n",
    "            .start()\n",
    "        \n",
    "        # Monitor query\n",
    "        while query.isActive:\n",
    "            try:\n",
    "                # Check for exceptions\n",
    "                if query.exception():\n",
    "                    print(f\"Query exception: {query.exception()}\")\n",
    "                    break\n",
    "                \n",
    "                # Print progress\n",
    "                progress = query.lastProgress\n",
    "                if progress:\n",
    "                    print(f\"Processed batch {progress.get('batchId')}\")\n",
    "                \n",
    "                time.sleep(10)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Stopping query due to keyboard interrupt\")\n",
    "                query.stop()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during monitoring: {e}\")\n",
    "                break\n",
    "        \n",
    "        return query\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error starting query: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Robust query handling utilities ready\")\n",
    "```\n",
    "\n",
    "### Performance Monitoring\n",
    "\n",
    "```python\n",
    "def monitor_streaming_performance(query):\n",
    "    \"\"\"\n",
    "    Monitor streaming query performance\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"query_id\": query.id,\n",
    "        \"is_active\": query.isActive,\n",
    "        \"recent_progress\": []\n",
    "    }\n",
    "    \n",
    "    # Collect recent progress\n",
    "    for progress in query.recentProgress:\n",
    "        batch_metrics = {\n",
    "            \"batch_id\": progress.get(\"batchId\"),\n",
    "            \"timestamp\": progress.get(\"timestamp\"),\n",
    "            \"input_rows_per_second\": progress.get(\"inputRowsPerSecond\"),\n",
    "            \"processing_time_ms\": progress.get(\"durationMs\", {}).get(\"triggerExecution\"),\n",
    "            \"sources\": len(progress.get(\"sources\", [])),\n",
    "            \"sink\": progress.get(\"sink\", {}).get(\"description\")\n",
    "        }\n",
    "        metrics[\"recent_progress\"].append(batch_metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Performance monitoring utilities ready\")\n",
    "```\n",
    "\n",
    "## 7.11 Data Generator for Testing\n",
    "\n",
    "### Creating Test Data\n",
    "\n",
    "```python\n",
    "def generate_test_data(output_dir, num_files=5, records_per_file=100):\n",
    "    \"\"\"\n",
    "    Generate test JSON files for streaming\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    actions = [\"view\", \"click\", \"purchase\", \"add_to_cart\", \"remove_from_cart\"]\n",
    "    \n",
    "    for file_num in range(num_files):\n",
    "        file_path = os.path.join(output_dir, f\"data_{file_num}.json\")\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            for record_num in range(records_per_file):\n",
    "                timestamp = datetime.now() - timedelta(\n",
    "                    minutes=random.randint(0, 60),\n",
    "                    seconds=random.randint(0, 60)\n",
    "                )\n",
    "                \n",
    "                record = {\n",
    "                    \"timestamp\": timestamp.isoformat(),\n",
    "                    \"user_id\": random.randint(1, 100),\n",
    "                    \"product_id\": random.randint(1, 50),\n",
    "                    \"action\": random.choice(actions),\n",
    "                    \"value\": round(random.uniform(1, 100), 2)\n",
    "                }\n",
    "                \n",
    "                f.write(json.dumps(record) + '\\n')\n",
    "        \n",
    "        print(f\"Generated {file_path}\")\n",
    "        time.sleep(1)  # Small delay between files\n",
    "\n",
    "# Generate test data\n",
    "test_data_dir = os.path.join(input_dir, \"test_data\")\n",
    "os.makedirs(test_data_dir, exist_ok=True)\n",
    "\n",
    "print(\"Test data generator ready\")\n",
    "print(f\"Run: generate_test_data('{test_data_dir}') to create test files\")\n",
    "```\n",
    "\n",
    "## 7.12 Complete Streaming Application Example\n",
    "\n",
    "### End-to-End Streaming Pipeline\n",
    "\n",
    "```python\n",
    "def create_complete_streaming_app():\n",
    "    \"\"\"\n",
    "    Complete streaming application example\n",
    "    \"\"\"\n",
    "    print(\"Creating complete streaming application...\")\n",
    "    \n",
    "    # 1. Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"user_id\", IntegerType(), True),\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"action\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # 2. Create input stream\n",
    "    input_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"json\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"path\", input_dir) \\\n",
    "        .load()\n",
    "    \n",
    "    # 3. Data preprocessing\n",
    "    cleaned_stream = input_stream \\\n",
    "        .filter(col(\"value\") > 0) \\\n",
    "        .withColumn(\"processed_time\", current_timestamp()) \\\n",
    "        .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"value_category\",\n",
    "                   when(col(\"value\") < 20, \"low\")\n",
    "                   .when(col(\"value\") < 60, \"medium\")\n",
    "                   .otherwise(\"high\"))\n",
    "    \n",
    "    # 4. Real-time aggregations\n",
    "    hourly_stats = cleaned_stream \\\n",
    "        .withWatermark(\"timestamp\", \"1 hour\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"1 hour\"),\n",
    "            col(\"action\"),\n",
    "            col(\"value_category\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"count\"),\n",
    "            avg(\"value\").alias(\"avg_value\"),\n",
    "            sum(\"value\").alias(\"total_value\"),\n",
    "            countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "        )\n",
    "    \n",
    "    # 5. User behavior analysis\n",
    "    user_behavior = cleaned_stream \\\n",
    "        .withWatermark(\"timestamp\", \"30 minutes\") \\\n",
    "        .groupBy(\n",
    "            col(\"user_id\"),\n",
    "            window(col(\"timestamp\"), \"15 minutes\", \"5 minutes\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"activity_count\"),\n",
    "            countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "            sum(\"value\").alias(\"total_spent\"),\n",
    "            collect_list(\"action\").alias(\"action_sequence\")\n",
    "        ) \\\n",
    "        .withColumn(\"is_high_activity\", col(\"activity_count\") > 10)\n",
    "    \n",
    "    # 6. Output queries\n",
    "    queries = []\n",
    "    \n",
    "    # Console output for monitoring\n",
    "    console_query = hourly_stats.writeStream \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .queryName(\"hourly_stats_console\") \\\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "    \n",
    "    # File output for persistence\n",
    "    file_query = user_behavior.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", os.path.join(output_dir, \"user_behavior\")) \\\n",
    "        .option(\"checkpointLocation\", os.path.join(checkpoint_dir, \"user_behavior\")) \\\n",
    "        .trigger(processingTime=\"1 minute\")\n",
    "    \n",
    "    queries.extend([console_query, file_query])\n",
    "    \n",
    "    return queries\n",
    "\n",
    "print(\"Complete streaming application template ready\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter, we covered:\n",
    "- Structured Streaming fundamentals and concepts\n",
    "- Various input sources (files, sockets)\n",
    "- Streaming transformations and aggregations\n",
    "- Windowing operations (tumbling, sliding, session)\n",
    "- Stateful operations and deduplication\n",
    "- Stream-stream joins\n",
    "- Output sinks (console, file, memory)\n",
    "- Trigger types and query management\n",
    "- Advanced patterns and error handling\n",
    "- Performance monitoring\n",
    "- Complete streaming application development\n",
    "\n",
    "Structured Streaming provides a powerful framework for building scalable, fault-tolerant real-time applications. The unified API allows you to write streaming logic using familiar DataFrame operations while Spark handles the complexities of distributed stream processing.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next chapter, we'll explore PySpark performance optimization techniques, including partitioning strategies, caching, and cluster tuning for maximum efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
