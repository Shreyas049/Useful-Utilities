{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced54736",
   "metadata": {},
   "source": [
    "## **PySpark Fundamentals - Part 1: Setting Up and Core Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c3589",
   "metadata": {},
   "source": [
    "### What is PySpark?\n",
    "PySpark is the Python API for Apache Spark, which is a distributed computing framework designed to process large datasets across clusters of computers. Think of it as a way to run Python code on multiple machines simultaneously, making it possible to handle datasets that are too large for a single computer's memory.\n",
    "The key insight is that PySpark operates on the principle of \"lazy evaluation\" - it builds up a plan of what you want to do with your data, but doesn't actually execute anything until you explicitly ask for results. This allows Spark to optimize the entire workflow before running it.\n",
    "\n",
    "### Setting Up PySpark\n",
    "First, let's look at how to initialize a PySpark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a506ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Setup and Initialization\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a Spark session - this is your entry point to PySpark\n",
    "# Think of it as opening a connection to the Spark cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApplication\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Context available as 'sc': {spark.sparkContext}\")\n",
    "print(f\"SQL Context available as 'spark': {spark}\")\n",
    "\n",
    "# At the end of your program, always stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6108da4",
   "metadata": {},
   "source": [
    "### **Core Data Structures in PySpark**\n",
    "\n",
    "PySpark has two main data structures you'll work with:\n",
    "#### **1. RDD (Resilient Distributed Dataset)**\n",
    "This is the lower-level abstraction. Think of it as a collection of objects distributed across your cluster. While powerful, it's more complex to work with and requires you to think about the underlying distributed nature of your data.\n",
    "\n",
    "#### **2. DataFrame**\n",
    "\n",
    "This is the higher-level abstraction that sits on top of RDDs. It's similar to a pandas DataFrame or a SQL table, with named columns and defined data types. This is what you'll use 95% of the time, and it's what we'll focus on.\n",
    "The DataFrame abstraction is incredibly powerful because it allows Spark's Catalyst optimizer to understand your data's structure and optimize your queries automatically. When you write DataFrame operations, Spark can rearrange, combine, and optimize your operations in ways that would be impossible with raw RDDs.\n",
    "Let me show you how to create and work with DataFrames in the next section. This will include reading data from various sources, understanding the DataFrame structure, and performing basic operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47a5e3",
   "metadata": {},
   "source": [
    "## **PySpark Fundamentals - Part 2: DataFrames and Basic Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8e65e",
   "metadata": {},
   "source": [
    "### **Creating DataFrames**\n",
    "DataFrames are like tables in a database or spreadsheets - they have rows and columns with defined data types. Let's explore the different ways to create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()\n",
    "\n",
    "# Method 1: Create DataFrame from Python list of tuples\n",
    "# This is great for small test datasets or when prototyping\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineer\", 75000.0),\n",
    "    (\"Bob\", 30, \"Manager\", 85000.0),\n",
    "    (\"Charlie\", 35, \"Director\", 120000.0),\n",
    "    (\"Diana\", 28, \"Analyst\", 65000.0)\n",
    "]\n",
    "\n",
    "# Define column names - Spark will infer data types\n",
    "columns = [\"name\", \"age\", \"job_title\", \"salary\"]\n",
    "df_simple = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Simple DataFrame created:\")\n",
    "df_simple.show()\n",
    "df_simple.printSchema()  # This shows the structure and data types\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 2: Create DataFrame with explicit schema\n",
    "# This is better for production code as you control exactly what data types you want\n",
    "# Think of schema as a blueprint that tells Spark exactly what to expect\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),      # True means nullable\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"job_title\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_with_schema = spark.createDataFrame(data, schema)\n",
    "print(\"DataFrame with explicit schema:\")\n",
    "df_with_schema.show()\n",
    "df_with_schema.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 3: Reading from files (most common in real applications)\n",
    "# Let's create a sample CSV first, then read it\n",
    "\n",
    "# Create sample data and save as CSV\n",
    "df_from_csv = spark.read.option(\"header\", \"true\").csv(\"path/to/your/file.csv\")\n",
    "df_from_json = spark.read.json(\"path/to/your/file.json\")\n",
    "df_from_parquet = spark.read.parquet(\"path/to/your/file.parquet\")\n",
    "\n",
    "print(\"Sample employee data:\")\n",
    "df_from_csv.show()\n",
    "\n",
    "# Method 4: Create DataFrame from dictionary (useful for configuration data)\n",
    "dict_data = [\n",
    "    {\"product\": \"Laptop\", \"price\": 1200, \"category\": \"Electronics\"},\n",
    "    {\"product\": \"Chair\", \"price\": 150, \"category\": \"Furniture\"},\n",
    "    {\"product\": \"Book\", \"price\": 25, \"category\": \"Education\"}\n",
    "]\n",
    "\n",
    "df_from_dict = spark.createDataFrame(dict_data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "df_from_dict.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae0aa2",
   "metadata": {},
   "source": [
    "### **Understanding DataFrames - Key Concepts**\n",
    "\n",
    "Now that you've seen how to create DataFrames, let's understand what makes them special. Think of a DataFrame as a distributed table where the data is automatically split across multiple computers, but you can work with it as if it were a single table.\n",
    "\n",
    "Here are the fundamental concepts you need to grasp:\n",
    "\n",
    "#### **Lazy Evaluation:** \n",
    "When you write DataFrame operations, Spark doesn't immediately execute them. Instead, it builds what's called a \"logical plan\" - essentially a recipe of what you want to do. Only when you call an \"action\" (like .show(), .collect(), or .count()) does Spark actually execute the plan. This allows Spark to optimize your entire workflow before running it.\n",
    "\n",
    "#### **Immutability:**\n",
    "DataFrames are immutable, meaning you can't change them directly. Every operation creates a new DataFrame. This might seem inefficient, but it's actually what allows Spark to run operations in parallel safely and recover from failures.\n",
    "\n",
    "#### **Partitioning:**\n",
    "Your data is automatically split into chunks called partitions, distributed across different machines. You usually don't need to worry about this, but understanding it helps explain why Spark can process massive datasets efficiently.\n",
    "\n",
    "#### **Basic DataFrame Operations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame Operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()\n",
    "\n",
    "# Create sample data for demonstration\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineer\", 75000.0, \"Tech\"),\n",
    "    (\"Bob\", 30, \"Manager\", 85000.0, \"Sales\"),\n",
    "    (\"Charlie\", 35, \"Director\", 120000.0, \"Tech\"),\n",
    "    (\"Diana\", 28, \"Analyst\", 65000.0, \"Finance\"),\n",
    "    (\"Eve\", 32, \"Engineer\", 78000.0, \"Tech\"),\n",
    "    (\"Frank\", 45, \"VP\", 150000.0, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"job_title\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 1. SELECTING COLUMNS\n",
    "# Think of this like choosing which columns you want to see in your spreadsheet\n",
    "print(\"1. Selecting specific columns:\")\n",
    "\n",
    "# Method 1: Using column names as strings\n",
    "df_selected = df.select(\"name\", \"salary\")\n",
    "df_selected.show()\n",
    "\n",
    "# Method 2: Using column objects (more flexible for complex operations)\n",
    "df_selected2 = df.select(col(\"name\"), col(\"salary\"))\n",
    "df_selected2.show()\n",
    "\n",
    "# Method 3: Selecting all columns from original df\n",
    "df_all = df.select(\"*\")\n",
    "print(f\"Total columns in original DataFrame: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. FILTERING ROWS\n",
    "# This is like applying a filter in Excel - only show rows that meet certain conditions\n",
    "print(\"2. Filtering rows:\")\n",
    "\n",
    "# Find employees with salary > 75000\n",
    "high_earners = df.filter(col(\"salary\") > 75000)\n",
    "print(\"Employees earning more than $75,000:\")\n",
    "high_earners.show()\n",
    "\n",
    "# Multiple conditions - find Tech employees earning > 75000\n",
    "tech_high_earners = df.filter(\n",
    "    (col(\"department\") == \"Tech\") & (col(\"salary\") > 75000)\n",
    ")\n",
    "print(\"Tech employees earning more than $75,000:\")\n",
    "tech_high_earners.show()\n",
    "\n",
    "# Using SQL-like string expressions (alternative syntax)\n",
    "young_employees = df.filter(\"age < 35\")\n",
    "print(\"Employees younger than 35:\")\n",
    "young_employees.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. ADDING NEW COLUMNS\n",
    "# Think of this as creating calculated fields in your spreadsheet\n",
    "print(\"3. Adding new columns:\")\n",
    "\n",
    "# Add a column for annual bonus (10% of salary)\n",
    "df_with_bonus = df.withColumn(\"annual_bonus\", col(\"salary\") * 0.10)\n",
    "print(\"DataFrame with bonus column:\")\n",
    "df_with_bonus.show()\n",
    "\n",
    "# Add multiple columns at once\n",
    "df_enhanced = df.withColumn(\"annual_bonus\", col(\"salary\") * 0.10) \\\n",
    "                .withColumn(\"age_category\", \n",
    "                           when(col(\"age\") < 30, \"Young\")\n",
    "                           .when(col(\"age\") < 40, \"Mid-Career\")\n",
    "                           .otherwise(\"Senior\"))\n",
    "\n",
    "print(\"DataFrame with multiple new columns:\")\n",
    "df_enhanced.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 4. RENAMING COLUMNS\n",
    "# Simple way to change column names\n",
    "print(\"4. Renaming columns:\")\n",
    "\n",
    "df_renamed = df.withColumnRenamed(\"job_title\", \"position\") \\\n",
    "               .withColumnRenamed(\"salary\", \"annual_salary\")\n",
    "\n",
    "print(\"DataFrame with renamed columns:\")\n",
    "df_renamed.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 5. SORTING DATA\n",
    "# Order your data by one or more columns\n",
    "print(\"5. Sorting data:\")\n",
    "\n",
    "# Sort by salary in descending order\n",
    "df_sorted = df.orderBy(col(\"salary\").desc())\n",
    "print(\"Employees sorted by salary (highest first):\")\n",
    "df_sorted.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_multi_sort = df.orderBy(col(\"department\").asc(), col(\"salary\").desc())\n",
    "print(\"Employees sorted by department, then by salary within each department:\")\n",
    "df_multi_sort.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 6. BASIC STATISTICS AND AGGREGATIONS\n",
    "print(\"6. Basic statistics:\")\n",
    "\n",
    "# Count total rows\n",
    "total_count = df.count()\n",
    "print(f\"Total number of employees: {total_count}\")\n",
    "\n",
    "# Basic statistics for numeric columns\n",
    "df.describe().show()\n",
    "\n",
    "# Specific aggregations\n",
    "avg_salary = df.agg(avg(\"salary\")).collect()[0][0]  # collect() brings data to driver\n",
    "max_age = df.agg(max(\"age\")).collect()[0][0]\n",
    "print(f\"Average salary: ${avg_salary:,.2f}\")\n",
    "print(f\"Maximum age: {max_age}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 7. VIEWING DATA IN DIFFERENT WAYS\n",
    "print(\"7. Different ways to examine your data:\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"First 3 rows:\")\n",
    "df.show(3)\n",
    "\n",
    "# Show all data (be careful with large datasets!)\n",
    "print(\"All data with unlimited width:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Get column names and types\n",
    "print(\"Column information:\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Get a single row as a Row object\n",
    "first_row = df.first()\n",
    "print(f\"First row: {first_row}\")\n",
    "print(f\"First employee's name: {first_row['name']}\")\n",
    "\n",
    "# Understanding the difference between transformations and actions:\n",
    "# Transformations (lazy - create new DataFrame): select, filter, withColumn, orderBy\n",
    "# Actions (eager - trigger computation): show, count, collect, first, take"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f10351",
   "metadata": {},
   "source": [
    "- The operations represent the foundation of almost everything you'll do with PySpark DataFrames. Notice how each operation follows a pattern: you start with a DataFrame, apply a transformation, and get back a new DataFrame. This chaining pattern is central to how PySpark works and makes it possible to build complex data processing pipelines by combining simple operations.\n",
    "\n",
    "- The key insight here is understanding the difference between transformations and actions. Transformations like select(), filter(), and withColumn() are lazy - they just build up a plan of what you want to do. Actions like show(), count(), and collect() actually trigger Spark to execute all the transformations you've defined. This design allows Spark to optimize your entire pipeline before running it, often making dramatic performance improvements.\n",
    "\n",
    "- Think of it like planning a route on a map app - you can add multiple stops and change your route as much as you want, but the app only calculates the optimal path when you press \"Start Navigation.\" Similarly, Spark waits until you ask for results before figuring out the most efficient way to get them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a1bbd",
   "metadata": {},
   "source": [
    "## **PySpark Advanced - Part 3: Grouping, Aggregations, and Joins**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb00dbe",
   "metadata": {},
   "source": [
    "### **Grouping and Aggregations**\n",
    "\n",
    "Think of grouping like creating pivot tables in Excel - you're organizing your data by certain categories and then calculating summaries for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregations in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a more comprehensive dataset for demonstration\n",
    "sales_data = [\n",
    "    (\"Alice\", \"Tech\", \"Q1\", 2023, 150000, 5),\n",
    "    (\"Alice\", \"Tech\", \"Q2\", 2023, 180000, 6),\n",
    "    (\"Bob\", \"Sales\", \"Q1\", 2023, 120000, 8),\n",
    "    (\"Bob\", \"Sales\", \"Q2\", 2023, 140000, 9),\n",
    "    (\"Charlie\", \"Tech\", \"Q1\", 2023, 200000, 4),\n",
    "    (\"Charlie\", \"Tech\", \"Q2\", 2023, 220000, 5),\n",
    "    (\"Diana\", \"Marketing\", \"Q1\", 2023, 80000, 12),\n",
    "    (\"Diana\", \"Marketing\", \"Q2\", 2023, 95000, 15),\n",
    "    (\"Eve\", \"Sales\", \"Q1\", 2023, 110000, 7),\n",
    "    (\"Eve\", \"Sales\", \"Q2\", 2023, 130000, 8),\n",
    "    (\"Frank\", \"Tech\", \"Q1\", 2023, 175000, 3),\n",
    "    (\"Frank\", \"Tech\", \"Q2\", 2023, 190000, 4)\n",
    "]\n",
    "\n",
    "columns = [\"employee\", \"department\", \"quarter\", \"year\", \"revenue\", \"deals_closed\"]\n",
    "df_sales = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "df_sales.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. BASIC GROUPING - Group by single column\n",
    "print(\"1. Basic Grouping - Revenue by Department:\")\n",
    "\n",
    "# Group by department and calculate total revenue\n",
    "dept_revenue = df_sales.groupBy(\"department\") \\\n",
    "                      .agg(sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                           avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "                           count(\"*\").alias(\"record_count\")) \\\n",
    "                      .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "dept_revenue.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 2. MULTIPLE GROUPING COLUMNS\n",
    "print(\"2. Multiple Grouping - Revenue by Department and Quarter:\")\n",
    "\n",
    "dept_quarter_stats = df_sales.groupBy(\"department\", \"quarter\") \\\n",
    "                            .agg(sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                                 sum(\"deals_closed\").alias(\"total_deals\"),\n",
    "                                 avg(\"revenue\").alias(\"avg_revenue_per_person\"),\n",
    "                                 count(\"employee\").alias(\"employee_count\")) \\\n",
    "                            .orderBy(\"department\", \"quarter\")\n",
    "\n",
    "dept_quarter_stats.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 3. ADVANCED AGGREGATIONS\n",
    "print(\"3. Advanced Aggregations with Multiple Functions:\")\n",
    "\n",
    "comprehensive_stats = df_sales.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        # Revenue statistics\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "        min(\"revenue\").alias(\"min_revenue\"),\n",
    "        max(\"revenue\").alias(\"max_revenue\"),\n",
    "        \n",
    "        # Deal statistics\n",
    "        sum(\"deals_closed\").alias(\"total_deals\"),\n",
    "        avg(\"deals_closed\").alias(\"avg_deals_per_person\"),\n",
    "        \n",
    "        # Count statistics\n",
    "        count(\"*\").alias(\"total_records\"),\n",
    "        countDistinct(\"employee\").alias(\"unique_employees\"),\n",
    "        \n",
    "        # Advanced: Standard deviation and variance\n",
    "        stddev(\"revenue\").alias(\"revenue_stddev\"),\n",
    "        \n",
    "        # Custom aggregation: Revenue per deal\n",
    "        (sum(\"revenue\") / sum(\"deals_closed\")).alias(\"revenue_per_deal\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "comprehensive_stats.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 4. CONDITIONAL AGGREGATIONS\n",
    "print(\"4. Conditional Aggregations:\")\n",
    "\n",
    "# Count how many high performers (>= 150k revenue) vs regular performers by department\n",
    "performance_analysis = df_sales.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        sum(when(col(\"revenue\") >= 150000, 1).otherwise(0)).alias(\"high_performers\"),\n",
    "        sum(when(col(\"revenue\") < 150000, 1).otherwise(0)).alias(\"regular_performers\"),\n",
    "        avg(\"revenue\").alias(\"avg_revenue\"),\n",
    "        # Calculate percentage of high performers\n",
    "        (sum(when(col(\"revenue\") >= 150000, 1).otherwise(0)) * 100.0 / count(\"*\")).alias(\"high_performer_percentage\")\n",
    "    )\n",
    "\n",
    "performance_analysis.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 5. COLLECT_LIST and COLLECT_SET - Gathering values\n",
    "print(\"5. Collecting Values - Who works in each department:\")\n",
    "\n",
    "department_employees = df_sales.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        collect_set(\"employee\").alias(\"unique_employees\"),  # collect_set removes duplicates\n",
    "        collect_list(\"employee\").alias(\"all_employee_records\"),  # collect_list keeps duplicates\n",
    "        count(\"*\").alias(\"total_records\")\n",
    "    )\n",
    "\n",
    "department_employees.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 6. PIVOT OPERATIONS - Reshape data like Excel pivot tables\n",
    "print(\"6. Pivot Operations - Department revenue by quarter:\")\n",
    "\n",
    "# Pivot: Turn quarter values into columns\n",
    "pivoted_data = df_sales.groupBy(\"department\") \\\n",
    "                      .pivot(\"quarter\") \\\n",
    "                      .agg(sum(\"revenue\")) \\\n",
    "                      .orderBy(\"department\")\n",
    "\n",
    "pivoted_data.show()\n",
    "\n",
    "# You can also pivot with multiple aggregations\n",
    "print(\"Multi-value pivot - Revenue and Deals by quarter:\")\n",
    "multi_pivot = df_sales.groupBy(\"department\") \\\n",
    "                     .pivot(\"quarter\") \\\n",
    "                     .agg(sum(\"revenue\").alias(\"revenue\"), \n",
    "                          sum(\"deals_closed\").alias(\"deals\"))\n",
    "\n",
    "multi_pivot.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 7. ROLLUP and CUBE - Multi-dimensional aggregations\n",
    "print(\"7. Rollup Operations - Hierarchical totals:\")\n",
    "\n",
    "# Rollup provides subtotals at different levels\n",
    "rollup_stats = df_sales.rollup(\"department\", \"quarter\") \\\n",
    "                      .agg(sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                           sum(\"deals_closed\").alias(\"total_deals\")) \\\n",
    "                      .orderBy(\"department\", \"quarter\")\n",
    "\n",
    "print(\"Rollup (includes subtotals and grand total):\")\n",
    "rollup_stats.show()\n",
    "\n",
    "# Cube provides all possible combinations of subtotals\n",
    "cube_stats = df_sales.cube(\"department\", \"quarter\") \\\n",
    "                    .agg(sum(\"revenue\").alias(\"total_revenue\")) \\\n",
    "                    .orderBy(\"department\", \"quarter\")\n",
    "\n",
    "print(\"Cube (all possible subtotal combinations):\")\n",
    "cube_stats.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 8. WINDOW FUNCTIONS PREVIEW\n",
    "print(\"8. Adding Running Totals with Window Functions:\")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window: partition by department, order by quarter\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"quarter\")\n",
    "\n",
    "# Add running total of revenue within each department\n",
    "df_with_running_total = df_sales.withColumn(\n",
    "    \"running_revenue_total\", \n",
    "    sum(\"revenue\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"revenue_rank_in_dept\",\n",
    "    rank().over(Window.partitionBy(\"department\").orderBy(col(\"revenue\").desc()))\n",
    ")\n",
    "\n",
    "print(\"Data with running totals and rankings:\")\n",
    "df_with_running_total.select(\"employee\", \"department\", \"quarter\", \"revenue\", \n",
    "                            \"running_revenue_total\", \"revenue_rank_in_dept\") \\\n",
    "                     .orderBy(\"department\", \"quarter\") \\\n",
    "                     .show()\n",
    "\n",
    "# Key insights about grouping and aggregations:\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY CONCEPTS:\")\n",
    "print(\"1. groupBy() creates groups, agg() performs calculations on each group\")\n",
    "print(\"2. Always use .alias() to name your aggregated columns clearly\")\n",
    "print(\"3. You can combine multiple aggregation functions in one agg() call\")\n",
    "print(\"4. Pivot reshapes data - turns row values into columns\")\n",
    "print(\"5. Rollup/Cube provide hierarchical subtotals\")\n",
    "print(\"6. Window functions let you do calculations across related rows\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dfdec",
   "metadata": {},
   "source": [
    "### **Joining DataFrames**\n",
    "\n",
    "Joins are how you combine data from multiple DataFrames, similar to JOIN operations in SQL. This is crucial when your data is spread across multiple tables or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Joins in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create sample datasets for demonstration\n",
    "# Employee basic info\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineer\", \"Tech\"),\n",
    "    (2, \"Bob\", \"Manager\", \"Sales\"),\n",
    "    (3, \"Charlie\", \"Director\", \"Tech\"),\n",
    "    (4, \"Diana\", \"Analyst\", \"Finance\"),\n",
    "    (5, \"Eve\", \"Engineer\", \"Tech\")\n",
    "]\n",
    "employees_df = spark.createDataFrame(employees_data, [\"emp_id\", \"name\", \"job_title\", \"department\"])\n",
    "\n",
    "# Employee salary info (separate table)\n",
    "salaries_data = [\n",
    "    (1, 75000, \"2023-01-01\"),\n",
    "    (2, 85000, \"2023-01-01\"),\n",
    "    (3, 120000, \"2023-01-01\"),\n",
    "    (4, 65000, \"2023-01-01\"),\n",
    "    (6, 90000, \"2023-01-01\")  # Note: emp_id 6 doesn't exist in employees table\n",
    "]\n",
    "salaries_df = spark.createDataFrame(salaries_data, [\"emp_id\", \"salary\", \"effective_date\"])\n",
    "\n",
    "# Department budget info\n",
    "dept_budget_data = [\n",
    "    (\"Tech\", 500000, \"John Smith\"),\n",
    "    (\"Sales\", 300000, \"Sarah Johnson\"),\n",
    "    (\"Finance\", 200000, \"Mike Brown\"),\n",
    "    (\"HR\", 150000, \"Lisa Davis\")  # Note: No employees in HR department\n",
    "]\n",
    "dept_budget_df = spark.createDataFrame(dept_budget_data, [\"department\", \"budget\", \"manager\"])\n",
    "\n",
    "print(\"SAMPLE DATASETS:\")\n",
    "print(\"\\nEmployees:\")\n",
    "employees_df.show()\n",
    "print(\"Salaries:\")\n",
    "salaries_df.show()\n",
    "print(\"Department Budgets:\")\n",
    "dept_budget_df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. INNER JOIN - Only matching records from both sides\n",
    "print(\"1. INNER JOIN - Employees with salary information:\")\n",
    "\n",
    "inner_join_result = employees_df.join(salaries_df, \"emp_id\", \"inner\")\n",
    "inner_join_result.show()\n",
    "\n",
    "print(\"Notice: Only employees 1-4 appear because emp_id 5 has no salary, and emp_id 6 has no employee record\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. LEFT (OUTER) JOIN - All records from left table, matching from right\n",
    "print(\"2. LEFT JOIN - All employees, with salary where available:\")\n",
    "\n",
    "left_join_result = employees_df.join(salaries_df, \"emp_id\", \"left\")\n",
    "left_join_result.show()\n",
    "\n",
    "print(\"Notice: All 5 employees appear, but Eve (emp_id 5) has null salary values\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. RIGHT (OUTER) JOIN - All records from right table, matching from left\n",
    "print(\"3. RIGHT JOIN - All salary records, with employee info where available:\")\n",
    "\n",
    "right_join_result = employees_df.join(salaries_df, \"emp_id\", \"right\")\n",
    "right_join_result.show()\n",
    "\n",
    "print(\"Notice: emp_id 6 appears with salary but null employee information\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. FULL OUTER JOIN - All records from both tables\n",
    "print(\"4. FULL OUTER JOIN - Everything from both tables:\")\n",
    "\n",
    "full_join_result = employees_df.join(salaries_df, \"emp_id\", \"full\")\n",
    "full_join_result.show()\n",
    "\n",
    "print(\"Notice: All employees AND all salary records appear, with nulls where data is missing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 5. JOINING ON MULTIPLE CONDITIONS\n",
    "print(\"5. Complex Join Conditions:\")\n",
    "\n",
    "# Let's create data with multiple join keys\n",
    "projects_data = [\n",
    "    (1, \"Tech\", \"Project Alpha\", \"2023-Q1\"),\n",
    "    (2, \"Sales\", \"Project Beta\", \"2023-Q1\"),\n",
    "    (3, \"Tech\", \"Project Gamma\", \"2023-Q2\"),\n",
    "    (1, \"Tech\", \"Project Delta\", \"2023-Q2\")\n",
    "]\n",
    "projects_df = spark.createDataFrame(projects_data, [\"emp_id\", \"department\", \"project_name\", \"quarter\"])\n",
    "\n",
    "# Join on multiple conditions\n",
    "multi_condition_join = employees_df.join(\n",
    "    projects_df, \n",
    "    (employees_df.emp_id == projects_df.emp_id) & \n",
    "    (employees_df.department == projects_df.department),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"Employees matched to projects (by employee ID AND department):\")\n",
    "multi_condition_join.select(\n",
    "    employees_df.name, \n",
    "    employees_df.department, \n",
    "    projects_df.project_name, \n",
    "    projects_df.quarter\n",
    ").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 6. JOINING TABLES WITH DIFFERENT COLUMN NAMES\n",
    "print(\"6. Joining on columns with different names:\")\n",
    "\n",
    "# Create a table where the join key has a different name\n",
    "performance_data = [\n",
    "    (1, 85, \"Excellent\"),\n",
    "    (2, 78, \"Good\"),\n",
    "    (3, 92, \"Outstanding\"),\n",
    "    (4, 70, \"Satisfactory\")\n",
    "]\n",
    "performance_df = spark.createDataFrame(performance_data, [\"employee_id\", \"score\", \"rating\"])\n",
    "\n",
    "# Join where left table has 'emp_id' and right table has 'employee_id'\n",
    "different_names_join = employees_df.join(\n",
    "    performance_df,\n",
    "    employees_df.emp_id == performance_df.employee_id,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"Employees with performance ratings:\")\n",
    "different_names_join.select(\n",
    "    \"name\", \"job_title\", \"score\", \"rating\"\n",
    ").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 7. SELF JOINS - Joining a table with itself\n",
    "print(\"7. Self Join - Finding employees in the same department:\")\n",
    "\n",
    "# Join employees table with itself to find colleagues\n",
    "emp_alias1 = employees_df.alias(\"emp1\")\n",
    "emp_alias2 = employees_df.alias(\"emp2\")\n",
    "\n",
    "colleagues = emp_alias1.join(\n",
    "    emp_alias2,\n",
    "    (emp_alias1.department == emp_alias2.department) & \n",
    "    (emp_alias1.emp_id != emp_alias2.emp_id),  # Don't match employee with themselves\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp1.name\").alias(\"employee_1\"),\n",
    "    col(\"emp2.name\").alias(\"employee_2\"),\n",
    "    col(\"emp1.department\").alias(\"shared_department\")\n",
    ")\n",
    "\n",
    "print(\"Employees who work in the same department:\")\n",
    "colleagues.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. ANTI JOIN - Records in left table that DON'T have matches in right table\n",
    "print(\"8. Anti Join - Employees without salary records:\")\n",
    "\n",
    "employees_without_salary = employees_df.join(salaries_df, \"emp_id\", \"left_anti\")\n",
    "employees_without_salary.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 9. SEMI JOIN - Records in left table that DO have matches in right table\n",
    "print(\"9. Semi Join - Employees who have salary records:\")\n",
    "\n",
    "employees_with_salary = employees_df.join(salaries_df, \"emp_id\", \"left_semi\")\n",
    "employees_with_salary.show()\n",
    "\n",
    "print(\"Notice: This shows employee info only, even though we joined with salary table\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 10. COMPLEX MULTI-TABLE JOIN\n",
    "print(\"10. Complex Multi-table Join - Complete Employee Information:\")\n",
    "\n",
    "# Join all three tables together\n",
    "complete_info = employees_df \\\n",
    "    .join(salaries_df, \"emp_id\", \"left\") \\\n",
    "    .join(dept_budget_df, \"department\", \"left\") \\\n",
    "    .select(\n",
    "        \"name\",\n",
    "        \"job_title\", \n",
    "        \"department\",\n",
    "        \"salary\",\n",
    "        \"budget\",\n",
    "        \"manager\"\n",
    "    ).orderBy(\"name\")\n",
    "\n",
    "print(\"Complete employee information with department details:\")\n",
    "complete_info.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 11. HANDLING DUPLICATE COLUMN NAMES AFTER JOINS\n",
    "print(\"11. Handling duplicate column names:\")\n",
    "\n",
    "# When joining tables with same column names, you need to be explicit\n",
    "employees_with_dept_col = employees_df.select(\"emp_id\", \"name\", \"department\")\n",
    "projects_with_dept_col = projects_df.select(\"emp_id\", \"department\", \"project_name\")\n",
    "\n",
    "# This would cause ambiguous column reference - which department column?\n",
    "# Solution: Select specific columns or rename before joining\n",
    "clean_join = employees_with_dept_col.alias(\"emp\").join(\n",
    "    projects_with_dept_col.alias(\"proj\"),\n",
    "    col(\"emp.emp_id\") == col(\"proj.emp_id\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"emp.name\"),\n",
    "    col(\"emp.department\").alias(\"employee_dept\"),\n",
    "    col(\"proj.department\").alias(\"project_dept\"),\n",
    "    col(\"proj.project_name\")\n",
    ")\n",
    "\n",
    "print(\"Handling duplicate column names:\")\n",
    "clean_join.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JOIN TYPE SUMMARY:\")\n",
    "print(\"• INNER: Only matching records from both tables\")\n",
    "print(\"• LEFT: All from left table + matching from right\")\n",
    "print(\"• RIGHT: All from right table + matching from left\") \n",
    "print(\"• FULL: All records from both tables\")\n",
    "print(\"• LEFT_ANTI: Records in left that DON'T match right\")\n",
    "print(\"• LEFT_SEMI: Records in left that DO match right (left columns only)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012c871",
   "metadata": {},
   "source": [
    "### **Key Concepts**\n",
    "\n",
    "- **Grouping and Aggregations** allow you to answer questions like \"What's the total sales by region?\" or \"Who are the top performers in each department?\" The pattern is always: group your data by some categories, then calculate summaries for each group.\n",
    "Joins let you bring together related information from different datasets. Think of them as ways to connect the dots between different pieces of information. The type of join you choose depends on what you want to include in your final result.\n",
    "\n",
    "- The most important insight about joins is understanding what each type preserves:\n",
    "\n",
    "    **Inner joins are conservative** - only keep perfect matches \\\n",
    "    **Left joins prioritize the left table** - keep everything from it \\\n",
    "    **Full outer joins are comprehensive** - keep everything from everywhere\n",
    "\n",
    "- These operations form the backbone of most data analysis workflows. You'll typically read data from multiple sources, join them together to create a complete picture, then group and aggregate to generate insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61498093",
   "metadata": {},
   "source": [
    "## **PySpark Advanced - Part 4: Window Functions and Complex Data Manipulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fc857",
   "metadata": {},
   "source": [
    "Window functions are one of PySpark's most powerful features. They let you perform calculations across a set of related rows without collapsing your data into groups. Think of them as a way to \"look around\" at neighboring rows while keeping all your original data intact.\n",
    "\n",
    "### **Window Functions Deep Dive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Functions in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a comprehensive sales dataset\n",
    "sales_data = [\n",
    "    (\"Alice\", \"Tech\", \"2023-01\", 150000, 5),\n",
    "    (\"Alice\", \"Tech\", \"2023-02\", 180000, 6),\n",
    "    (\"Alice\", \"Tech\", \"2023-03\", 200000, 7),\n",
    "    (\"Bob\", \"Sales\", \"2023-01\", 120000, 8),\n",
    "    (\"Bob\", \"Sales\", \"2023-02\", 140000, 9),\n",
    "    (\"Bob\", \"Sales\", \"2023-03\", 160000, 10),\n",
    "    (\"Charlie\", \"Tech\", \"2023-01\", 200000, 4),\n",
    "    (\"Charlie\", \"Tech\", \"2023-02\", 220000, 5),\n",
    "    (\"Charlie\", \"Tech\", \"2023-03\", 240000, 6),\n",
    "    (\"Diana\", \"Marketing\", \"2023-01\", 80000, 12),\n",
    "    (\"Diana\", \"Marketing\", \"2023-02\", 95000, 15),\n",
    "    (\"Diana\", \"Marketing\", \"2023-03\", 110000, 18),\n",
    "    (\"Eve\", \"Sales\", \"2023-01\", 110000, 7),\n",
    "    (\"Eve\", \"Sales\", \"2023-02\", 130000, 8),\n",
    "    (\"Eve\", \"Sales\", \"2023-03\", 150000, 9)\n",
    "]\n",
    "\n",
    "columns = [\"employee\", \"department\", \"month\", \"revenue\", \"deals_closed\"]\n",
    "df = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "df.orderBy(\"employee\", \"month\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. RANKING FUNCTIONS\n",
    "print(\"1. Ranking Functions - Who are the top performers?\")\n",
    "\n",
    "# Define window for ranking within each department\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(col(\"revenue\").desc())\n",
    "\n",
    "df_with_rankings = df.withColumn(\"revenue_rank\", rank().over(dept_window)) \\\n",
    "                    .withColumn(\"revenue_dense_rank\", dense_rank().over(dept_window)) \\\n",
    "                    .withColumn(\"revenue_row_number\", row_number().over(dept_window))\n",
    "\n",
    "print(\"Rankings within each department:\")\n",
    "df_with_rankings.select(\"employee\", \"department\", \"revenue\", \n",
    "                       \"revenue_rank\", \"revenue_dense_rank\", \"revenue_row_number\") \\\n",
    "                .orderBy(\"department\", \"revenue_rank\") \\\n",
    "                .show()\n",
    "\n",
    "print(\"Key differences:\")\n",
    "print(\"- rank(): Gaps in ranking when there are ties (1, 2, 2, 4)\")\n",
    "print(\"- dense_rank(): No gaps in ranking (1, 2, 2, 3)\")\n",
    "print(\"- row_number(): Always unique numbers (1, 2, 3, 4)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. RUNNING TOTALS AND CUMULATIVE CALCULATIONS\n",
    "print(\"2. Running Totals - Track cumulative performance over time\")\n",
    "\n",
    "# Window ordered by month for each employee\n",
    "employee_time_window = Window.partitionBy(\"employee\").orderBy(\"month\")\n",
    "\n",
    "df_with_running_totals = df.withColumn(\n",
    "    \"cumulative_revenue\", sum(\"revenue\").over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"cumulative_deals\", sum(\"deals_closed\").over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"running_avg_revenue\", avg(\"revenue\").over(employee_time_window)\n",
    ")\n",
    "\n",
    "print(\"Running totals for each employee over time:\")\n",
    "df_with_running_totals.select(\"employee\", \"month\", \"revenue\", \"deals_closed\",\n",
    "                             \"cumulative_revenue\", \"cumulative_deals\", \"running_avg_revenue\") \\\n",
    "                     .orderBy(\"employee\", \"month\") \\\n",
    "                     .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. LAG AND LEAD - Looking at previous and next rows\n",
    "print(\"3. Lag and Lead - Compare with previous/next periods\")\n",
    "\n",
    "df_with_lag_lead = df.withColumn(\n",
    "    \"prev_month_revenue\", lag(\"revenue\", 1).over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"next_month_revenue\", lead(\"revenue\", 1).over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"revenue_growth\", col(\"revenue\") - lag(\"revenue\", 1).over(employee_time_window)\n",
    ").withColumn(\n",
    "    \"revenue_growth_pct\", \n",
    "    ((col(\"revenue\") - lag(\"revenue\", 1).over(employee_time_window)) / \n",
    "     lag(\"revenue\", 1).over(employee_time_window) * 100)\n",
    ")\n",
    "\n",
    "print(\"Month-over-month comparisons:\")\n",
    "df_with_lag_lead.select(\"employee\", \"month\", \"revenue\", \"prev_month_revenue\", \n",
    "                       \"revenue_growth\", \"revenue_growth_pct\") \\\n",
    "                .orderBy(\"employee\", \"month\") \\\n",
    "                .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. MOVING AVERAGES AND WINDOW FRAMES\n",
    "print(\"4. Moving Averages - Smooth out trends\")\n",
    "\n",
    "# Define a window with a specific frame: current row and 1 preceding row\n",
    "moving_avg_window = Window.partitionBy(\"employee\") \\\n",
    "                         .orderBy(\"month\") \\\n",
    "                         .rowsBetween(-1, 0)  # Previous row and current row\n",
    "\n",
    "df_with_moving_avg = df.withColumn(\n",
    "    \"moving_avg_revenue_2months\", avg(\"revenue\").over(moving_avg_window)\n",
    ").withColumn(\n",
    "    \"moving_sum_deals_2months\", sum(\"deals_closed\").over(moving_avg_window)\n",
    ")\n",
    "\n",
    "# 3-month moving average (current + 2 preceding)\n",
    "moving_avg_3month_window = Window.partitionBy(\"employee\") \\\n",
    "                                .orderBy(\"month\") \\\n",
    "                                .rowsBetween(-2, 0)\n",
    "\n",
    "df_with_moving_avg = df_with_moving_avg.withColumn(\n",
    "    \"moving_avg_revenue_3months\", avg(\"revenue\").over(moving_avg_3month_window)\n",
    ")\n",
    "\n",
    "print(\"Moving averages to smooth trends:\")\n",
    "df_with_moving_avg.select(\"employee\", \"month\", \"revenue\", \n",
    "                         \"moving_avg_revenue_2months\", \"moving_avg_revenue_3months\") \\\n",
    "                 .orderBy(\"employee\", \"month\") \\\n",
    "                 .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 5. PERCENTILES AND NTILE\n",
    "print(\"5. Percentiles and Distribution Analysis\")\n",
    "\n",
    "# Overall window for percentiles across all employees\n",
    "overall_window = Window.orderBy(\"revenue\")\n",
    "\n",
    "df_with_percentiles = df.withColumn(\n",
    "    \"revenue_percentile\", percent_rank().over(overall_window)\n",
    ").withColumn(\n",
    "    \"revenue_quartile\", ntile(4).over(overall_window)  # Divide into 4 groups\n",
    ").withColumn(\n",
    "    \"revenue_decile\", ntile(10).over(overall_window)   # Divide into 10 groups\n",
    ")\n",
    "\n",
    "print(\"Percentile analysis:\")\n",
    "df_with_percentiles.select(\"employee\", \"month\", \"revenue\", \n",
    "                          \"revenue_percentile\", \"revenue_quartile\", \"revenue_decile\") \\\n",
    "                  .orderBy(\"revenue\") \\\n",
    "                  .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 6. FIRST_VALUE AND LAST_VALUE\n",
    "print(\"6. First and Last Values - Benchmarking against period extremes\")\n",
    "\n",
    "dept_month_window = Window.partitionBy(\"department\", \"month\").orderBy(\"revenue\")\n",
    "\n",
    "df_with_first_last = df.withColumn(\n",
    "    \"dept_min_revenue_this_month\", first(\"revenue\").over(dept_month_window)\n",
    ").withColumn(\n",
    "    \"dept_max_revenue_this_month\", last(\"revenue\").over(dept_month_window)\n",
    ").withColumn(\n",
    "    \"performance_vs_dept_min\", col(\"revenue\") - first(\"revenue\").over(dept_month_window)\n",
    ").withColumn(\n",
    "    \"performance_vs_dept_max\", col(\"revenue\") - last(\"revenue\").over(dept_month_window)\n",
    ")\n",
    "\n",
    "print(\"Performance vs department extremes:\")\n",
    "df_with_first_last.select(\"employee\", \"department\", \"month\", \"revenue\",\n",
    "                         \"dept_min_revenue_this_month\", \"dept_max_revenue_this_month\",\n",
    "                         \"performance_vs_dept_min\", \"performance_vs_dept_max\") \\\n",
    "                 .orderBy(\"department\", \"month\", \"revenue\") \\\n",
    "                 .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 7. COMPLEX WINDOW ANALYSIS - Multiple windows in one query\n",
    "print(\"7. Complex Analysis - Multiple Window Functions Together\")\n",
    "\n",
    "# Define multiple windows for different analyses\n",
    "employee_window = Window.partitionBy(\"employee\").orderBy(\"month\")\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(col(\"revenue\").desc())\n",
    "overall_window = Window.orderBy(\"revenue\")\n",
    "\n",
    "comprehensive_analysis = df.withColumn(\n",
    "    # Time-based analysis for each employee\n",
    "    \"employee_running_total\", sum(\"revenue\").over(employee_window)\n",
    ").withColumn(\n",
    "    \"employee_avg_so_far\", avg(\"revenue\").over(employee_window)\n",
    ").withColumn(\n",
    "    \"month_over_month_change\", col(\"revenue\") - lag(\"revenue\", 1).over(employee_window)\n",
    ").withColumn(\n",
    "    # Department ranking\n",
    "    \"dept_revenue_rank\", rank().over(dept_window)\n",
    ").withColumn(\n",
    "    # Overall percentile\n",
    "    \"overall_percentile\", percent_rank().over(overall_window)\n",
    ").withColumn(\n",
    "    # Performance indicators\n",
    "    \"is_top_performer_in_dept\", when(rank().over(dept_window) <= 2, \"Yes\").otherwise(\"No\")\n",
    ").withColumn(\n",
    "    \"performance_tier\", \n",
    "    when(percent_rank().over(overall_window) >= 0.8, \"Top 20%\")\n",
    "    .when(percent_rank().over(overall_window) >= 0.6, \"Top 40%\") \n",
    "    .when(percent_rank().over(overall_window) >= 0.4, \"Middle 20%\")\n",
    "    .when(percent_rank().over(overall_window) >= 0.2, \"Bottom 40%\")\n",
    "    .otherwise(\"Bottom 20%\")\n",
    ")\n",
    "\n",
    "print(\"Comprehensive performance analysis:\")\n",
    "comprehensive_analysis.select(\n",
    "    \"employee\", \"department\", \"month\", \"revenue\",\n",
    "    \"employee_running_total\", \"month_over_month_change\",\n",
    "    \"dept_revenue_rank\", \"performance_tier\", \"is_top_performer_in_dept\"\n",
    ").orderBy(\"employee\", \"month\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. WINDOW FRAMES - Different ways to define your \"window\"\n",
    "print(\"8. Understanding Window Frames\")\n",
    "\n",
    "# Current row only\n",
    "current_only = Window.partitionBy(\"employee\").orderBy(\"month\").rowsBetween(0, 0)\n",
    "\n",
    "# Unbounded preceding to current (running total)\n",
    "running_total_frame = Window.partitionBy(\"employee\").orderBy(\"month\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Range-based frame (useful for time-based windows)\n",
    "# Note: This would work better with actual date columns\n",
    "range_frame = Window.partitionBy(\"employee\").orderBy(\"month\").rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_frame_examples = df.withColumn(\n",
    "    \"current_revenue_only\", sum(\"revenue\").over(current_only)\n",
    ").withColumn(\n",
    "    \"running_total_explicit\", sum(\"revenue\").over(running_total_frame)\n",
    ").withColumn(\n",
    "    \"count_records_so_far\", count(\"*\").over(running_total_frame)\n",
    ")\n",
    "\n",
    "print(\"Different window frame examples:\")\n",
    "df_frame_examples.select(\"employee\", \"month\", \"revenue\", \n",
    "                        \"current_revenue_only\", \"running_total_explicit\", \"count_records_so_far\") \\\n",
    "                 .orderBy(\"employee\", \"month\") \\\n",
    "                 .show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WINDOW FUNCTIONS KEY CONCEPTS:\")\n",
    "print(\"• Window = Partition (group) + Order + Frame (which rows to include)\")\n",
    "print(\"• Partition: Which records to group together\")\n",
    "print(\"• Order: How to sort within each partition\") \n",
    "print(\"• Frame: Which rows to include in calculation (default is unbounded preceding to current)\")\n",
    "print(\"• Ranking: rank(), dense_rank(), row_number(), ntile()\")\n",
    "print(\"• Analytics: lag(), lead(), first_value(), last_value()\")\n",
    "print(\"• Aggregates: sum(), avg(), count(), etc. over windows\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da041f56",
   "metadata": {},
   "source": [
    "### **Advanced Data Manipulation Techniques**\n",
    "\n",
    "Now let's explore some sophisticated data manipulation techniques that you'll use in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43672dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Manipulation in PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# Create complex sample data for demonstration\n",
    "complex_data = [\n",
    "    (1, \"Alice Johnson\", \"alice.johnson@company.com\", \"2023-01-15\", \n",
    "     \"{'skills': ['Python', 'SQL', 'Machine Learning'], 'years_exp': 5, 'certifications': ['AWS', 'GCP']}\", \n",
    "     \"Engineering,Data Science\", \"New York,San Francisco\"),\n",
    "    (2, \"Bob Smith\", \"bob.smith@company.com\", \"2023-02-20\",\n",
    "     \"{'skills': ['Java', 'Spring', 'Microservices'], 'years_exp': 8, 'certifications': ['Oracle', 'Spring']}\", \n",
    "     \"Engineering,Architecture\", \"Chicago\"),\n",
    "    (3, \"Carol Davis\", \"carol.davis@company.com\", \"2023-03-10\",\n",
    "     \"{'skills': ['Marketing', 'Analytics', 'SEO'], 'years_exp': 6, 'certifications': ['Google Analytics', 'HubSpot']}\", \n",
    "     \"Marketing,Growth\", \"Los Angeles,Austin\"),\n",
    "    (4, \"David Wilson\", \"david.wilson@company.com\", \"2023-01-05\",\n",
    "     \"{'skills': ['Finance', 'Excel', 'PowerBI'], 'years_exp': 10, 'certifications': ['CPA', 'CFA']}\", \n",
    "     \"Finance,Analytics\", \"Boston\")\n",
    "]\n",
    "\n",
    "columns = [\"emp_id\", \"full_name\", \"email\", \"hire_date\", \"profile_json\", \"departments\", \"locations\"]\n",
    "df = spark.createDataFrame(complex_data, columns)\n",
    "\n",
    "print(\"Original Complex Dataset:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 1. STRING MANIPULATION AND EXTRACTION\n",
    "print(\"1. Advanced String Operations\")\n",
    "\n",
    "df_string_ops = df.withColumn(\n",
    "    # Extract first and last names\n",
    "    \"first_name\", split(col(\"full_name\"), \" \").getItem(0)\n",
    ").withColumn(\n",
    "    \"last_name\", split(col(\"full_name\"), \" \").getItem(1)\n",
    ").withColumn(\n",
    "    # Extract domain from email\n",
    "    \"email_domain\", regexp_extract(col(\"email\"), r\"@(.+)\", 1)\n",
    ").withColumn(\n",
    "    # Clean and standardize email\n",
    "    \"email_clean\", lower(trim(col(\"email\")))\n",
    ").withColumn(\n",
    "    # Extract year from hire date\n",
    "    \"hire_year\", year(to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    ").withColumn(\n",
    "    # Create initials\n",
    "    \"initials\", concat(\n",
    "        substring(col(\"first_name\"), 1, 1),\n",
    "        lit(\".\"),\n",
    "        substring(col(\"last_name\"), 1, 1),\n",
    "        lit(\".\")\n",
    "    )\n",
    ").withColumn(\n",
    "    # Check if name contains specific pattern\n",
    "    \"has_common_lastname\", when(col(\"last_name\").rlike(\"Smith|Johnson|Davis\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "print(\"String manipulation results:\")\n",
    "df_string_ops.select(\"full_name\", \"first_name\", \"last_name\", \"email_domain\", \n",
    "                    \"initials\", \"hire_year\", \"has_common_lastname\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 2. WORKING WITH ARRAYS AND COMPLEX DATA\n",
    "print(\"2. Array and Complex Data Operations\")\n",
    "\n",
    "# Split comma-separated values into arrays\n",
    "df_arrays = df.withColumn(\n",
    "    \"departments_array\", split(col(\"departments\"), \",\")\n",
    ").withColumn(\n",
    "    \"locations_array\", split(col(\"locations\"), \",\")\n",
    ").withColumn(\n",
    "    # Get array size\n",
    "    \"num_departments\", size(split(col(\"departments\"), \",\"))\n",
    ").withColumn(\n",
    "    \"num_locations\", size(split(col(\"locations\"), \",\"))\n",
    ").withColumn(\n",
    "    # Check if array contains specific value\n",
    "    \"works_in_engineering\", array_contains(split(col(\"departments\"), \",\"), \"Engineering\")\n",
    ").withColumn(\n",
    "    # Get first element\n",
    "    \"primary_department\", split(col(\"departments\"), \",\").getItem(0)\n",
    ").withColumn(\n",
    "    # Get all except first element (if more than one)\n",
    "    \"secondary_departments\", \n",
    "    when(size(split(col(\"departments\"), \",\")) > 1,\n",
    "         slice(split(col(\"departments\"), \",\"), 2, 10))\n",
    "    .otherwise(array())\n",
    ")\n",
    "\n",
    "print(\"Array operations:\")\n",
    "df_arrays.select(\"emp_id\", \"full_name\", \"departments_array\", \"num_departments\", \n",
    "                \"works_in_engineering\", \"primary_department\", \"secondary_departments\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 3. WORKING WITH JSON DATA\n",
    "print(\"3. JSON Data Extraction\")\n",
    "\n",
    "# Parse JSON strings to extract structured data\n",
    "df_json = df.withColumn(\n",
    "    # Parse JSON string to struct\n",
    "    \"profile_struct\", from_json(col(\"profile_json\"), \n",
    "        StructType([\n",
    "            StructField(\"skills\", ArrayType(StringType()), True),\n",
    "            StructField(\"years_exp\", IntegerType(), True),\n",
    "            StructField(\"certifications\", ArrayType(StringType()), True)\n",
    "        ])\n",
    "    )\n",
    ").withColumn(\n",
    "    # Extract specific fields from JSON\n",
    "    \"skills_array\", col(\"profile_struct.skills\")\n",
    ").withColumn(\n",
    "    \"years_experience\", col(\"profile_struct.years_exp\")\n",
    ").withColumn(\n",
    "    \"certifications_array\", col(\"profile_struct.certifications\")\n",
    ").withColumn(\n",
    "    # Work with extracted arrays\n",
    "    \"num_skills\", size(col(\"profile_struct.skills\"))\n",
    ").withColumn(\n",
    "    \"num_certifications\", size(col(\"profile_struct.certifications\"))\n",
    ").withColumn(\n",
    "    \"has_python_skill\", array_contains(col(\"profile_struct.skills\"), \"Python\")\n",
    ").withColumn(\n",
    "    \"primary_skill\", col(\"profile_struct.skills\").getItem(0)\n",
    ")\n",
    "\n",
    "print(\"JSON extraction results:\")\n",
    "df_json.select(\"emp_id\", \"full_name\", \"skills_array\", \"years_experience\", \n",
    "              \"num_skills\", \"has_python_skill\", \"primary_skill\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 4. CONDITIONAL LOGIC AND CASE STATEMENTS\n",
    "print(\"4. Complex Conditional Logic\")\n",
    "\n",
    "df_conditional = df_json.withColumn(\n",
    "    # Multi-level conditional logic\n",
    "    \"experience_level\",\n",
    "    when(col(\"years_experience\") < 3, \"Junior\")\n",
    "    .when(col(\"years_experience\") < 7, \"Mid-Level\") \n",
    "    .when(col(\"years_experience\") < 10, \"Senior\")\n",
    "    .otherwise(\"Principal\")\n",
    ").withColumn(\n",
    "    # Complex conditions with AND/OR\n",
    "    \"tech_leader\",\n",
    "    when(\n",
    "        (col(\"years_experience\") >= 5) & \n",
    "        (array_contains(col(\"skills_array\"), \"Python\") | array_contains(col(\"skills_array\"), \"Java\")) &\n",
    "        (size(col(\"certifications_array\")) >= 2),\n",
    "        \"Yes\"\n",
    "    ).otherwise(\"No\")\n",
    ").withColumn(\n",
    "    # Nested conditions\n",
    "    \"employee_category\",\n",
    "    when(col(\"works_in_engineering\") & (col(\"years_experience\") >= 5), \"Senior Engineer\")\n",
    "    .when(col(\"works_in_engineering\") & (col(\"years_experience\") < 5), \"Junior Engineer\")\n",
    "    .when(~col(\"works_in_engineering\") & (col(\"years_experience\") >= 8), \"Senior Non-Tech\")\n",
    "    .otherwise(\"Junior Non-Tech\")\n",
    ").withColumn(\n",
    "    # Using case with multiple columns\n",
    "    \"compensation_band\",\n",
    "    when((col(\"experience_level\") == \"Principal\") | (col(\"tech_leader\") == \"Yes\"), \"Band A\")\n",
    "    .when(col(\"experience_level\") == \"Senior\", \"Band B\")\n",
    "    .when(col(\"experience_level\") == \"Mid-Level\", \"Band C\")\n",
    "    .otherwise(\"Band D\")\n",
    ")\n",
    "\n",
    "print(\"Conditional logic results:\")\n",
    "df_conditional.select(\"full_name\", \"years_experience\", \"experience_level\", \n",
    "                     \"tech_leader\", \"employee_category\", \"compensation_band\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 5. DATA TYPE CONVERSIONS AND CLEANING\n",
    "print(\"5. Data Type Conversions and Cleaning\")\n",
    "\n",
    "# Create some messy data for cleaning demonstration\n",
    "messy_data = [\n",
    "    (\"  John Doe  \", \"123.45\", \"2023-01-15T10:30:00\", \"true\", \"NULL\"),\n",
    "    (\"Jane Smith\", \"67.8\", \"2023-02-20T14:15:30\", \"false\", \"\"),\n",
    "    (\"Bob Johnson\", \"invalid_number\", \"invalid_date\", \"maybe\", \"N/A\"),\n",
    "    (\"\", \"0.00\", \"2023-03-10T09:45:00\", \"1\", \"none\")\n",
    "]\n",
    "\n",
    "messy_df = spark.createDataFrame(messy_data, [\"name\", \"score\", \"timestamp\", \"active\", \"notes\"])\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "messy_df.show()\n",
    "\n",
    "# Clean and convert the data\n",
    "cleaned_df = messy_df.withColumn(\n",
    "    # Clean strings\n",
    "    \"name_clean\", trim(col(\"name\"))\n",
    ").withColumn(\n",
    "    # Handle empty strings\n",
    "    \"name_final\", when(trim(col(\"name\")) == \"\", lit(\"Unknown\")).otherwise(trim(col(\"name\")))\n",
    ").withColumn(\n",
    "    # Safe numeric conversion\n",
    "    \"score_numeric\", \n",
    "    when(col(\"score\").rlike(r\"^\\d+\\.?\\d*$\"), col(\"score\").cast(DoubleType()))\n",
    "    .otherwise(lit(0.0))\n",
    ").withColumn(\n",
    "    # Safe date conversion\n",
    "    \"timestamp_clean\",\n",
    "    when(col(\"timestamp\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}$\"),\n",
    "         to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "    .otherwise(lit(None).cast(TimestampType()))\n",
    ").withColumn(\n",
    "    # Standardize boolean values\n",
    "    \"active_boolean\",\n",
    "    when(lower(col(\"active\")).isin(\"true\", \"1\", \"yes\"), lit(True))\n",
    "    .when(lower(col(\"active\")).isin(\"false\", \"0\", \"no\"), lit(False))\n",
    "    .otherwise(lit(None).cast(BooleanType()))\n",
    ").withColumn(\n",
    "    # Handle various null representations\n",
    "    \"notes_clean\",\n",
    "    when(lower(trim(col(\"notes\"))).isin(\"null\", \"\", \"n/a\", \"none\"), lit(None))\n",
    "    .otherwise(col(\"notes\"))\n",
    ")\n",
    "\n",
    "print(\"Cleaned data:\")\n",
    "cleaned_df.show()\n",
    "print(\"Schema after cleaning:\")\n",
    "cleaned_df.printSchema()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 6. EXPLODING ARRAYS - Converting arrays to separate rows\n",
    "print(\"6. Exploding Arrays - Array to Rows Conversion\")\n",
    "\n",
    "# Using our earlier data with arrays\n",
    "df_to_explode = df_json.select(\"emp_id\", \"full_name\", \"skills_array\", \"certifications_array\")\n",
    "\n",
    "# Explode skills array - each skill becomes a separate row\n",
    "df_skills_exploded = df_to_explode.select(\n",
    "    \"emp_id\", \"full_name\", \n",
    "    explode(\"skills_array\").alias(\"individual_skill\")\n",
    ")\n",
    "\n",
    "print(\"Skills exploded to separate rows:\")\n",
    "df_skills_exploded.show()\n",
    "\n",
    "# Explode with position to keep track of array index\n",
    "df_skills_with_pos = df_to_explode.select(\n",
    "    \"emp_id\", \"full_name\",\n",
    "    posexplode(\"skills_array\").alias(\"skill_position\", \"individual_skill\")\n",
    ")\n",
    "\n",
    "print(\"Skills exploded with positions:\")\n",
    "df_skills_with_pos.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 7. PIVOT AND UNPIVOT OPERATIONS\n",
    "print(\"7. Pivot and Unpivot Operations\")\n",
    "\n",
    "# Create data suitable for pivoting\n",
    "performance_data = [\n",
    "    (\"Alice\", \"Q1\", 85, 12),\n",
    "    (\"Alice\", \"Q2\", 92, 15),\n",
    "    (\"Alice\", \"Q3\", 88, 13),\n",
    "    (\"Bob\", \"Q1\", 78, 10),\n",
    "    (\"Bob\", \"Q2\", 85, 14),\n",
    "    (\"Bob\", \"Q3\", 90, 16),\n",
    "    (\"Carol\", \"Q1\", 95, 18),\n",
    "    (\"Carol\", \"Q2\", 88, 16),\n",
    "    (\"Carol\", \"Q3\", 92, 19)\n",
    "]\n",
    "\n",
    "perf_df = spark.createDataFrame(performance_data, [\"employee\", \"quarter\", \"score\", \"sales\"])\n",
    "\n",
    "print(\"Original performance data:\")\n",
    "perf_df.show()\n",
    "\n",
    "# Pivot to make quarters into columns\n",
    "pivoted_perf = perf_df.groupBy(\"employee\") \\\n",
    "                     .pivot(\"quarter\") \\\n",
    "                     .agg(first(\"score\").alias(\"score\"), \n",
    "                          first(\"sales\").alias(\"sales\"))\n",
    "\n",
    "print(\"Pivoted performance data:\")\n",
    "pivoted_perf.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90 + \"\\n\")\n",
    "\n",
    "# 8. USER-DEFINED FUNCTIONS (UDFs)\n",
    "print(\"8. User-Defined Functions for Custom Logic\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define a complex function that would be hard to express with built-in functions\n",
    "def calculate_risk_score(years_exp, num_skills, num_certs):\n",
    "    \"\"\"Calculate employee risk score based on multiple factors\"\"\"\n",
    "    if years_exp is None or num_skills is None or num_certs is None:\n",
    "        return 0\n",
    "    \n",
    "    base_score = min(years_exp * 10, 100)  # Cap at 100\n",
    "    skill_bonus = min(num_skills * 5, 50)   # Cap at 50\n",
    "    cert_bonus = min(num_certs * 10, 30)    # Cap at 30\n",
    "    \n",
    "    total_score = base_score + skill_bonus + cert_bonus\n",
    "    \n",
    "    # Apply risk categories\n",
    "    if total_score >= 150:\n",
    "        return 1  # Low risk\n",
    "    elif total_score >= 100:\n",
    "        return 2  # Medium risk\n",
    "    else:\n",
    "        return 3  # High risk\n",
    "\n",
    "# Register UDF\n",
    "risk_score_udf = udf(calculate_risk_score, IntegerType())\n",
    "\n",
    "# Apply UDF\n",
    "df_with_risk = df_json.withColumn(\n",
    "    \"risk_score\",\n",
    "    risk_score_udf(col(\"years_experience\"), col(\"num_skills\"), col(\"num_certifications\"))\n",
    ").withColumn(\n",
    "    \"risk_category\",\n",
    "    when(col(\"risk_score\") == 1, \"Low Risk\")\n",
    "    .when(col(\"risk_score\") == 2, \"Medium Risk\")\n",
    "    .otherwise(\"High Risk\")\n",
    ")\n",
    "\n",
    "print(\"Data with custom risk scoring:\")\n",
    "df_with_risk.select(\"full_name\", \"years_experience\", \"num_skills\", \"num_certifications\",\n",
    "                   \"risk_score\", \"risk_category\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ADVANCED MANIPULATION KEY CONCEPTS:\")\n",
    "print(\"• String functions: split(), regexp_extract(), substring(), concat()\")\n",
    "print(\"• Array functions: array_contains(), size(), slice(), explode()\")\n",
    "print(\"• JSON functions: from_json(), to_json(), get_json_object()\")\n",
    "print(\"• Conditional logic: when().otherwise(), complex boolean conditions\")\n",
    "print(\"• Data cleaning: trim(), cast(), safe conversions with when()\")\n",
    "print(\"• UDFs: For complex logic that can't be expressed with built-in functions\")\n",
    "print(\"• Always prefer built-in functions over UDFs for performance\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73b097",
   "metadata": {},
   "source": [
    "### **What You've Just Mastered**\n",
    "\n",
    "- **Window Functions** are the secret weapon of advanced data analysts. They let you perform complex calculations like running totals, rankings, and comparisons without losing the detail of your original data. The key insight is understanding the three components: partition (which records to group), order (how to sort within groups), and frame (which rows to include in calculations).\n",
    "\n",
    "- **Advanced Data Manipulation** techniques let you handle real-world messy data. You'll rarely get clean, perfectly structured data in practice. These techniques - from JSON parsing to complex conditional logic to custom functions - are what transform you from someone who can work with toy datasets to someone who can handle production data systems.\n",
    "The most important concept here is that PySpark gives you both high-level abstractions (DataFrame operations) and the flexibility to handle complex scenarios (UDFs, complex transformations). The art is knowing when to use each approach - built-in functions are almost always faster and more reliable than custom code.\n",
    "\n",
    "- **Window functions**, in particular, solve a class of problems that would otherwise require complex self-joins or multiple passes through your data. They're essential for time-series analysis, ranking problems, and any scenario where you need to compare each row with its neighbors or with aggregate statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
