{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10068725",
   "metadata": {},
   "source": [
    "# Comprehensive PySpark Guide: Installation and Cluster Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef7140",
   "metadata": {},
   "source": [
    "## Part 1: Installing PySpark in Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96215e",
   "metadata": {},
   "source": [
    "### Prerequisites Installation\n",
    "\n",
    "First, let's install the necessary prerequisites:\n",
    "\n",
    "1. **Update your system packages**:\n",
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt upgrade\n",
    "   ```\n",
    "\n",
    "2. **Install Java (PySpark requires Java 8 or higher)**:\n",
    "   ```bash\n",
    "   sudo apt install default-jdk\n",
    "   ```\n",
    "\n",
    "3. **Verify Java installation**:\n",
    "   ```bash\n",
    "   java -version\n",
    "   ```\n",
    "\n",
    "4. **Install Python and pip** (if not already installed):\n",
    "   ```bash\n",
    "   sudo apt install python3 python3-pip\n",
    "   ```\n",
    "\n",
    "### Installing PySpark\n",
    "\n",
    "There are two main approaches to install PySpark:\n",
    "\n",
    "#### Method 1: Using pip (Recommended for beginners)\n",
    "\n",
    "1. **Install PySpark package**:\n",
    "   ```bash\n",
    "   pip3 install pyspark\n",
    "   ```\n",
    "\n",
    "2. **Verify installation**:\n",
    "   ```bash\n",
    "   python3 -c \"import pyspark; print(pyspark.__version__)\"\n",
    "   ```\n",
    "\n",
    "#### Method 2: Manual installation (More control)\n",
    "\n",
    "1. **Download Apache Spark**:\n",
    "   ```bash\n",
    "   wget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "   ```\n",
    "\n",
    "2. **Extract the archive**:\n",
    "   ```bash\n",
    "   tar -xzf spark-3.5.0-bin-hadoop3.tgz\n",
    "   ```\n",
    "\n",
    "3. **Move to /opt directory** (recommended location):\n",
    "   ```bash\n",
    "   sudo mv spark-3.5.0-bin-hadoop3 /opt/spark\n",
    "   ```\n",
    "\n",
    "4. **Install PySpark with pip** (to ensure Python bindings):\n",
    "   ```bash\n",
    "   pip3 install pyspark\n",
    "   ```\n",
    "\n",
    "### Setting Up Environment Variables\n",
    "\n",
    "Add these to your `~/.bashrc` or `~/.profile` file:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "```\n",
    "\n",
    "After adding, apply the changes:\n",
    "```bash\n",
    "source ~/.bashrc  # or source ~/.profile\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd321f",
   "metadata": {},
   "source": [
    "## Part 2: Configuring a Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b832a29",
   "metadata": {},
   "source": [
    "### Types of Spark Cluster Deployment\n",
    "\n",
    "You can configure Spark in several modes:\n",
    "\n",
    "1. **Standalone Mode** - Spark's built-in cluster manager\n",
    "2. **YARN Mode** - Using Hadoop's resource manager\n",
    "3. **Mesos Mode** - Using Apache Mesos\n",
    "4. **Kubernetes Mode** - Deploying on Kubernetes\n",
    "\n",
    "We'll focus primarily on Standalone mode as it's the simplest to set up.\n",
    "\n",
    "### Prerequisites for Cluster Setup\n",
    "\n",
    "Before configuring your cluster, ensure you have:\n",
    "\n",
    "- Java 8 or higher installed on all machines\n",
    "- SSH access between all nodes\n",
    "- Same version of Spark installed in the same directory path on all machines\n",
    "- Consistent user accounts across machines\n",
    "- Open ports for Spark communication (default: 7077 for master, 8080 for web UI)\n",
    "\n",
    "### Step 1: Install Spark on All Machines\n",
    "\n",
    "Follow these steps on each machine in your cluster:\n",
    "\n",
    "```bash\n",
    "# Download and install Spark (same version on all machines)\n",
    "wget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "tar -xzf spark-3.5.0-bin-hadoop3.tgz\n",
    "sudo mv spark-3.5.0-bin-hadoop3 /opt/spark\n",
    "\n",
    "# Set environment variables on all machines\n",
    "echo 'export SPARK_HOME=/opt/spark' >> ~/.bashrc\n",
    "echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' >> ~/.bashrc\n",
    "echo 'export PYSPARK_PYTHON=python3' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### Step 2: Configure SSH Key-based Authentication\n",
    "\n",
    "For the master to communicate with workers without password prompts:\n",
    "\n",
    "```bash\n",
    "# On master node\n",
    "ssh-keygen -t rsa -P \"\"\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "\n",
    "# Copy the public key to each worker node\n",
    "ssh-copy-id username@worker1\n",
    "ssh-copy-id username@worker2\n",
    "# Repeat for all worker nodes\n",
    "```\n",
    "\n",
    "### Step 3: Configure Spark Environment\n",
    "\n",
    "#### On Master Node\n",
    "\n",
    "1. Create the configuration files by copying the templates:\n",
    "\n",
    "```bash\n",
    "cd $SPARK_HOME/conf\n",
    "cp spark-env.sh.template spark-env.sh\n",
    "cp spark-defaults.conf.template spark-defaults.conf\n",
    "```\n",
    "\n",
    "2. Edit the `spark-env.sh` file:\n",
    "\n",
    "```bash\n",
    "# Add these lines to spark-env.sh\n",
    "export SPARK_MASTER_HOST=master-hostname  # Use the actual hostname or IP\n",
    "export SPARK_MASTER_PORT=7077\n",
    "export SPARK_MASTER_WEBUI_PORT=8080\n",
    "export SPARK_WORKER_CORES=4  # Set to number of cores you want to allocate\n",
    "export SPARK_WORKER_MEMORY=8g  # Set to amount of memory you want to allocate\n",
    "export SPARK_WORKER_INSTANCES=1\n",
    "export SPARK_PUBLIC_DNS=master-hostname  # Use your actual public DNS if available\n",
    "```\n",
    "\n",
    "3. Create a workers file:\n",
    "\n",
    "```bash\n",
    "cp workers.template workers\n",
    "```\n",
    "\n",
    "4. Edit the `workers` file to include all worker nodes:\n",
    "\n",
    "```\n",
    "worker1\n",
    "worker2\n",
    "worker3\n",
    "# Add more worker hostnames or IPs as needed\n",
    "```\n",
    "\n",
    "#### On Worker Nodes\n",
    "\n",
    "1. Create and edit the `spark-env.sh` file with the same settings as the master, but make sure to use the worker's own hostname where appropriate:\n",
    "\n",
    "```bash\n",
    "cd $SPARK_HOME/conf\n",
    "cp spark-env.sh.template spark-env.sh\n",
    "```\n",
    "\n",
    "2. Edit the `spark-env.sh` file with worker-specific settings:\n",
    "\n",
    "```bash\n",
    "# Add these lines to spark-env.sh\n",
    "export SPARK_MASTER_HOST=master-hostname  # Use the actual master hostname or IP\n",
    "export SPARK_WORKER_CORES=4  # Adjust based on server capacity\n",
    "export SPARK_WORKER_MEMORY=8g  # Adjust based on server capacity\n",
    "export SPARK_WORKER_INSTANCES=1\n",
    "export SPARK_PUBLIC_DNS=worker-hostname  # Use this worker's hostname\n",
    "```\n",
    "\n",
    "### Step 4: Configure Additional Spark Settings\n",
    "\n",
    "Edit `spark-defaults.conf` on all machines for cluster-wide settings:\n",
    "\n",
    "```\n",
    "spark.master                     spark://master-hostname:7077\n",
    "spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
    "spark.driver.memory              4g\n",
    "spark.executor.memory            4g\n",
    "spark.executor.cores             2\n",
    "spark.default.parallelism        8\n",
    "spark.sql.shuffle.partitions     200\n",
    "spark.local.dir                  /tmp/spark-temp\n",
    "```\n",
    "\n",
    "### Step 5: Start the Cluster\n",
    "\n",
    "#### Start the Master Node\n",
    "\n",
    "On the master machine:\n",
    "\n",
    "```bash\n",
    "$SPARK_HOME/sbin/start-master.sh\n",
    "```\n",
    "\n",
    "Check the master web UI at `http://master-hostname:8080` to confirm it's running.\n",
    "\n",
    "#### Start the Worker Nodes\n",
    "\n",
    "You can start workers in two ways:\n",
    "\n",
    "1. **From the master node** (if you've configured SSH properly):\n",
    "\n",
    "```bash\n",
    "$SPARK_HOME/sbin/start-workers.sh\n",
    "```\n",
    "\n",
    "2. **On each worker individually**:\n",
    "\n",
    "```bash\n",
    "$SPARK_HOME/sbin/start-worker.sh spark://master-hostname:7077\n",
    "```\n",
    "\n",
    "### Step 6: Submitting Applications to the Cluster\n",
    "\n",
    "You can now submit applications to your cluster using `spark-submit`:\n",
    "\n",
    "```bash\n",
    "$SPARK_HOME/bin/spark-submit \\\n",
    "  --master spark://master-hostname:7077 \\\n",
    "  --deploy-mode cluster \\\n",
    "  --driver-memory 4g \\\n",
    "  --executor-memory 2g \\\n",
    "  --executor-cores 1 \\\n",
    "  --num-executors 3 \\\n",
    "  your_application.py\n",
    "```\n",
    "\n",
    "### Step 7: Monitoring and Managing Your Cluster\n",
    "\n",
    "1. **Web UI**: Access the master web UI at `http://master-hostname:8080`\n",
    "2. **Spark History Server**: To track completed applications:\n",
    "\n",
    "```bash\n",
    "# First, configure the event log directory\n",
    "echo 'spark.eventLog.enabled true' >> $SPARK_HOME/conf/spark-defaults.conf\n",
    "echo 'spark.eventLog.dir file:/tmp/spark-events' >> $SPARK_HOME/conf/spark-defaults.conf\n",
    "echo 'spark.history.fs.logDirectory file:/tmp/spark-events' >> $SPARK_HOME/conf/spark-defaults.conf\n",
    "\n",
    "# Create the event log directory\n",
    "mkdir -p /tmp/spark-events\n",
    "\n",
    "# Start the history server\n",
    "$SPARK_HOME/sbin/start-history-server.sh\n",
    "```\n",
    "\n",
    "Access the history server at `http://master-hostname:18080`\n",
    "\n",
    "### Step 8: Advanced Configuration Options\n",
    "\n",
    "#### High Availability Setup\n",
    "\n",
    "For production environments, configure Spark with ZooKeeper for high availability:\n",
    "\n",
    "1. Install ZooKeeper on a separate set of machines\n",
    "2. Add these to `spark-defaults.conf`:\n",
    "\n",
    "```\n",
    "spark.deploy.recoveryMode ZOOKEEPER\n",
    "spark.deploy.zookeeper.url zk1:2181,zk2:2181,zk3:2181\n",
    "spark.deploy.zookeeper.dir /spark\n",
    "```\n",
    "\n",
    "#### Resource Management\n",
    "\n",
    "Tune these parameters based on your workload:\n",
    "\n",
    "```\n",
    "# Memory management\n",
    "spark.memory.fraction 0.6\n",
    "spark.memory.storageFraction 0.5\n",
    "\n",
    "# Execution\n",
    "spark.speculation true\n",
    "spark.speculation.multiplier 3\n",
    "```\n",
    "\n",
    "#### Network Configuration\n",
    "\n",
    "For improved network performance:\n",
    "\n",
    "```\n",
    "spark.network.timeout 120s\n",
    "spark.rpc.message.maxSize 256\n",
    "```\n",
    "\n",
    "### Step 9: Stopping Your Cluster\n",
    "\n",
    "When you're done, stop the cluster:\n",
    "\n",
    "```bash\n",
    "# From the master node (if SSH is configured)\n",
    "$SPARK_HOME/sbin/stop-all.sh\n",
    "\n",
    "# Or stop individually\n",
    "$SPARK_HOME/sbin/stop-master.sh\n",
    "$SPARK_HOME/sbin/stop-workers.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70753c62",
   "metadata": {},
   "source": [
    "## Part 3: Basic Usage of PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87e3a0",
   "metadata": {},
   "source": [
    "### Starting the PySpark Shell & Creating a simple script\n",
    "\n",
    "1. **Launch the interactive PySpark shell**:\n",
    "   ```bash\n",
    "   pyspark\n",
    "   ```\n",
    "\n",
    "2. **You should see a Spark context (sc) and Spark session (spark) automatically created**\n",
    "\n",
    "#### Creating a Simple PySpark Script\n",
    "\n",
    "Create a file named `simple_pyspark.py`:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"Simple PySpark Example\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"DataFrame contents:\")\n",
    "df.show()\n",
    "\n",
    "# Perform some operations\n",
    "print(\"Average age:\")\n",
    "df.select(\"Age\").groupBy().avg().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "Run the script:\n",
    "```bash\n",
    "python3 simple_pyspark.py\n",
    "```\n",
    "\n",
    "### Running PySpark in Jupyter Notebook\n",
    "\n",
    "1. **Install Jupyter**:\n",
    "   ```bash\n",
    "   pip3 install jupyter\n",
    "   ```\n",
    "\n",
    "2. **Configure PySpark to work with Jupyter**:\n",
    "   Add to your `~/.bashrc`:\n",
    "   ```bash\n",
    "   export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "   export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "   ```\n",
    "\n",
    "3. **Start Jupyter with PySpark**:\n",
    "   ```bash\n",
    "   pyspark\n",
    "   ```\n",
    "   \n",
    "   Or to use regular Python and import PySpark:\n",
    "   ```bash\n",
    "   jupyter notebook\n",
    "   ```\n",
    "\n",
    "### Using PySpark with Larger Datasets\n",
    "\n",
    "For a more realistic example, create another script that processes CSV data:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV Processing Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file (replace with your actual file path)\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"your_data.csv\")\n",
    "\n",
    "# Show schema and sample data\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "print(\"Sample Data:\")\n",
    "df.show(5)\n",
    "\n",
    "# Perform aggregations (example assuming columns 'age' and 'salary' exist)\n",
    "print(\"Summary Statistics:\")\n",
    "df.select(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    count(\"*\").alias(\"count\")\n",
    ").show()\n",
    "\n",
    "# Filter data\n",
    "filtered_df = df.filter(col(\"age\") > 30)\n",
    "print(\"Filtered data (age > 30):\")\n",
    "filtered_df.show(5)\n",
    "\n",
    "# Save results\n",
    "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_results\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73977cbf",
   "metadata": {},
   "source": [
    "## Troubleshooting Common Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7b7f3",
   "metadata": {},
   "source": [
    "### For PySpark Installation\n",
    "\n",
    "1. **Java issues**: If you encounter Java-related errors, ensure you have the correct Java version:\n",
    "   ```bash\n",
    "   sudo update-alternatives --config java\n",
    "   ```\n",
    "\n",
    "2. **Memory issues**: Edit Spark configuration for more memory:\n",
    "   ```bash\n",
    "   export SPARK_OPTS=\"--driver-memory 4g --executor-memory 4g\"\n",
    "   ```\n",
    "\n",
    "3. **Python version conflicts**: Set the Python version explicitly:\n",
    "   ```bash\n",
    "   export PYSPARK_PYTHON=python3.8  # or your specific version\n",
    "   ```\n",
    "\n",
    "4. **Connection refused errors**: Check if ports are being blocked by a firewall:\n",
    "   ```bash\n",
    "   sudo ufw status\n",
    "   ```\n",
    "\n",
    "### For Cluster Configuration\n",
    "\n",
    "1. **Nodes not connecting**: Check firewall settings and ensure ports 7077, 8080 are open\n",
    "2. **Memory issues**: Adjust SPARK_WORKER_MEMORY and make sure physical RAM is sufficient\n",
    "3. **SSH problems**: Verify SSH key configuration with `ssh worker1 date` to test\n",
    "4. **Application failures**: Check logs in `$SPARK_HOME/logs` directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7709361",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once you're comfortable with basic PySpark usage and cluster configuration, you might want to explore:\n",
    "\n",
    "1. Spark SQL for structured data processing\n",
    "2. Spark MLlib for machine learning\n",
    "3. Spark Streaming for real-time data processing\n",
    "4. GraphX for graph processing\n",
    "5. Integration with data sources like HDFS, Hive, or cloud storage"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
